{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6ba10f-0def-48d3-b726-926f4558d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ce285c-6ee1-41ab-ace2-f49af16857c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d5c2f-6373-4d1e-a762-41d5d53c1f9a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93377edb-602d-4ddd-9c12-d89c1e445297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, root, phase, transformer=None):\n",
    "        self.root=root\n",
    "        self.phase=phase\n",
    "        self.transformer=transformer\n",
    "        self.image_list=sorted(os.listdir(root+phase+\"/image/\"))\n",
    "        self.des_list=sorted(os.listdir(root+phase+\"/description/\"))\n",
    "        self.label_list=sorted(os.listdir(root+phase+\"/label/\"))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, des, label = self.get_data(index)\n",
    "        return img['image'], des, label\n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def get_data(self, index):\n",
    "        # label\n",
    "        try:\n",
    "            label_file_name=self.label_list[index]\n",
    "            lab_f=open(self.root+self.phase+\"/label/\"+label_file_name, \"r\")\n",
    "            label=lab_f.read()\n",
    "            if(len(label)>=10):\n",
    "                label=label[0:10]\n",
    "            elif(len(label)<10):\n",
    "                margin=10-len(label)\n",
    "                padding=\" \"*margin\n",
    "                label=label+padding\n",
    "            label=list(label.lower())\n",
    "\n",
    "            # description\n",
    "            des_file_name=self.des_list[index]\n",
    "            des_f=open(self.root+self.phase+\"/description/\"+des_file_name, \"r\")\n",
    "            des_text=des_f.read()\n",
    "    #         des=des_text.split(\" \")\n",
    "            des=des_text\n",
    "\n",
    "            # image\n",
    "            img_file_name=self.image_list[index]\n",
    "            image=cv2.imread(self.root+self.phase+\"/image/\"+img_file_name)\n",
    "            img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if(self.transformer!=None):\n",
    "                transformed_img=self.transformer(image=img)\n",
    "                img=transformed_img\n",
    "        except:\n",
    "            print(f\"error: image name=>{img_file_name} des name=>{des_file_name} label name=>{label_file_name}\")\n",
    "            img={'image':torch.zeros((3,448,448))}\n",
    "            des=''\n",
    "            label=''\n",
    "        return img, des, label\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "15795556",
   "metadata": {},
   "source": [
    "PATH=\"/workspace/team2/data/filter_50000/\"\n",
    "\n",
    "is_cuda = True\n",
    "\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 16\n",
    "VERBOSE_FREQ = 20\n",
    "LR=0.0001\n",
    "\n",
    "IMAGE_ENC=\"RESNET34\"\n",
    "TEXT_ENC=\"BERT\"\n",
    "DECODER=\"LSTM\"\n",
    "num_epochs = 100\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    des_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        des_list.append(b)\n",
    "        label_list.append(c)\n",
    "\n",
    "    return torch.stack(image_list, dim=0), des_list, label_list\n",
    "\n",
    "dataloaders = build_dataloader(PATH=PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "\n",
    "    running_loss = defaultdict(float)\n",
    "    for index, batch in enumerate(dataloaders[phase]):\n",
    "        images = batch[0]\n",
    "        description = batch[1]\n",
    "        label = batch[2]\n",
    "        if phase == \"train\":\n",
    "            if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                text = f\"<<<train iteration:[{index}/{len(dataloaders[phase])}] - \"\n",
    "                print(text)\n",
    "        elif phase == \"val\":\n",
    "            if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                text = f\"<<<valid iteration:[{index}/{len(dataloaders[phase])}] - \"\n",
    "                print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37180c3-40c0-4f29-84b7-da062c85be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=448\n",
    "transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e40fb9-26a2-4084-a74c-62d7647138cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root='/workspace/team2/data/filter_50000/'\n",
    "train_dataset=Dataset(root=root, phase=\"train\", transformer=transformer)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a25b3eb2-ebf3-403c-98db-b3b6fd964e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae5d03e0cc34f6abc20e15219b6bb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=21931, description='index', max=43863), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(index=(0, len(train_dataset)-1))\n",
    "def show_sample(index):\n",
    "    img, des, label = train_dataset[index]\n",
    "    image=img.permute(1,2,0).numpy()\n",
    "    print(f'desciption: {des}')\n",
    "    print(f'label: {label}-> label length:{len(label)}')\n",
    "    print(image.shape)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36d0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(PATH, batch_size=2):\n",
    "    IMAGE_SIZE = 448\n",
    "    transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    dataloaders = {}\n",
    "#     train_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer, aug=None)\n",
    "    train_dataset=Dataset(root=PATH, phase=\"train\", transformer=transformer)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    val_dataset=Dataset(root=PATH, phase=\"valid\", transformer=transformer)\n",
    "    dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    print(f\"trainset:{len(train_dataset)} validset:{len(val_dataset)}\")\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "674524df",
   "metadata": {},
   "source": [
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    des_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        des_list.append(b)\n",
    "        label_list.append(c)\n",
    "\n",
    "    return torch.stack(image_list, dim=0), des_list, label_list\n",
    "\n",
    "PATH='/workspace/team2/data/filter_50000/'\n",
    "BATCH_SIZE=16\n",
    "dataloaders = build_dataloader(PATH=PATH, batch_size=BATCH_SIZE)\n",
    "for index, batch in enumerate(dataloaders['train']):\n",
    "    images = batch[0].to(device)\n",
    "    targets = batch[1]\n",
    "    filenames = batch[2]\n",
    "    print(f\"{index}--{images.shape} {len(targets)} {len(targets)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb5975-f093-4940-952b-abff5967f2fc",
   "metadata": {},
   "source": [
    "## MODELs\n",
    " ![Untitled](../img/nickCLIP_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae290c3",
   "metadata": {},
   "source": [
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d328de53-85c7-43e6-808c-32c4241de00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        resnet = torchvision.models.resnet34(pretrained = True)\n",
    "        layers = [m for m in resnet.children()]\n",
    "        \n",
    "        self.backbone = nn.Sequential(*layers[:-2]) \n",
    "        self.head = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, padding=0,bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=32, out_channels=4, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(in_features=784, out_features=768)\n",
    "            \n",
    "            )\n",
    "        \n",
    "#         self.head = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, padding=0,bias=False),\n",
    "#                 nn.BatchNorm2d(256),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1,bias=False),\n",
    "#                 nn.BatchNorm2d(128),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3, padding=1,bias=False),\n",
    "#                 nn.BatchNorm2d(32),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, padding=1,bias=False),\n",
    "#                 nn.BatchNorm2d(1),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Flatten()\n",
    "#             )\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.head(out) # final output=> (1, 196)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b041c468-eff7-4f10-a803-cebecf1bbe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-11         [-1, 64, 112, 112]               0\n",
      "           Conv2d-12         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
      "             ReLU-14         [-1, 64, 112, 112]               0\n",
      "           Conv2d-15         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-16         [-1, 64, 112, 112]             128\n",
      "             ReLU-17         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-18         [-1, 64, 112, 112]               0\n",
      "           Conv2d-19         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-20         [-1, 64, 112, 112]             128\n",
      "             ReLU-21         [-1, 64, 112, 112]               0\n",
      "           Conv2d-22         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-23         [-1, 64, 112, 112]             128\n",
      "             ReLU-24         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-25         [-1, 64, 112, 112]               0\n",
      "           Conv2d-26          [-1, 128, 56, 56]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 56, 56]             256\n",
      "             ReLU-28          [-1, 128, 56, 56]               0\n",
      "           Conv2d-29          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 56, 56]             256\n",
      "           Conv2d-31          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
      "             ReLU-33          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-34          [-1, 128, 56, 56]               0\n",
      "           Conv2d-35          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 56, 56]             256\n",
      "             ReLU-37          [-1, 128, 56, 56]               0\n",
      "           Conv2d-38          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 56, 56]             256\n",
      "             ReLU-40          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-41          [-1, 128, 56, 56]               0\n",
      "           Conv2d-42          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 56, 56]             256\n",
      "             ReLU-44          [-1, 128, 56, 56]               0\n",
      "           Conv2d-45          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 56, 56]             256\n",
      "             ReLU-47          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-48          [-1, 128, 56, 56]               0\n",
      "           Conv2d-49          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 56, 56]             256\n",
      "             ReLU-51          [-1, 128, 56, 56]               0\n",
      "           Conv2d-52          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 56, 56]             256\n",
      "             ReLU-54          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-55          [-1, 128, 56, 56]               0\n",
      "           Conv2d-56          [-1, 256, 28, 28]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 28, 28]             512\n",
      "             ReLU-58          [-1, 256, 28, 28]               0\n",
      "           Conv2d-59          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 28, 28]             512\n",
      "           Conv2d-61          [-1, 256, 28, 28]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 28, 28]             512\n",
      "             ReLU-63          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-64          [-1, 256, 28, 28]               0\n",
      "           Conv2d-65          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 28, 28]             512\n",
      "             ReLU-67          [-1, 256, 28, 28]               0\n",
      "           Conv2d-68          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 28, 28]             512\n",
      "             ReLU-70          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-71          [-1, 256, 28, 28]               0\n",
      "           Conv2d-72          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 28, 28]             512\n",
      "             ReLU-74          [-1, 256, 28, 28]               0\n",
      "           Conv2d-75          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 28, 28]             512\n",
      "             ReLU-77          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-78          [-1, 256, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 28, 28]             512\n",
      "             ReLU-84          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-85          [-1, 256, 28, 28]               0\n",
      "           Conv2d-86          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 28, 28]             512\n",
      "             ReLU-88          [-1, 256, 28, 28]               0\n",
      "           Conv2d-89          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 28, 28]             512\n",
      "             ReLU-91          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-92          [-1, 256, 28, 28]               0\n",
      "           Conv2d-93          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 28, 28]             512\n",
      "             ReLU-95          [-1, 256, 28, 28]               0\n",
      "           Conv2d-96          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 28, 28]             512\n",
      "             ReLU-98          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-99          [-1, 256, 28, 28]               0\n",
      "          Conv2d-100          [-1, 512, 14, 14]       1,179,648\n",
      "     BatchNorm2d-101          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-102          [-1, 512, 14, 14]               0\n",
      "          Conv2d-103          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-104          [-1, 512, 14, 14]           1,024\n",
      "          Conv2d-105          [-1, 512, 14, 14]         131,072\n",
      "     BatchNorm2d-106          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-107          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-108          [-1, 512, 14, 14]               0\n",
      "          Conv2d-109          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-110          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-111          [-1, 512, 14, 14]               0\n",
      "          Conv2d-112          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-113          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-114          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-115          [-1, 512, 14, 14]               0\n",
      "          Conv2d-116          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-117          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-118          [-1, 512, 14, 14]               0\n",
      "          Conv2d-119          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-120          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-121          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-122          [-1, 512, 14, 14]               0\n",
      "          Conv2d-123          [-1, 256, 14, 14]         131,072\n",
      "     BatchNorm2d-124          [-1, 256, 14, 14]             512\n",
      "            ReLU-125          [-1, 256, 14, 14]               0\n",
      "          Conv2d-126          [-1, 128, 14, 14]         294,912\n",
      "     BatchNorm2d-127          [-1, 128, 14, 14]             256\n",
      "            ReLU-128          [-1, 128, 14, 14]               0\n",
      "          Conv2d-129           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-130           [-1, 32, 14, 14]              64\n",
      "            ReLU-131           [-1, 32, 14, 14]               0\n",
      "          Conv2d-132            [-1, 4, 14, 14]           1,152\n",
      "     BatchNorm2d-133            [-1, 4, 14, 14]               8\n",
      "            ReLU-134            [-1, 4, 14, 14]               0\n",
      "         Flatten-135                  [-1, 784]               0\n",
      "          Linear-136                  [-1, 768]         602,880\n",
      "================================================================\n",
      "Total params: 22,352,392\n",
      "Trainable params: 22,352,392\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 387.01\n",
      "Params size (MB): 85.27\n",
      "Estimated Total Size (MB): 474.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Image_Enc = Image_Encoder()\n",
    "Image_Enc.to(device)\n",
    "torchsummary.summary(Image_Enc, (3,448,448))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04259a95",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d841b979-6222-4183-854d-a83b06918ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Encoder(nn.Module):\n",
    "    def __init__(self, device, pretrained='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.pretrained=pretrained\n",
    "        self.device=device\n",
    "        self.BERT = BertModel.from_pretrained(self.pretrained)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.pretrained)\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        \n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to(self.device)\n",
    "        segments_tensors = torch.tensor([segments_ids]).to(self.device)\n",
    "        \n",
    "        return tokens_tensor, segments_tensors\n",
    "        \n",
    "    def postprocess(self, encoded_layers):\n",
    "#         token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "#         token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        token_vecs = encoded_layers[11][0]\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        \n",
    "        return sentence_embedding\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         tokens_tensor, segments_tensors = self.preprocess(x)\n",
    "        \n",
    "#         encoded_layers, _ = self.BERT(tokens_tensor, segments_tensors)\n",
    "        \n",
    "#         sentence_embedding = self.postprocess(encoded_layers)\n",
    "        \n",
    "#         return sentence_embedding\n",
    "    def forward(self,x):\n",
    "        tokens=self.tokenizer(x, \n",
    "                 add_special_tokens=True, \n",
    "                 max_length=100, \n",
    "                 padding=\"max_length\",\n",
    "                 truncation=True,\n",
    "                 return_tensors=\"pt\")\n",
    "        tokens.to(self.device)\n",
    "        output = self.BERT(**tokens)\n",
    "        out = output.last_hidden_state.mean(axis=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "926c73a0-8099-4ced-a244-25a563220ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Enc=Text_Encoder(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767d95a6-5999-45a7-9573-4749e7931ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text_Encoder(\n",
       "  (BERT): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_Enc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d637de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac69b465ef2c4ba7b140dd772a3ad661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=21931, description='index', max=43863), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(index=(0, len(train_dataset)-1))\n",
    "def show_sample(index):\n",
    "    img, des, label = train_dataset[index]\n",
    "#     image=img['image'].permute(1,2,0).numpy()\n",
    "    image=img.unsqueeze(0).to(device)\n",
    "    img_embed=Image_Enc(image)\n",
    "    sen_embed=Text_Enc(des)\n",
    "    print(f'desciption: {des}')\n",
    "    print(f'label: {label}')\n",
    "    print(f\"img shape:{image.shape}\")\n",
    "    print(f'sen_emb shape:{sen_embed.shape}')\n",
    "    print(f'img_embed shape:{img_embed.shape}')\n",
    "    image=image.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d98926ca",
   "metadata": {},
   "source": [
    "PATH=\"/workspace/team2/data/filter_50000/\"\n",
    "BATCH_SIZE = 16\n",
    "dataloaders = build_dataloader(PATH=PATH, batch_size=BATCH_SIZE)\n",
    "for index, batch in enumerate(dataloaders['train']):\n",
    "    images = batch[0]\n",
    "    description = batch[1]\n",
    "    label = batch[2]\n",
    "    print(f\"{index}-{images.shape} {len(description)} {len(label)}\")\n",
    "    \n",
    "    img_embed=Image_Enc(images.to(device))\n",
    "    sen_embed=Text_Enc(description)\n",
    "    concat_embed=img_embed+sen_embed\n",
    "    \n",
    "    print(f\"{img_embed.shape}, {sen_embed.shape}. {concat_embed.shape}\")\n",
    "#     target=alp_to_mat(label,len(label))\n",
    "#     output=model(image, description)\n",
    "    \n",
    "#     target_for_loss=target.view(-1,370).to(device)\n",
    "#     output_for_loss=output.view(-1,370).to(device)\n",
    "    \n",
    "#     loss=criterion(output_for_loss, target_for_loss, torch.Tensor(output_for_loss.size(0)).cuda().fill_(1.0))\n",
    "#     print(f\"output shape:{output.shape} <=> label shape:{len(label)}, target shape:{target.shape}\")\n",
    "#     print(f\"loss: {loss}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d41ac",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e14cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "#         print(x_input.shape)\n",
    "        lstm_out, self.hidden = self.lstm(x_input, encoder_hidden_states)\n",
    "#         print(f\"lstm_out shape:{lstm_out.shape}\") #lstm_out.shape->(1,1,768)\n",
    "        output = self.linear(lstm_out)     \n",
    "        \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e3a4bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_decoder(\n",
       "  (lstm): LSTM(37, 768, batch_first=True)\n",
       "  (linear): Linear(in_features=768, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder=lstm_decoder(input_size=37, hidden_size=768)\n",
    "decoder.to(device)\n",
    "# torchsummary.summary(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83f4e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supermodel turned actress turned horse\n"
     ]
    }
   ],
   "source": [
    "img, des, label = train_dataset[0]\n",
    "print(des)\n",
    "image=img.unsqueeze(0).to(device)\n",
    "img_embed=Image_Enc(image).unsqueeze(0)\n",
    "sen_embed=Text_Enc(des).unsqueeze(0).unsqueeze(0)\n",
    "merged_embed=img_embed+sen_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8349e9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 448, 448])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe417223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c341b18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb90fae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e389faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_c=torch.zeros(1,1,768).to(device)\n",
    "inputs = torch.zeros(1,1,37).to(device)\n",
    "\n",
    "# input->(1,27), merged_embed->(1,1,768), initial_c->(1,1,768)\n",
    "output,(hidden_state, cell_state)=decoder(inputs,(merged_embed, initial_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84198e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 37])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape) #[1(B), 1, 27]\n",
    "print(hidden_state.shape) #[1, 1(B), 768]\n",
    "print(cell_state.shape) #[1, 1(B), 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09ee86f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset:43864 validset:11340\n",
      "0-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-3.9634e-02,  1.9520e-02,  3.9103e-02,  ..., -1.1104e-02,\n",
      "           1.1740e-02,  9.3796e-02],\n",
      "         [-3.0007e-02, -1.6612e-03,  2.5583e-02,  ...,  1.9044e-02,\n",
      "           4.6704e-03,  5.3334e-02],\n",
      "         [-2.5383e-02, -1.1842e-02,  2.7639e-02,  ...,  2.8665e-02,\n",
      "           8.9096e-03,  3.5920e-02],\n",
      "         ...,\n",
      "         [-2.4674e-02, -2.1827e-02,  3.5728e-02,  ...,  3.1756e-02,\n",
      "           1.5382e-02,  1.0883e-02],\n",
      "         [-2.4801e-02, -2.1698e-02,  3.5886e-02,  ...,  3.1401e-02,\n",
      "           1.5600e-02,  1.0008e-02],\n",
      "         [-2.4855e-02, -2.1561e-02,  3.5955e-02,  ...,  3.1135e-02,\n",
      "           1.5717e-02,  9.4895e-03]],\n",
      "\n",
      "        [[-5.1369e-02,  8.8383e-03, -4.6239e-03,  ...,  8.5014e-03,\n",
      "           6.6959e-03,  3.0605e-02],\n",
      "         [-5.1995e-02, -8.4714e-03,  1.4700e-02,  ...,  2.8090e-02,\n",
      "           1.2753e-02,  2.3119e-02],\n",
      "         [-4.2720e-02, -1.5946e-02,  2.5367e-02,  ...,  3.4538e-02,\n",
      "           1.5901e-02,  1.8152e-02],\n",
      "         ...,\n",
      "         [-2.6420e-02, -2.1170e-02,  3.5502e-02,  ...,  3.2751e-02,\n",
      "           1.6410e-02,  9.4703e-03],\n",
      "         [-2.5736e-02, -2.1139e-02,  3.5671e-02,  ...,  3.2052e-02,\n",
      "           1.6213e-02,  9.1622e-03],\n",
      "         [-2.5325e-02, -2.1121e-02,  3.5772e-02,  ...,  3.1554e-02,\n",
      "           1.6071e-02,  8.9825e-03]],\n",
      "\n",
      "        [[-1.9679e-02, -2.8063e-02,  2.4204e-02,  ...,  3.2560e-02,\n",
      "           1.6011e-02,  1.4971e-02],\n",
      "         [-2.4831e-02, -2.9537e-02,  2.4227e-02,  ...,  3.5037e-02,\n",
      "           1.1211e-02,  1.2016e-02],\n",
      "         [-2.5194e-02, -2.9654e-02,  2.8201e-02,  ...,  3.3654e-02,\n",
      "           1.0130e-02,  1.3408e-02],\n",
      "         ...,\n",
      "         [-2.5015e-02, -2.2716e-02,  3.5779e-02,  ...,  3.0608e-02,\n",
      "           1.4919e-02,  9.7129e-03],\n",
      "         [-2.4957e-02, -2.2161e-02,  3.5913e-02,  ...,  3.0508e-02,\n",
      "           1.5288e-02,  9.3450e-03],\n",
      "         [-2.4904e-02, -2.1801e-02,  3.5971e-02,  ...,  3.0474e-02,\n",
      "           1.5512e-02,  9.1092e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.9198e-02, -3.9729e-02, -3.3298e-02,  ..., -2.2723e-02,\n",
      "           6.1618e-02, -4.6166e-02],\n",
      "         [-3.4810e-02, -4.0630e-02,  2.4988e-03,  ...,  1.1268e-02,\n",
      "           3.9923e-02, -1.4851e-02],\n",
      "         [-3.1847e-02, -3.6801e-02,  2.1414e-02,  ...,  2.4850e-02,\n",
      "           2.6721e-02, -2.1150e-05],\n",
      "         ...,\n",
      "         [-2.5633e-02, -2.3404e-02,  3.5917e-02,  ...,  3.0866e-02,\n",
      "           1.6829e-02,  9.0302e-03],\n",
      "         [-2.5294e-02, -2.2551e-02,  3.6027e-02,  ...,  3.0716e-02,\n",
      "           1.6496e-02,  8.9209e-03],\n",
      "         [-2.5073e-02, -2.2009e-02,  3.6059e-02,  ...,  3.0628e-02,\n",
      "           1.6263e-02,  8.8433e-03]],\n",
      "\n",
      "        [[-3.0760e-02,  2.4005e-02, -1.1118e-03,  ..., -8.4384e-03,\n",
      "           7.3312e-02,  1.0896e-01],\n",
      "         [-3.0947e-02, -4.2085e-03,  3.3142e-03,  ...,  1.5153e-02,\n",
      "           4.6938e-02,  6.3907e-02],\n",
      "         [-2.8934e-02, -1.6754e-02,  1.4726e-02,  ...,  2.1894e-02,\n",
      "           3.4842e-02,  4.0684e-02],\n",
      "         ...,\n",
      "         [-2.5122e-02, -2.2222e-02,  3.5067e-02,  ...,  2.9999e-02,\n",
      "           1.7135e-02,  1.0219e-02],\n",
      "         [-2.4990e-02, -2.1911e-02,  3.5571e-02,  ...,  3.0270e-02,\n",
      "           1.6613e-02,  9.4491e-03],\n",
      "         [-2.4907e-02, -2.1679e-02,  3.5821e-02,  ...,  3.0421e-02,\n",
      "           1.6304e-02,  9.0523e-03]],\n",
      "\n",
      "        [[-6.2449e-02,  3.4468e-03, -2.7545e-02,  ...,  9.3213e-04,\n",
      "           4.7278e-02, -5.9607e-02],\n",
      "         [-5.2315e-02, -1.2833e-02, -1.3510e-03,  ...,  1.8362e-02,\n",
      "           2.8827e-02, -2.4818e-02],\n",
      "         [-4.4098e-02, -1.6646e-02,  1.3592e-02,  ...,  2.5048e-02,\n",
      "           2.1060e-02, -1.0071e-03],\n",
      "         ...,\n",
      "         [-2.7348e-02, -2.0742e-02,  3.4788e-02,  ...,  3.0506e-02,\n",
      "           1.5718e-02,  1.0468e-02],\n",
      "         [-2.6390e-02, -2.0854e-02,  3.5361e-02,  ...,  3.0545e-02,\n",
      "           1.5728e-02,  9.8917e-03],\n",
      "         [-2.5772e-02, -2.0942e-02,  3.5675e-02,  ...,  3.0566e-02,\n",
      "           1.5745e-02,  9.4711e-03]]], grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0396,  0.0195,  0.0391,  ...,  0.0311,  0.0157,  0.0095],\n",
      "        [-0.0514,  0.0088, -0.0046,  ...,  0.0316,  0.0161,  0.0090],\n",
      "        [-0.0197, -0.0281,  0.0242,  ...,  0.0305,  0.0155,  0.0091],\n",
      "        ...,\n",
      "        [-0.0292, -0.0397, -0.0333,  ...,  0.0306,  0.0163,  0.0088],\n",
      "        [-0.0308,  0.0240, -0.0011,  ...,  0.0304,  0.0163,  0.0091],\n",
      "        [-0.0624,  0.0034, -0.0275,  ...,  0.0306,  0.0157,  0.0095]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "1-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0207, -0.0114,  0.0542,  ..., -0.0457, -0.0009,  0.0793],\n",
      "         [-0.0272, -0.0211,  0.0317,  ..., -0.0048,  0.0057,  0.0381],\n",
      "         [-0.0260, -0.0243,  0.0311,  ...,  0.0133,  0.0113,  0.0226],\n",
      "         ...,\n",
      "         [-0.0252, -0.0224,  0.0363,  ...,  0.0313,  0.0163,  0.0098],\n",
      "         [-0.0251, -0.0220,  0.0363,  ...,  0.0312,  0.0162,  0.0094],\n",
      "         [-0.0250, -0.0217,  0.0363,  ...,  0.0311,  0.0161,  0.0091]],\n",
      "\n",
      "        [[-0.0585, -0.0090,  0.0372,  ...,  0.0020,  0.0221,  0.0477],\n",
      "         [-0.0435, -0.0162,  0.0268,  ...,  0.0171,  0.0203,  0.0258],\n",
      "         [-0.0343, -0.0184,  0.0283,  ...,  0.0242,  0.0171,  0.0192],\n",
      "         ...,\n",
      "         [-0.0258, -0.0215,  0.0355,  ...,  0.0310,  0.0156,  0.0093],\n",
      "         [-0.0255, -0.0215,  0.0358,  ...,  0.0310,  0.0157,  0.0090],\n",
      "         [-0.0253, -0.0214,  0.0360,  ...,  0.0309,  0.0157,  0.0088]],\n",
      "\n",
      "        [[ 0.0002, -0.0387,  0.0164,  ...,  0.0385,  0.0235,  0.0207],\n",
      "         [-0.0197, -0.0409,  0.0239,  ...,  0.0338,  0.0173,  0.0165],\n",
      "         [-0.0249, -0.0372,  0.0316,  ...,  0.0347,  0.0146,  0.0173],\n",
      "         ...,\n",
      "         [-0.0257, -0.0236,  0.0366,  ...,  0.0316,  0.0153,  0.0100],\n",
      "         [-0.0254, -0.0227,  0.0364,  ...,  0.0313,  0.0155,  0.0095],\n",
      "         [-0.0251, -0.0222,  0.0363,  ...,  0.0310,  0.0156,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0603,  0.0399, -0.0489,  ...,  0.0192,  0.0278, -0.0263],\n",
      "         [-0.0452,  0.0089, -0.0148,  ...,  0.0206,  0.0188, -0.0120],\n",
      "         [-0.0365, -0.0101,  0.0071,  ...,  0.0269,  0.0154, -0.0025],\n",
      "         ...,\n",
      "         [-0.0249, -0.0221,  0.0346,  ...,  0.0313,  0.0152,  0.0084],\n",
      "         [-0.0247, -0.0219,  0.0353,  ...,  0.0311,  0.0154,  0.0086],\n",
      "         [-0.0247, -0.0216,  0.0356,  ...,  0.0310,  0.0156,  0.0087]],\n",
      "\n",
      "        [[-0.0736,  0.0007,  0.0006,  ..., -0.0319,  0.0742, -0.0715],\n",
      "         [-0.0542, -0.0242,  0.0231,  ...,  0.0079,  0.0543, -0.0234],\n",
      "         [-0.0408, -0.0286,  0.0318,  ...,  0.0241,  0.0384, -0.0018],\n",
      "         ...,\n",
      "         [-0.0259, -0.0228,  0.0357,  ...,  0.0316,  0.0167,  0.0093],\n",
      "         [-0.0254, -0.0222,  0.0358,  ...,  0.0312,  0.0163,  0.0091],\n",
      "         [-0.0251, -0.0218,  0.0359,  ...,  0.0310,  0.0160,  0.0090]],\n",
      "\n",
      "        [[-0.0468, -0.0507,  0.0183,  ..., -0.0426,  0.0180,  0.0605],\n",
      "         [-0.0436, -0.0445,  0.0227,  ...,  0.0038,  0.0129,  0.0374],\n",
      "         [-0.0376, -0.0370,  0.0303,  ...,  0.0183,  0.0153,  0.0260],\n",
      "         ...,\n",
      "         [-0.0260, -0.0228,  0.0367,  ...,  0.0310,  0.0164,  0.0098],\n",
      "         [-0.0255, -0.0221,  0.0365,  ...,  0.0310,  0.0162,  0.0093],\n",
      "         [-0.0252, -0.0217,  0.0363,  ...,  0.0309,  0.0161,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0207, -0.0114,  0.0542,  ...,  0.0311,  0.0161,  0.0091],\n",
      "        [-0.0585, -0.0090,  0.0372,  ...,  0.0309,  0.0157,  0.0088],\n",
      "        [ 0.0002, -0.0387,  0.0164,  ...,  0.0310,  0.0156,  0.0092],\n",
      "        ...,\n",
      "        [-0.0603,  0.0399, -0.0489,  ...,  0.0310,  0.0156,  0.0087],\n",
      "        [-0.0736,  0.0007,  0.0006,  ...,  0.0310,  0.0160,  0.0090],\n",
      "        [-0.0468, -0.0507,  0.0183,  ...,  0.0309,  0.0161,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0206, -0.0045, -0.0035,  ...,  0.0069,  0.0123, -0.0292],\n",
      "         [-0.0205, -0.0110,  0.0183,  ...,  0.0185,  0.0051, -0.0076],\n",
      "         [-0.0210, -0.0189,  0.0296,  ...,  0.0243,  0.0055,  0.0025],\n",
      "         ...,\n",
      "         [-0.0243, -0.0230,  0.0366,  ...,  0.0308,  0.0150,  0.0090],\n",
      "         [-0.0245, -0.0225,  0.0364,  ...,  0.0307,  0.0155,  0.0090],\n",
      "         [-0.0246, -0.0221,  0.0362,  ...,  0.0307,  0.0157,  0.0089]],\n",
      "\n",
      "        [[-0.0660, -0.0238,  0.0591,  ..., -0.0402,  0.0224, -0.0106],\n",
      "         [-0.0542, -0.0288,  0.0467,  ...,  0.0037,  0.0174,  0.0093],\n",
      "         [-0.0444, -0.0269,  0.0428,  ...,  0.0212,  0.0132,  0.0136],\n",
      "         ...,\n",
      "         [-0.0268, -0.0216,  0.0360,  ...,  0.0319,  0.0156,  0.0098],\n",
      "         [-0.0260, -0.0214,  0.0359,  ...,  0.0315,  0.0158,  0.0093],\n",
      "         [-0.0255, -0.0213,  0.0359,  ...,  0.0312,  0.0159,  0.0091]],\n",
      "\n",
      "        [[-0.0655,  0.0192, -0.0371,  ...,  0.0384,  0.0844,  0.0694],\n",
      "         [-0.0638, -0.0031, -0.0130,  ...,  0.0340,  0.0499,  0.0381],\n",
      "         [-0.0506, -0.0125,  0.0095,  ...,  0.0342,  0.0333,  0.0292],\n",
      "         ...,\n",
      "         [-0.0266, -0.0210,  0.0353,  ...,  0.0321,  0.0159,  0.0104],\n",
      "         [-0.0258, -0.0212,  0.0358,  ...,  0.0316,  0.0157,  0.0096],\n",
      "         [-0.0254, -0.0212,  0.0360,  ...,  0.0313,  0.0157,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0571, -0.0455,  0.0133,  ..., -0.0284,  0.0216, -0.0257],\n",
      "         [-0.0398, -0.0413,  0.0250,  ...,  0.0015,  0.0204, -0.0077],\n",
      "         [-0.0319, -0.0357,  0.0319,  ...,  0.0152,  0.0191,  0.0019],\n",
      "         ...,\n",
      "         [-0.0252, -0.0230,  0.0365,  ...,  0.0299,  0.0161,  0.0089],\n",
      "         [-0.0251, -0.0223,  0.0364,  ...,  0.0302,  0.0159,  0.0088],\n",
      "         [-0.0250, -0.0219,  0.0363,  ...,  0.0304,  0.0159,  0.0088]],\n",
      "\n",
      "        [[-0.0736, -0.0448, -0.0205,  ..., -0.0545,  0.0721, -0.0349],\n",
      "         [-0.0626, -0.0416,  0.0059,  ..., -0.0132,  0.0434, -0.0157],\n",
      "         [-0.0479, -0.0367,  0.0197,  ...,  0.0083,  0.0302, -0.0035],\n",
      "         ...,\n",
      "         [-0.0264, -0.0222,  0.0350,  ...,  0.0301,  0.0168,  0.0083],\n",
      "         [-0.0257, -0.0217,  0.0354,  ...,  0.0304,  0.0164,  0.0085],\n",
      "         [-0.0253, -0.0215,  0.0356,  ...,  0.0306,  0.0162,  0.0086]],\n",
      "\n",
      "        [[-0.0412,  0.0090, -0.0274,  ..., -0.0161,  0.0588,  0.0088],\n",
      "         [-0.0398, -0.0213,  0.0089,  ..., -0.0013,  0.0304,  0.0110],\n",
      "         [-0.0337, -0.0283,  0.0253,  ...,  0.0102,  0.0208,  0.0132],\n",
      "         ...,\n",
      "         [-0.0254, -0.0226,  0.0360,  ...,  0.0295,  0.0160,  0.0100],\n",
      "         [-0.0252, -0.0220,  0.0361,  ...,  0.0300,  0.0160,  0.0095],\n",
      "         [-0.0251, -0.0217,  0.0361,  ...,  0.0303,  0.0159,  0.0092]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0206, -0.0045, -0.0035,  ...,  0.0307,  0.0157,  0.0089],\n",
      "        [-0.0660, -0.0238,  0.0591,  ...,  0.0312,  0.0159,  0.0091],\n",
      "        [-0.0655,  0.0192, -0.0371,  ...,  0.0313,  0.0157,  0.0092],\n",
      "        ...,\n",
      "        [-0.0571, -0.0455,  0.0133,  ...,  0.0304,  0.0159,  0.0088],\n",
      "        [-0.0736, -0.0448, -0.0205,  ...,  0.0306,  0.0162,  0.0086],\n",
      "        [-0.0412,  0.0090, -0.0274,  ...,  0.0303,  0.0159,  0.0092]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "3-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0867,  0.0095, -0.1124,  ..., -0.0072,  0.0048, -0.0235],\n",
      "         [-0.0635, -0.0067, -0.0466,  ...,  0.0082,  0.0113, -0.0053],\n",
      "         [-0.0456, -0.0147, -0.0069,  ...,  0.0185,  0.0142,  0.0049],\n",
      "         ...,\n",
      "         [-0.0255, -0.0216,  0.0352,  ...,  0.0308,  0.0160,  0.0095],\n",
      "         [-0.0252, -0.0216,  0.0358,  ...,  0.0309,  0.0160,  0.0092],\n",
      "         [-0.0250, -0.0215,  0.0360,  ...,  0.0309,  0.0159,  0.0090]],\n",
      "\n",
      "        [[-0.0801,  0.0330, -0.0158,  ..., -0.0804,  0.0269, -0.0033],\n",
      "         [-0.0455,  0.0051,  0.0081,  ..., -0.0289,  0.0166,  0.0047],\n",
      "         [-0.0338, -0.0100,  0.0236,  ...,  0.0006,  0.0141,  0.0097],\n",
      "         ...,\n",
      "         [-0.0264, -0.0213,  0.0366,  ...,  0.0298,  0.0158,  0.0092],\n",
      "         [-0.0260, -0.0213,  0.0365,  ...,  0.0302,  0.0159,  0.0090],\n",
      "         [-0.0256, -0.0213,  0.0364,  ...,  0.0304,  0.0159,  0.0088]],\n",
      "\n",
      "        [[-0.0282, -0.0347, -0.0455,  ...,  0.0230,  0.0473, -0.0316],\n",
      "         [-0.0376, -0.0309, -0.0088,  ...,  0.0234,  0.0240, -0.0247],\n",
      "         [-0.0354, -0.0279,  0.0131,  ...,  0.0256,  0.0171, -0.0115],\n",
      "         ...,\n",
      "         [-0.0259, -0.0221,  0.0357,  ...,  0.0303,  0.0155,  0.0076],\n",
      "         [-0.0254, -0.0218,  0.0360,  ...,  0.0304,  0.0157,  0.0081],\n",
      "         [-0.0251, -0.0216,  0.0361,  ...,  0.0305,  0.0158,  0.0084]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0155, -0.0608, -0.0224,  ..., -0.0103,  0.0661,  0.0406],\n",
      "         [-0.0279, -0.0433, -0.0035,  ...,  0.0137,  0.0318,  0.0258],\n",
      "         [-0.0288, -0.0338,  0.0132,  ...,  0.0243,  0.0205,  0.0205],\n",
      "         ...,\n",
      "         [-0.0257, -0.0220,  0.0350,  ...,  0.0314,  0.0155,  0.0098],\n",
      "         [-0.0254, -0.0217,  0.0355,  ...,  0.0311,  0.0156,  0.0093],\n",
      "         [-0.0252, -0.0215,  0.0357,  ...,  0.0309,  0.0157,  0.0091]],\n",
      "\n",
      "        [[-0.0622,  0.0109, -0.0206,  ...,  0.0326,  0.0286,  0.0531],\n",
      "         [-0.0569, -0.0106,  0.0002,  ...,  0.0339,  0.0177,  0.0294],\n",
      "         [-0.0447, -0.0181,  0.0128,  ...,  0.0338,  0.0140,  0.0204],\n",
      "         ...,\n",
      "         [-0.0262, -0.0212,  0.0341,  ...,  0.0317,  0.0152,  0.0098],\n",
      "         [-0.0256, -0.0212,  0.0349,  ...,  0.0314,  0.0154,  0.0094],\n",
      "         [-0.0253, -0.0212,  0.0354,  ...,  0.0312,  0.0156,  0.0091]],\n",
      "\n",
      "        [[-0.0346, -0.0554,  0.0425,  ...,  0.0464, -0.0160, -0.0013],\n",
      "         [-0.0470, -0.0418,  0.0319,  ...,  0.0455, -0.0082,  0.0024],\n",
      "         [-0.0411, -0.0347,  0.0314,  ...,  0.0406,  0.0007,  0.0055],\n",
      "         ...,\n",
      "         [-0.0264, -0.0230,  0.0362,  ...,  0.0317,  0.0151,  0.0082],\n",
      "         [-0.0258, -0.0223,  0.0362,  ...,  0.0313,  0.0155,  0.0083],\n",
      "         [-0.0254, -0.0219,  0.0362,  ...,  0.0310,  0.0157,  0.0084]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0867,  0.0095, -0.1124,  ...,  0.0309,  0.0159,  0.0090],\n",
      "        [-0.0801,  0.0330, -0.0158,  ...,  0.0304,  0.0159,  0.0088],\n",
      "        [-0.0282, -0.0347, -0.0455,  ...,  0.0305,  0.0158,  0.0084],\n",
      "        ...,\n",
      "        [-0.0155, -0.0608, -0.0224,  ...,  0.0309,  0.0157,  0.0091],\n",
      "        [-0.0622,  0.0109, -0.0206,  ...,  0.0312,  0.0156,  0.0091],\n",
      "        [-0.0346, -0.0554,  0.0425,  ...,  0.0310,  0.0157,  0.0084]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "error: image name=>jessicaalba_540.jpg des name=>jessicaalba_540.txt label name=>jessicaalba_540.txt\n",
      "4-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0426,  0.0186,  0.0543,  ..., -0.0235,  0.0099,  0.0376],\n",
      "         [-0.0384, -0.0147,  0.0290,  ...,  0.0113,  0.0128,  0.0239],\n",
      "         [-0.0343, -0.0199,  0.0314,  ...,  0.0251,  0.0146,  0.0207],\n",
      "         ...,\n",
      "         [-0.0260, -0.0221,  0.0363,  ...,  0.0330,  0.0163,  0.0096],\n",
      "         [-0.0256, -0.0219,  0.0362,  ...,  0.0323,  0.0162,  0.0092],\n",
      "         [-0.0253, -0.0217,  0.0361,  ...,  0.0318,  0.0161,  0.0089]],\n",
      "\n",
      "        [[-0.0692, -0.0364,  0.0113,  ..., -0.0202,  0.0054,  0.0759],\n",
      "         [-0.0571, -0.0327,  0.0165,  ...,  0.0008,  0.0086,  0.0428],\n",
      "         [-0.0462, -0.0305,  0.0244,  ...,  0.0149,  0.0117,  0.0279],\n",
      "         ...,\n",
      "         [-0.0271, -0.0224,  0.0352,  ...,  0.0302,  0.0161,  0.0100],\n",
      "         [-0.0263, -0.0219,  0.0355,  ...,  0.0304,  0.0161,  0.0094],\n",
      "         [-0.0257, -0.0216,  0.0357,  ...,  0.0305,  0.0161,  0.0091]],\n",
      "\n",
      "        [[-0.0478,  0.0485, -0.0208,  ..., -0.0130,  0.0011,  0.0979],\n",
      "         [-0.0444,  0.0063, -0.0024,  ...,  0.0137,  0.0175,  0.0544],\n",
      "         [-0.0384, -0.0104,  0.0124,  ...,  0.0226,  0.0208,  0.0350],\n",
      "         ...,\n",
      "         [-0.0267, -0.0213,  0.0346,  ...,  0.0312,  0.0167,  0.0096],\n",
      "         [-0.0260, -0.0213,  0.0353,  ...,  0.0311,  0.0163,  0.0091],\n",
      "         [-0.0255, -0.0213,  0.0356,  ...,  0.0309,  0.0161,  0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1034,  0.0306,  0.0187,  ..., -0.0057,  0.0364,  0.0222],\n",
      "         [-0.0646, -0.0048,  0.0217,  ...,  0.0138,  0.0274,  0.0170],\n",
      "         [-0.0449, -0.0153,  0.0261,  ...,  0.0235,  0.0224,  0.0157],\n",
      "         ...,\n",
      "         [-0.0260, -0.0221,  0.0352,  ...,  0.0319,  0.0162,  0.0093],\n",
      "         [-0.0256, -0.0219,  0.0356,  ...,  0.0316,  0.0160,  0.0090],\n",
      "         [-0.0253, -0.0217,  0.0358,  ...,  0.0313,  0.0159,  0.0089]],\n",
      "\n",
      "        [[-0.0575, -0.0613,  0.0301,  ...,  0.0724,  0.0266,  0.0363],\n",
      "         [-0.0487, -0.0417,  0.0252,  ...,  0.0511,  0.0210,  0.0266],\n",
      "         [-0.0403, -0.0322,  0.0287,  ...,  0.0396,  0.0177,  0.0223],\n",
      "         ...,\n",
      "         [-0.0263, -0.0219,  0.0360,  ...,  0.0303,  0.0160,  0.0105],\n",
      "         [-0.0257, -0.0216,  0.0361,  ...,  0.0303,  0.0159,  0.0098],\n",
      "         [-0.0253, -0.0214,  0.0361,  ...,  0.0304,  0.0159,  0.0094]],\n",
      "\n",
      "        [[-0.0902, -0.0274,  0.0321,  ...,  0.0090,  0.0205,  0.0558],\n",
      "         [-0.0551, -0.0195,  0.0292,  ...,  0.0193,  0.0149,  0.0368],\n",
      "         [-0.0363, -0.0203,  0.0300,  ...,  0.0254,  0.0151,  0.0256],\n",
      "         ...,\n",
      "         [-0.0246, -0.0210,  0.0355,  ...,  0.0307,  0.0159,  0.0095],\n",
      "         [-0.0246, -0.0210,  0.0357,  ...,  0.0308,  0.0159,  0.0091],\n",
      "         [-0.0247, -0.0211,  0.0358,  ...,  0.0308,  0.0159,  0.0089]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0426,  0.0186,  0.0543,  ...,  0.0318,  0.0161,  0.0089],\n",
      "        [-0.0692, -0.0364,  0.0113,  ...,  0.0305,  0.0161,  0.0091],\n",
      "        [-0.0478,  0.0485, -0.0208,  ...,  0.0309,  0.0161,  0.0089],\n",
      "        ...,\n",
      "        [-0.1034,  0.0306,  0.0187,  ...,  0.0313,  0.0159,  0.0089],\n",
      "        [-0.0575, -0.0613,  0.0301,  ...,  0.0304,  0.0159,  0.0094],\n",
      "        [-0.0902, -0.0274,  0.0321,  ...,  0.0308,  0.0159,  0.0089]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0090,  0.0038,  0.0470,  ...,  0.0091,  0.0461, -0.0220],\n",
      "         [-0.0213, -0.0253,  0.0400,  ...,  0.0188,  0.0291, -0.0054],\n",
      "         [-0.0212, -0.0298,  0.0350,  ...,  0.0286,  0.0200,  0.0043],\n",
      "         ...,\n",
      "         [-0.0249, -0.0229,  0.0352,  ...,  0.0323,  0.0157,  0.0091],\n",
      "         [-0.0249, -0.0222,  0.0355,  ...,  0.0318,  0.0158,  0.0090],\n",
      "         [-0.0249, -0.0218,  0.0357,  ...,  0.0314,  0.0158,  0.0089]],\n",
      "\n",
      "        [[-0.0870,  0.0176, -0.0320,  ...,  0.0006,  0.0283,  0.0026],\n",
      "         [-0.0601, -0.0117, -0.0252,  ...,  0.0250,  0.0250,  0.0053],\n",
      "         [-0.0455, -0.0204, -0.0023,  ...,  0.0304,  0.0174,  0.0124],\n",
      "         ...,\n",
      "         [-0.0263, -0.0218,  0.0345,  ...,  0.0329,  0.0156,  0.0099],\n",
      "         [-0.0258, -0.0215,  0.0353,  ...,  0.0323,  0.0158,  0.0095],\n",
      "         [-0.0254, -0.0214,  0.0357,  ...,  0.0318,  0.0158,  0.0092]],\n",
      "\n",
      "        [[-0.0595, -0.0490, -0.0124,  ..., -0.0218,  0.0615,  0.0375],\n",
      "         [-0.0457, -0.0349,  0.0141,  ...,  0.0069,  0.0400,  0.0217],\n",
      "         [-0.0367, -0.0287,  0.0253,  ...,  0.0214,  0.0271,  0.0156],\n",
      "         ...,\n",
      "         [-0.0250, -0.0217,  0.0362,  ...,  0.0317,  0.0153,  0.0089],\n",
      "         [-0.0248, -0.0215,  0.0362,  ...,  0.0314,  0.0153,  0.0088],\n",
      "         [-0.0248, -0.0214,  0.0362,  ...,  0.0312,  0.0155,  0.0087]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0853, -0.0315, -0.0695,  ..., -0.0030,  0.0612, -0.0126],\n",
      "         [-0.0624, -0.0352, -0.0181,  ...,  0.0187,  0.0341, -0.0014],\n",
      "         [-0.0454, -0.0322,  0.0077,  ...,  0.0267,  0.0254,  0.0063],\n",
      "         ...,\n",
      "         [-0.0260, -0.0225,  0.0350,  ...,  0.0305,  0.0162,  0.0093],\n",
      "         [-0.0255, -0.0220,  0.0355,  ...,  0.0305,  0.0160,  0.0090],\n",
      "         [-0.0252, -0.0217,  0.0358,  ...,  0.0306,  0.0159,  0.0089]],\n",
      "\n",
      "        [[-0.0740, -0.0725, -0.0009,  ...,  0.0110,  0.0576,  0.0171],\n",
      "         [-0.0609, -0.0470,  0.0156,  ...,  0.0248,  0.0395,  0.0057],\n",
      "         [-0.0475, -0.0345,  0.0263,  ...,  0.0296,  0.0292,  0.0061],\n",
      "         ...,\n",
      "         [-0.0262, -0.0218,  0.0362,  ...,  0.0312,  0.0172,  0.0087],\n",
      "         [-0.0256, -0.0215,  0.0362,  ...,  0.0310,  0.0167,  0.0087],\n",
      "         [-0.0252, -0.0214,  0.0362,  ...,  0.0309,  0.0164,  0.0087]],\n",
      "\n",
      "        [[-0.1524,  0.0199,  0.0287,  ..., -0.0344, -0.0052,  0.0208],\n",
      "         [-0.0905, -0.0128,  0.0215,  ..., -0.0206,  0.0141,  0.0157],\n",
      "         [-0.0580, -0.0225,  0.0267,  ..., -0.0014,  0.0175,  0.0133],\n",
      "         ...,\n",
      "         [-0.0267, -0.0223,  0.0362,  ...,  0.0288,  0.0161,  0.0096],\n",
      "         [-0.0259, -0.0219,  0.0362,  ...,  0.0296,  0.0160,  0.0093],\n",
      "         [-0.0255, -0.0217,  0.0362,  ...,  0.0301,  0.0159,  0.0091]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0090,  0.0038,  0.0470,  ...,  0.0314,  0.0158,  0.0089],\n",
      "        [-0.0870,  0.0176, -0.0320,  ...,  0.0318,  0.0158,  0.0092],\n",
      "        [-0.0595, -0.0490, -0.0124,  ...,  0.0312,  0.0155,  0.0087],\n",
      "        ...,\n",
      "        [-0.0853, -0.0315, -0.0695,  ...,  0.0306,  0.0159,  0.0089],\n",
      "        [-0.0740, -0.0725, -0.0009,  ...,  0.0309,  0.0164,  0.0087],\n",
      "        [-0.1524,  0.0199,  0.0287,  ...,  0.0301,  0.0159,  0.0091]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "6-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0369, -0.0663,  0.0455,  ...,  0.0402,  0.0235,  0.0411],\n",
      "         [-0.0378, -0.0510,  0.0382,  ...,  0.0405,  0.0083,  0.0276],\n",
      "         [-0.0359, -0.0408,  0.0357,  ...,  0.0381,  0.0063,  0.0248],\n",
      "         ...,\n",
      "         [-0.0266, -0.0224,  0.0357,  ...,  0.0313,  0.0144,  0.0102],\n",
      "         [-0.0259, -0.0218,  0.0358,  ...,  0.0310,  0.0150,  0.0095],\n",
      "         [-0.0255, -0.0215,  0.0359,  ...,  0.0308,  0.0153,  0.0091]],\n",
      "\n",
      "        [[-0.0932, -0.0212, -0.0025,  ...,  0.0163,  0.0274,  0.0054],\n",
      "         [-0.0611, -0.0255,  0.0054,  ...,  0.0202,  0.0181, -0.0039],\n",
      "         [-0.0434, -0.0229,  0.0180,  ...,  0.0269,  0.0162,  0.0002],\n",
      "         ...,\n",
      "         [-0.0254, -0.0212,  0.0355,  ...,  0.0320,  0.0157,  0.0079],\n",
      "         [-0.0251, -0.0212,  0.0358,  ...,  0.0316,  0.0157,  0.0082],\n",
      "         [-0.0249, -0.0212,  0.0360,  ...,  0.0313,  0.0158,  0.0084]],\n",
      "\n",
      "        [[-0.0182,  0.0089, -0.0436,  ...,  0.0265,  0.0133,  0.0181],\n",
      "         [-0.0125, -0.0128, -0.0048,  ...,  0.0181,  0.0137,  0.0237],\n",
      "         [-0.0163, -0.0223,  0.0150,  ...,  0.0243,  0.0127,  0.0201],\n",
      "         ...,\n",
      "         [-0.0243, -0.0223,  0.0350,  ...,  0.0306,  0.0158,  0.0098],\n",
      "         [-0.0245, -0.0219,  0.0355,  ...,  0.0306,  0.0159,  0.0093],\n",
      "         [-0.0247, -0.0217,  0.0357,  ...,  0.0306,  0.0159,  0.0091]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0496, -0.0600, -0.0003,  ..., -0.0017,  0.0131, -0.0242],\n",
      "         [-0.0446, -0.0471,  0.0116,  ...,  0.0133,  0.0166, -0.0054],\n",
      "         [-0.0356, -0.0377,  0.0219,  ...,  0.0265,  0.0161,  0.0045],\n",
      "         ...,\n",
      "         [-0.0263, -0.0224,  0.0347,  ...,  0.0317,  0.0159,  0.0090],\n",
      "         [-0.0258, -0.0218,  0.0352,  ...,  0.0314,  0.0159,  0.0089],\n",
      "         [-0.0255, -0.0215,  0.0355,  ...,  0.0311,  0.0159,  0.0088]],\n",
      "\n",
      "        [[-0.0382, -0.0473,  0.0052,  ..., -0.0080,  0.0535,  0.0200],\n",
      "         [-0.0474, -0.0333,  0.0174,  ...,  0.0112,  0.0316,  0.0113],\n",
      "         [-0.0416, -0.0277,  0.0251,  ...,  0.0203,  0.0223,  0.0113],\n",
      "         ...,\n",
      "         [-0.0262, -0.0218,  0.0360,  ...,  0.0308,  0.0158,  0.0094],\n",
      "         [-0.0256, -0.0216,  0.0361,  ...,  0.0308,  0.0158,  0.0091],\n",
      "         [-0.0252, -0.0215,  0.0361,  ...,  0.0308,  0.0158,  0.0089]],\n",
      "\n",
      "        [[-0.0301, -0.0220, -0.0059,  ..., -0.0233,  0.0022,  0.0135],\n",
      "         [-0.0287, -0.0249,  0.0150,  ...,  0.0058,  0.0052,  0.0050],\n",
      "         [-0.0269, -0.0270,  0.0275,  ...,  0.0214,  0.0084,  0.0080],\n",
      "         ...,\n",
      "         [-0.0249, -0.0226,  0.0365,  ...,  0.0313,  0.0153,  0.0095],\n",
      "         [-0.0249, -0.0221,  0.0364,  ...,  0.0311,  0.0155,  0.0093],\n",
      "         [-0.0248, -0.0218,  0.0363,  ...,  0.0310,  0.0157,  0.0091]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0369, -0.0663,  0.0455,  ...,  0.0308,  0.0153,  0.0091],\n",
      "        [-0.0932, -0.0212, -0.0025,  ...,  0.0313,  0.0158,  0.0084],\n",
      "        [-0.0182,  0.0089, -0.0436,  ...,  0.0306,  0.0159,  0.0091],\n",
      "        ...,\n",
      "        [-0.0496, -0.0600, -0.0003,  ...,  0.0311,  0.0159,  0.0088],\n",
      "        [-0.0382, -0.0473,  0.0052,  ...,  0.0308,  0.0158,  0.0089],\n",
      "        [-0.0301, -0.0220, -0.0059,  ...,  0.0310,  0.0157,  0.0091]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "7-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0398, -0.0248,  0.0297,  ...,  0.0117,  0.0359,  0.0187],\n",
      "         [-0.0367, -0.0225,  0.0354,  ...,  0.0222,  0.0176,  0.0089],\n",
      "         [-0.0334, -0.0216,  0.0347,  ...,  0.0263,  0.0132,  0.0083],\n",
      "         ...,\n",
      "         [-0.0265, -0.0213,  0.0356,  ...,  0.0306,  0.0152,  0.0091],\n",
      "         [-0.0259, -0.0213,  0.0358,  ...,  0.0306,  0.0155,  0.0090],\n",
      "         [-0.0255, -0.0213,  0.0359,  ...,  0.0306,  0.0156,  0.0089]],\n",
      "\n",
      "        [[-0.0331, -0.0565,  0.0339,  ...,  0.0109,  0.0031,  0.0302],\n",
      "         [-0.0359, -0.0439,  0.0301,  ...,  0.0136,  0.0078,  0.0178],\n",
      "         [-0.0332, -0.0354,  0.0337,  ...,  0.0195,  0.0131,  0.0175],\n",
      "         ...,\n",
      "         [-0.0257, -0.0223,  0.0365,  ...,  0.0304,  0.0158,  0.0103],\n",
      "         [-0.0254, -0.0219,  0.0363,  ...,  0.0305,  0.0158,  0.0097],\n",
      "         [-0.0251, -0.0216,  0.0362,  ...,  0.0306,  0.0159,  0.0093]],\n",
      "\n",
      "        [[-0.1090,  0.0161,  0.0255,  ..., -0.0471,  0.0170,  0.0277],\n",
      "         [-0.0693, -0.0038,  0.0151,  ..., -0.0191,  0.0235,  0.0166],\n",
      "         [-0.0476, -0.0140,  0.0222,  ...,  0.0023,  0.0216,  0.0153],\n",
      "         ...,\n",
      "         [-0.0264, -0.0216,  0.0353,  ...,  0.0295,  0.0162,  0.0097],\n",
      "         [-0.0258, -0.0215,  0.0357,  ...,  0.0300,  0.0161,  0.0094],\n",
      "         [-0.0254, -0.0214,  0.0359,  ...,  0.0303,  0.0160,  0.0091]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0172, -0.0446, -0.0243,  ..., -0.0668,  0.0167,  0.0634],\n",
      "         [-0.0207, -0.0421,  0.0002,  ..., -0.0244,  0.0110,  0.0423],\n",
      "         [-0.0238, -0.0356,  0.0166,  ...,  0.0021,  0.0100,  0.0290],\n",
      "         ...,\n",
      "         [-0.0255, -0.0227,  0.0356,  ...,  0.0300,  0.0154,  0.0099],\n",
      "         [-0.0253, -0.0222,  0.0359,  ...,  0.0304,  0.0156,  0.0094],\n",
      "         [-0.0251, -0.0218,  0.0360,  ...,  0.0306,  0.0157,  0.0091]],\n",
      "\n",
      "        [[-0.0390, -0.0193,  0.1284,  ...,  0.0133, -0.0154,  0.0191],\n",
      "         [-0.0413, -0.0360,  0.0818,  ...,  0.0237,  0.0054,  0.0189],\n",
      "         [-0.0376, -0.0357,  0.0575,  ...,  0.0287,  0.0108,  0.0145],\n",
      "         ...,\n",
      "         [-0.0264, -0.0229,  0.0359,  ...,  0.0314,  0.0153,  0.0084],\n",
      "         [-0.0258, -0.0221,  0.0358,  ...,  0.0311,  0.0155,  0.0084],\n",
      "         [-0.0254, -0.0217,  0.0358,  ...,  0.0309,  0.0156,  0.0085]],\n",
      "\n",
      "        [[-0.0798, -0.0189,  0.0531,  ..., -0.0263,  0.0105,  0.0359],\n",
      "         [-0.0661, -0.0136,  0.0238,  ..., -0.0049,  0.0150,  0.0308],\n",
      "         [-0.0498, -0.0151,  0.0274,  ...,  0.0091,  0.0171,  0.0274],\n",
      "         ...,\n",
      "         [-0.0271, -0.0208,  0.0359,  ...,  0.0300,  0.0172,  0.0112],\n",
      "         [-0.0262, -0.0209,  0.0360,  ...,  0.0304,  0.0168,  0.0102],\n",
      "         [-0.0257, -0.0210,  0.0360,  ...,  0.0306,  0.0165,  0.0096]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0398, -0.0248,  0.0297,  ...,  0.0306,  0.0156,  0.0089],\n",
      "        [-0.0331, -0.0565,  0.0339,  ...,  0.0306,  0.0159,  0.0093],\n",
      "        [-0.1090,  0.0161,  0.0255,  ...,  0.0303,  0.0160,  0.0091],\n",
      "        ...,\n",
      "        [-0.0172, -0.0446, -0.0243,  ...,  0.0306,  0.0157,  0.0091],\n",
      "        [-0.0390, -0.0193,  0.1284,  ...,  0.0309,  0.0156,  0.0085],\n",
      "        [-0.0798, -0.0189,  0.0531,  ...,  0.0306,  0.0165,  0.0096]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0698, -0.0233,  0.0073,  ..., -0.0134,  0.0167,  0.0384],\n",
      "         [-0.0513, -0.0212,  0.0141,  ...,  0.0163,  0.0062,  0.0231],\n",
      "         [-0.0396, -0.0225,  0.0211,  ...,  0.0284,  0.0058,  0.0166],\n",
      "         ...,\n",
      "         [-0.0256, -0.0215,  0.0350,  ...,  0.0317,  0.0146,  0.0095],\n",
      "         [-0.0253, -0.0213,  0.0355,  ...,  0.0313,  0.0151,  0.0092],\n",
      "         [-0.0251, -0.0213,  0.0357,  ...,  0.0310,  0.0154,  0.0090]],\n",
      "\n",
      "        [[-0.0983, -0.0321,  0.0134,  ...,  0.0204, -0.0226, -0.0616],\n",
      "         [-0.0560, -0.0355,  0.0219,  ...,  0.0325, -0.0003, -0.0300],\n",
      "         [-0.0396, -0.0324,  0.0288,  ...,  0.0361,  0.0106, -0.0114],\n",
      "         ...,\n",
      "         [-0.0259, -0.0229,  0.0355,  ...,  0.0323,  0.0157,  0.0075],\n",
      "         [-0.0255, -0.0223,  0.0356,  ...,  0.0317,  0.0156,  0.0079],\n",
      "         [-0.0252, -0.0219,  0.0357,  ...,  0.0313,  0.0157,  0.0082]],\n",
      "\n",
      "        [[-0.0634,  0.0589, -0.0308,  ..., -0.0062,  0.0050,  0.0265],\n",
      "         [-0.0564,  0.0101, -0.0116,  ...,  0.0184,  0.0031,  0.0206],\n",
      "         [-0.0474, -0.0107,  0.0058,  ...,  0.0310,  0.0072,  0.0177],\n",
      "         ...,\n",
      "         [-0.0270, -0.0216,  0.0340,  ...,  0.0333,  0.0154,  0.0097],\n",
      "         [-0.0261, -0.0215,  0.0350,  ...,  0.0324,  0.0156,  0.0093],\n",
      "         [-0.0256, -0.0214,  0.0355,  ...,  0.0318,  0.0157,  0.0091]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0412,  0.0321,  0.0070,  ...,  0.0393,  0.0389, -0.0304],\n",
      "         [-0.0419, -0.0018,  0.0163,  ...,  0.0362,  0.0150, -0.0085],\n",
      "         [-0.0372, -0.0140,  0.0251,  ...,  0.0335,  0.0103,  0.0053],\n",
      "         ...,\n",
      "         [-0.0258, -0.0212,  0.0363,  ...,  0.0317,  0.0148,  0.0097],\n",
      "         [-0.0254, -0.0212,  0.0363,  ...,  0.0314,  0.0152,  0.0093],\n",
      "         [-0.0251, -0.0212,  0.0363,  ...,  0.0311,  0.0155,  0.0091]],\n",
      "\n",
      "        [[-0.0470, -0.0603, -0.0030,  ..., -0.0113,  0.0062, -0.0078],\n",
      "         [-0.0372, -0.0465,  0.0229,  ...,  0.0086,  0.0075,  0.0018],\n",
      "         [-0.0297, -0.0395,  0.0328,  ...,  0.0180,  0.0101,  0.0067],\n",
      "         ...,\n",
      "         [-0.0249, -0.0232,  0.0369,  ...,  0.0301,  0.0161,  0.0083],\n",
      "         [-0.0249, -0.0224,  0.0366,  ...,  0.0304,  0.0161,  0.0083],\n",
      "         [-0.0248, -0.0219,  0.0364,  ...,  0.0305,  0.0161,  0.0084]],\n",
      "\n",
      "        [[-0.0306, -0.1111,  0.0356,  ..., -0.0302, -0.0089,  0.0495],\n",
      "         [-0.0223, -0.0751,  0.0344,  ..., -0.0058, -0.0009,  0.0298],\n",
      "         [-0.0200, -0.0539,  0.0380,  ...,  0.0107,  0.0061,  0.0200],\n",
      "         ...,\n",
      "         [-0.0243, -0.0230,  0.0367,  ...,  0.0305,  0.0159,  0.0094],\n",
      "         [-0.0245, -0.0221,  0.0364,  ...,  0.0308,  0.0160,  0.0091],\n",
      "         [-0.0246, -0.0217,  0.0362,  ...,  0.0308,  0.0160,  0.0089]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0698, -0.0233,  0.0073,  ...,  0.0310,  0.0154,  0.0090],\n",
      "        [-0.0983, -0.0321,  0.0134,  ...,  0.0313,  0.0157,  0.0082],\n",
      "        [-0.0634,  0.0589, -0.0308,  ...,  0.0318,  0.0157,  0.0091],\n",
      "        ...,\n",
      "        [-0.0412,  0.0321,  0.0070,  ...,  0.0311,  0.0155,  0.0091],\n",
      "        [-0.0470, -0.0603, -0.0030,  ...,  0.0305,  0.0161,  0.0084],\n",
      "        [-0.0306, -0.1111,  0.0356,  ...,  0.0308,  0.0160,  0.0089]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "9-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0340, -0.0093,  0.0688,  ..., -0.0171,  0.0383,  0.0332],\n",
      "         [-0.0384, -0.0206,  0.0502,  ...,  0.0157,  0.0177,  0.0222],\n",
      "         [-0.0350, -0.0238,  0.0439,  ...,  0.0258,  0.0146,  0.0179],\n",
      "         ...,\n",
      "         [-0.0263, -0.0215,  0.0359,  ...,  0.0317,  0.0159,  0.0091],\n",
      "         [-0.0257, -0.0213,  0.0358,  ...,  0.0314,  0.0160,  0.0089],\n",
      "         [-0.0254, -0.0212,  0.0358,  ...,  0.0312,  0.0160,  0.0087]],\n",
      "\n",
      "        [[-0.1079, -0.0280,  0.0679,  ...,  0.0726, -0.0276,  0.0699],\n",
      "         [-0.0786, -0.0295,  0.0399,  ...,  0.0666,  0.0024,  0.0357],\n",
      "         [-0.0565, -0.0277,  0.0336,  ...,  0.0537,  0.0101,  0.0226],\n",
      "         ...,\n",
      "         [-0.0272, -0.0219,  0.0352,  ...,  0.0328,  0.0162,  0.0096],\n",
      "         [-0.0262, -0.0216,  0.0355,  ...,  0.0320,  0.0162,  0.0092],\n",
      "         [-0.0257, -0.0215,  0.0357,  ...,  0.0315,  0.0161,  0.0090]],\n",
      "\n",
      "        [[-0.0706, -0.0358, -0.0226,  ...,  0.0164,  0.0731,  0.0457],\n",
      "         [-0.0444, -0.0335, -0.0005,  ...,  0.0358,  0.0449,  0.0241],\n",
      "         [-0.0341, -0.0289,  0.0153,  ...,  0.0375,  0.0308,  0.0191],\n",
      "         ...,\n",
      "         [-0.0251, -0.0216,  0.0353,  ...,  0.0322,  0.0163,  0.0101],\n",
      "         [-0.0250, -0.0215,  0.0357,  ...,  0.0317,  0.0161,  0.0096],\n",
      "         [-0.0249, -0.0214,  0.0359,  ...,  0.0313,  0.0160,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0358, -0.0350, -0.0052,  ..., -0.0010,  0.0404,  0.0112],\n",
      "         [-0.0287, -0.0303,  0.0169,  ...,  0.0196,  0.0234,  0.0170],\n",
      "         [-0.0261, -0.0278,  0.0258,  ...,  0.0266,  0.0183,  0.0158],\n",
      "         ...,\n",
      "         [-0.0252, -0.0222,  0.0356,  ...,  0.0307,  0.0157,  0.0093],\n",
      "         [-0.0251, -0.0218,  0.0358,  ...,  0.0307,  0.0157,  0.0090],\n",
      "         [-0.0250, -0.0216,  0.0359,  ...,  0.0306,  0.0157,  0.0089]],\n",
      "\n",
      "        [[-0.0935, -0.0171,  0.0407,  ...,  0.0029,  0.0554,  0.0494],\n",
      "         [-0.0624, -0.0255,  0.0339,  ...,  0.0155,  0.0334,  0.0295],\n",
      "         [-0.0439, -0.0284,  0.0344,  ...,  0.0266,  0.0224,  0.0206],\n",
      "         ...,\n",
      "         [-0.0258, -0.0229,  0.0364,  ...,  0.0315,  0.0158,  0.0093],\n",
      "         [-0.0254, -0.0223,  0.0363,  ...,  0.0312,  0.0158,  0.0090],\n",
      "         [-0.0251, -0.0219,  0.0362,  ...,  0.0309,  0.0159,  0.0088]],\n",
      "\n",
      "        [[-0.0497, -0.0119,  0.0380,  ..., -0.0095,  0.0420, -0.0361],\n",
      "         [-0.0463, -0.0097,  0.0355,  ...,  0.0111,  0.0202, -0.0052],\n",
      "         [-0.0398, -0.0141,  0.0341,  ...,  0.0230,  0.0142,  0.0040],\n",
      "         ...,\n",
      "         [-0.0263, -0.0205,  0.0355,  ...,  0.0309,  0.0155,  0.0087],\n",
      "         [-0.0257, -0.0207,  0.0357,  ...,  0.0308,  0.0157,  0.0087],\n",
      "         [-0.0253, -0.0209,  0.0358,  ...,  0.0307,  0.0158,  0.0087]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0340, -0.0093,  0.0688,  ...,  0.0312,  0.0160,  0.0087],\n",
      "        [-0.1079, -0.0280,  0.0679,  ...,  0.0315,  0.0161,  0.0090],\n",
      "        [-0.0706, -0.0358, -0.0226,  ...,  0.0313,  0.0160,  0.0092],\n",
      "        ...,\n",
      "        [-0.0358, -0.0350, -0.0052,  ...,  0.0306,  0.0157,  0.0089],\n",
      "        [-0.0935, -0.0171,  0.0407,  ...,  0.0309,  0.0159,  0.0088],\n",
      "        [-0.0497, -0.0119,  0.0380,  ...,  0.0307,  0.0158,  0.0087]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "10-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0253, -0.0037, -0.0058,  ..., -0.0110,  0.0409,  0.0002],\n",
      "         [-0.0285, -0.0195,  0.0055,  ...,  0.0132,  0.0206,  0.0128],\n",
      "         [-0.0269, -0.0247,  0.0199,  ...,  0.0236,  0.0168,  0.0167],\n",
      "         ...,\n",
      "         [-0.0255, -0.0228,  0.0359,  ...,  0.0308,  0.0163,  0.0094],\n",
      "         [-0.0253, -0.0222,  0.0361,  ...,  0.0307,  0.0162,  0.0090],\n",
      "         [-0.0251, -0.0218,  0.0361,  ...,  0.0306,  0.0161,  0.0089]],\n",
      "\n",
      "        [[-0.0038, -0.0072,  0.0834,  ...,  0.0053,  0.0286,  0.0365],\n",
      "         [-0.0165, -0.0291,  0.0483,  ...,  0.0197,  0.0125,  0.0266],\n",
      "         [-0.0187, -0.0297,  0.0383,  ...,  0.0259,  0.0101,  0.0225],\n",
      "         ...,\n",
      "         [-0.0246, -0.0223,  0.0355,  ...,  0.0320,  0.0149,  0.0099],\n",
      "         [-0.0248, -0.0219,  0.0357,  ...,  0.0317,  0.0153,  0.0094],\n",
      "         [-0.0248, -0.0216,  0.0358,  ...,  0.0314,  0.0155,  0.0091]],\n",
      "\n",
      "        [[-0.0713,  0.0085,  0.0054,  ...,  0.0521,  0.0248,  0.0383],\n",
      "         [-0.0610, -0.0121,  0.0082,  ...,  0.0418,  0.0165,  0.0218],\n",
      "         [-0.0482, -0.0217,  0.0179,  ...,  0.0399,  0.0133,  0.0177],\n",
      "         ...,\n",
      "         [-0.0265, -0.0221,  0.0346,  ...,  0.0319,  0.0152,  0.0095],\n",
      "         [-0.0258, -0.0217,  0.0352,  ...,  0.0314,  0.0155,  0.0091],\n",
      "         [-0.0254, -0.0215,  0.0356,  ...,  0.0311,  0.0156,  0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0143, -0.0860,  0.0831,  ..., -0.0352,  0.0378,  0.0383],\n",
      "         [-0.0239, -0.0616,  0.0551,  ...,  0.0023,  0.0252,  0.0286],\n",
      "         [-0.0252, -0.0457,  0.0433,  ...,  0.0198,  0.0179,  0.0236],\n",
      "         ...,\n",
      "         [-0.0255, -0.0229,  0.0356,  ...,  0.0312,  0.0155,  0.0096],\n",
      "         [-0.0252, -0.0221,  0.0358,  ...,  0.0310,  0.0156,  0.0092],\n",
      "         [-0.0251, -0.0217,  0.0358,  ...,  0.0309,  0.0157,  0.0089]],\n",
      "\n",
      "        [[-0.0103, -0.0026,  0.0140,  ...,  0.0263,  0.0345,  0.0485],\n",
      "         [-0.0253, -0.0199,  0.0257,  ...,  0.0318,  0.0201,  0.0444],\n",
      "         [-0.0256, -0.0256,  0.0326,  ...,  0.0314,  0.0159,  0.0366],\n",
      "         ...,\n",
      "         [-0.0258, -0.0221,  0.0356,  ...,  0.0309,  0.0159,  0.0120],\n",
      "         [-0.0255, -0.0217,  0.0357,  ...,  0.0308,  0.0160,  0.0107],\n",
      "         [-0.0253, -0.0215,  0.0358,  ...,  0.0307,  0.0160,  0.0099]],\n",
      "\n",
      "        [[-0.0743, -0.0484,  0.0244,  ..., -0.0144,  0.0040,  0.0033],\n",
      "         [-0.0506, -0.0332,  0.0302,  ...,  0.0042,  0.0089,  0.0068],\n",
      "         [-0.0383, -0.0277,  0.0327,  ...,  0.0149,  0.0115,  0.0107],\n",
      "         ...,\n",
      "         [-0.0260, -0.0215,  0.0355,  ...,  0.0298,  0.0156,  0.0096],\n",
      "         [-0.0256, -0.0214,  0.0357,  ...,  0.0302,  0.0157,  0.0093],\n",
      "         [-0.0253, -0.0213,  0.0359,  ...,  0.0304,  0.0158,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0253, -0.0037, -0.0058,  ...,  0.0306,  0.0161,  0.0089],\n",
      "        [-0.0038, -0.0072,  0.0834,  ...,  0.0314,  0.0155,  0.0091],\n",
      "        [-0.0713,  0.0085,  0.0054,  ...,  0.0311,  0.0156,  0.0089],\n",
      "        ...,\n",
      "        [-0.0143, -0.0860,  0.0831,  ...,  0.0309,  0.0157,  0.0089],\n",
      "        [-0.0103, -0.0026,  0.0140,  ...,  0.0307,  0.0160,  0.0099],\n",
      "        [-0.0743, -0.0484,  0.0244,  ...,  0.0304,  0.0158,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0294,  0.0538,  0.0021,  ...,  0.0143,  0.0201,  0.0281],\n",
      "         [-0.0365,  0.0190,  0.0076,  ...,  0.0206,  0.0199,  0.0191],\n",
      "         [-0.0353, -0.0005,  0.0176,  ...,  0.0235,  0.0171,  0.0175],\n",
      "         ...,\n",
      "         [-0.0259, -0.0204,  0.0349,  ...,  0.0303,  0.0156,  0.0102],\n",
      "         [-0.0254, -0.0208,  0.0354,  ...,  0.0305,  0.0157,  0.0097],\n",
      "         [-0.0252, -0.0210,  0.0357,  ...,  0.0306,  0.0157,  0.0093]],\n",
      "\n",
      "        [[-0.0713, -0.0401,  0.0606,  ..., -0.0425,  0.0580,  0.0691],\n",
      "         [-0.0517, -0.0353,  0.0466,  ..., -0.0014,  0.0414,  0.0447],\n",
      "         [-0.0391, -0.0308,  0.0438,  ...,  0.0168,  0.0313,  0.0317],\n",
      "         ...,\n",
      "         [-0.0256, -0.0225,  0.0370,  ...,  0.0303,  0.0168,  0.0106],\n",
      "         [-0.0252, -0.0221,  0.0366,  ...,  0.0304,  0.0163,  0.0097],\n",
      "         [-0.0250, -0.0217,  0.0364,  ...,  0.0305,  0.0161,  0.0093]],\n",
      "\n",
      "        [[-0.1162, -0.0112,  0.0291,  ...,  0.0441,  0.0270,  0.0711],\n",
      "         [-0.0770, -0.0151,  0.0274,  ...,  0.0502,  0.0119,  0.0359],\n",
      "         [-0.0541, -0.0180,  0.0328,  ...,  0.0451,  0.0132,  0.0272],\n",
      "         ...,\n",
      "         [-0.0268, -0.0218,  0.0359,  ...,  0.0326,  0.0162,  0.0107],\n",
      "         [-0.0260, -0.0217,  0.0359,  ...,  0.0318,  0.0161,  0.0099],\n",
      "         [-0.0255, -0.0215,  0.0360,  ...,  0.0313,  0.0160,  0.0094]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0655,  0.0289,  0.0202,  ..., -0.0683, -0.0237,  0.0786],\n",
      "         [-0.0465,  0.0016,  0.0182,  ..., -0.0252,  0.0021,  0.0334],\n",
      "         [-0.0366, -0.0122,  0.0255,  ...,  0.0016,  0.0114,  0.0198],\n",
      "         ...,\n",
      "         [-0.0258, -0.0221,  0.0366,  ...,  0.0306,  0.0162,  0.0093],\n",
      "         [-0.0254, -0.0219,  0.0365,  ...,  0.0308,  0.0160,  0.0090],\n",
      "         [-0.0252, -0.0217,  0.0364,  ...,  0.0308,  0.0159,  0.0089]],\n",
      "\n",
      "        [[ 0.0149, -0.0440,  0.0013,  ...,  0.0364,  0.0220, -0.0035],\n",
      "         [-0.0078, -0.0400,  0.0209,  ...,  0.0286,  0.0185,  0.0058],\n",
      "         [-0.0171, -0.0336,  0.0297,  ...,  0.0279,  0.0183,  0.0125],\n",
      "         ...,\n",
      "         [-0.0251, -0.0227,  0.0362,  ...,  0.0306,  0.0162,  0.0100],\n",
      "         [-0.0251, -0.0222,  0.0362,  ...,  0.0307,  0.0161,  0.0095],\n",
      "         [-0.0250, -0.0218,  0.0362,  ...,  0.0307,  0.0160,  0.0092]],\n",
      "\n",
      "        [[-0.0948, -0.0441,  0.0148,  ...,  0.0156,  0.0407,  0.0097],\n",
      "         [-0.0644, -0.0376,  0.0200,  ...,  0.0116,  0.0233,  0.0090],\n",
      "         [-0.0461, -0.0326,  0.0270,  ...,  0.0167,  0.0189,  0.0116],\n",
      "         ...,\n",
      "         [-0.0257, -0.0218,  0.0359,  ...,  0.0297,  0.0155,  0.0100],\n",
      "         [-0.0252, -0.0215,  0.0361,  ...,  0.0301,  0.0156,  0.0096],\n",
      "         [-0.0250, -0.0213,  0.0361,  ...,  0.0303,  0.0156,  0.0093]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0294,  0.0538,  0.0021,  ...,  0.0306,  0.0157,  0.0093],\n",
      "        [-0.0713, -0.0401,  0.0606,  ...,  0.0305,  0.0161,  0.0093],\n",
      "        [-0.1162, -0.0112,  0.0291,  ...,  0.0313,  0.0160,  0.0094],\n",
      "        ...,\n",
      "        [-0.0655,  0.0289,  0.0202,  ...,  0.0308,  0.0159,  0.0089],\n",
      "        [ 0.0149, -0.0440,  0.0013,  ...,  0.0307,  0.0160,  0.0092],\n",
      "        [-0.0948, -0.0441,  0.0148,  ...,  0.0303,  0.0156,  0.0093]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "12-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0748, -0.0450, -0.0298,  ...,  0.0084,  0.0162,  0.0418],\n",
      "         [-0.0600, -0.0330, -0.0062,  ...,  0.0227,  0.0203,  0.0250],\n",
      "         [-0.0461, -0.0277,  0.0105,  ...,  0.0280,  0.0211,  0.0192],\n",
      "         ...,\n",
      "         [-0.0262, -0.0219,  0.0346,  ...,  0.0309,  0.0164,  0.0098],\n",
      "         [-0.0256, -0.0217,  0.0353,  ...,  0.0308,  0.0161,  0.0093],\n",
      "         [-0.0253, -0.0215,  0.0356,  ...,  0.0308,  0.0160,  0.0091]],\n",
      "\n",
      "        [[-0.0327, -0.0381,  0.0990,  ...,  0.0019,  0.0823,  0.0256],\n",
      "         [-0.0281, -0.0305,  0.0680,  ...,  0.0175,  0.0546,  0.0260],\n",
      "         [-0.0289, -0.0289,  0.0548,  ...,  0.0279,  0.0385,  0.0211],\n",
      "         ...,\n",
      "         [-0.0264, -0.0224,  0.0364,  ...,  0.0315,  0.0172,  0.0092],\n",
      "         [-0.0259, -0.0219,  0.0360,  ...,  0.0312,  0.0166,  0.0088],\n",
      "         [-0.0255, -0.0215,  0.0359,  ...,  0.0310,  0.0163,  0.0087]],\n",
      "\n",
      "        [[-0.0827,  0.0394, -0.0181,  ..., -0.0402,  0.0449, -0.0222],\n",
      "         [-0.0605,  0.0110,  0.0058,  ...,  0.0010,  0.0285, -0.0085],\n",
      "         [-0.0454, -0.0053,  0.0214,  ...,  0.0186,  0.0218,  0.0021],\n",
      "         ...,\n",
      "         [-0.0259, -0.0212,  0.0361,  ...,  0.0307,  0.0160,  0.0097],\n",
      "         [-0.0254, -0.0213,  0.0361,  ...,  0.0307,  0.0159,  0.0094],\n",
      "         [-0.0251, -0.0214,  0.0361,  ...,  0.0307,  0.0159,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0734,  0.0226, -0.0080,  ...,  0.0122,  0.0474,  0.0268],\n",
      "         [-0.0607, -0.0087,  0.0035,  ...,  0.0232,  0.0261,  0.0113],\n",
      "         [-0.0483, -0.0208,  0.0164,  ...,  0.0275,  0.0181,  0.0135],\n",
      "         ...,\n",
      "         [-0.0269, -0.0223,  0.0350,  ...,  0.0305,  0.0155,  0.0095],\n",
      "         [-0.0261, -0.0219,  0.0355,  ...,  0.0305,  0.0157,  0.0091],\n",
      "         [-0.0256, -0.0216,  0.0357,  ...,  0.0305,  0.0158,  0.0090]],\n",
      "\n",
      "        [[-0.0203, -0.0244, -0.0210,  ...,  0.0279,  0.0004, -0.0171],\n",
      "         [-0.0295, -0.0277,  0.0015,  ...,  0.0267,  0.0040, -0.0046],\n",
      "         [-0.0315, -0.0279,  0.0171,  ...,  0.0294,  0.0089,  0.0046],\n",
      "         ...,\n",
      "         [-0.0260, -0.0223,  0.0352,  ...,  0.0310,  0.0158,  0.0095],\n",
      "         [-0.0256, -0.0219,  0.0356,  ...,  0.0309,  0.0159,  0.0092],\n",
      "         [-0.0253, -0.0217,  0.0358,  ...,  0.0308,  0.0159,  0.0090]],\n",
      "\n",
      "        [[-0.0763, -0.0088,  0.0006,  ..., -0.0243,  0.0048,  0.0078],\n",
      "         [-0.0587, -0.0202,  0.0172,  ..., -0.0002,  0.0076,  0.0082],\n",
      "         [-0.0445, -0.0219,  0.0268,  ...,  0.0146,  0.0094,  0.0114],\n",
      "         ...,\n",
      "         [-0.0260, -0.0217,  0.0356,  ...,  0.0308,  0.0152,  0.0092],\n",
      "         [-0.0255, -0.0215,  0.0358,  ...,  0.0308,  0.0155,  0.0090],\n",
      "         [-0.0252, -0.0214,  0.0359,  ...,  0.0308,  0.0156,  0.0089]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0748, -0.0450, -0.0298,  ...,  0.0308,  0.0160,  0.0091],\n",
      "        [-0.0327, -0.0381,  0.0990,  ...,  0.0310,  0.0163,  0.0087],\n",
      "        [-0.0827,  0.0394, -0.0181,  ...,  0.0307,  0.0159,  0.0092],\n",
      "        ...,\n",
      "        [-0.0734,  0.0226, -0.0080,  ...,  0.0305,  0.0158,  0.0090],\n",
      "        [-0.0203, -0.0244, -0.0210,  ...,  0.0308,  0.0159,  0.0090],\n",
      "        [-0.0763, -0.0088,  0.0006,  ...,  0.0308,  0.0156,  0.0089]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "13-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0432, -0.0367, -0.0298,  ..., -0.0282,  0.0205,  0.0446],\n",
      "         [-0.0410, -0.0309, -0.0027,  ...,  0.0049,  0.0160,  0.0298],\n",
      "         [-0.0343, -0.0272,  0.0169,  ...,  0.0159,  0.0150,  0.0217],\n",
      "         ...,\n",
      "         [-0.0255, -0.0218,  0.0360,  ...,  0.0300,  0.0156,  0.0097],\n",
      "         [-0.0252, -0.0216,  0.0362,  ...,  0.0303,  0.0157,  0.0093],\n",
      "         [-0.0250, -0.0215,  0.0362,  ...,  0.0304,  0.0158,  0.0090]],\n",
      "\n",
      "        [[-0.0732,  0.0371,  0.0770,  ...,  0.0469,  0.0434,  0.0757],\n",
      "         [-0.0652,  0.0021,  0.0519,  ...,  0.0407,  0.0261,  0.0260],\n",
      "         [-0.0524, -0.0132,  0.0417,  ...,  0.0406,  0.0179,  0.0139],\n",
      "         ...,\n",
      "         [-0.0269, -0.0221,  0.0360,  ...,  0.0327,  0.0153,  0.0088],\n",
      "         [-0.0261, -0.0219,  0.0360,  ...,  0.0319,  0.0155,  0.0088],\n",
      "         [-0.0255, -0.0217,  0.0360,  ...,  0.0314,  0.0157,  0.0088]],\n",
      "\n",
      "        [[-0.0548,  0.0460, -0.0063,  ..., -0.0005,  0.0309,  0.0303],\n",
      "         [-0.0472, -0.0024,  0.0047,  ...,  0.0140,  0.0122,  0.0222],\n",
      "         [-0.0393, -0.0195,  0.0170,  ...,  0.0224,  0.0108,  0.0180],\n",
      "         ...,\n",
      "         [-0.0265, -0.0226,  0.0354,  ...,  0.0308,  0.0151,  0.0088],\n",
      "         [-0.0259, -0.0221,  0.0358,  ...,  0.0308,  0.0154,  0.0087],\n",
      "         [-0.0255, -0.0218,  0.0360,  ...,  0.0307,  0.0156,  0.0086]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0570, -0.0134, -0.0288,  ...,  0.0206,  0.0097,  0.0249],\n",
      "         [-0.0548, -0.0192,  0.0028,  ...,  0.0337,  0.0094,  0.0150],\n",
      "         [-0.0461, -0.0207,  0.0174,  ...,  0.0357,  0.0119,  0.0109],\n",
      "         ...,\n",
      "         [-0.0270, -0.0207,  0.0350,  ...,  0.0314,  0.0159,  0.0085],\n",
      "         [-0.0261, -0.0208,  0.0354,  ...,  0.0311,  0.0159,  0.0086],\n",
      "         [-0.0255, -0.0209,  0.0357,  ...,  0.0309,  0.0159,  0.0086]],\n",
      "\n",
      "        [[-0.0206,  0.0191, -0.0029,  ..., -0.0301,  0.0128,  0.0364],\n",
      "         [-0.0269, -0.0081,  0.0024,  ..., -0.0070,  0.0163,  0.0240],\n",
      "         [-0.0268, -0.0157,  0.0142,  ...,  0.0077,  0.0153,  0.0181],\n",
      "         ...,\n",
      "         [-0.0255, -0.0217,  0.0357,  ...,  0.0296,  0.0158,  0.0090],\n",
      "         [-0.0252, -0.0216,  0.0360,  ...,  0.0301,  0.0158,  0.0088],\n",
      "         [-0.0251, -0.0215,  0.0361,  ...,  0.0303,  0.0159,  0.0087]],\n",
      "\n",
      "        [[-0.0786, -0.0705,  0.0363,  ...,  0.0582,  0.0335,  0.0666],\n",
      "         [-0.0643, -0.0546,  0.0330,  ...,  0.0451,  0.0281,  0.0339],\n",
      "         [-0.0482, -0.0421,  0.0349,  ...,  0.0406,  0.0205,  0.0213],\n",
      "         ...,\n",
      "         [-0.0264, -0.0225,  0.0363,  ...,  0.0321,  0.0155,  0.0095],\n",
      "         [-0.0257, -0.0219,  0.0362,  ...,  0.0316,  0.0156,  0.0092],\n",
      "         [-0.0253, -0.0216,  0.0361,  ...,  0.0312,  0.0157,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0432, -0.0367, -0.0298,  ...,  0.0304,  0.0158,  0.0090],\n",
      "        [-0.0732,  0.0371,  0.0770,  ...,  0.0314,  0.0157,  0.0088],\n",
      "        [-0.0548,  0.0460, -0.0063,  ...,  0.0307,  0.0156,  0.0086],\n",
      "        ...,\n",
      "        [-0.0570, -0.0134, -0.0288,  ...,  0.0309,  0.0159,  0.0086],\n",
      "        [-0.0206,  0.0191, -0.0029,  ...,  0.0303,  0.0159,  0.0087],\n",
      "        [-0.0786, -0.0705,  0.0363,  ...,  0.0312,  0.0157,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0194, -0.0026,  0.0533,  ...,  0.0027, -0.0288,  0.0517],\n",
      "         [-0.0258, -0.0158,  0.0372,  ...,  0.0256, -0.0105,  0.0334],\n",
      "         [-0.0270, -0.0198,  0.0346,  ...,  0.0326,  0.0006,  0.0215],\n",
      "         ...,\n",
      "         [-0.0253, -0.0214,  0.0360,  ...,  0.0319,  0.0148,  0.0093],\n",
      "         [-0.0251, -0.0213,  0.0360,  ...,  0.0315,  0.0152,  0.0090],\n",
      "         [-0.0250, -0.0213,  0.0360,  ...,  0.0312,  0.0155,  0.0089]],\n",
      "\n",
      "        [[ 0.0078,  0.0028,  0.0060,  ..., -0.0500,  0.0062,  0.0356],\n",
      "         [-0.0165, -0.0188,  0.0092,  ..., -0.0068,  0.0109,  0.0224],\n",
      "         [-0.0201, -0.0258,  0.0172,  ...,  0.0131,  0.0116,  0.0183],\n",
      "         ...,\n",
      "         [-0.0249, -0.0228,  0.0348,  ...,  0.0305,  0.0157,  0.0099],\n",
      "         [-0.0249, -0.0222,  0.0354,  ...,  0.0307,  0.0158,  0.0094],\n",
      "         [-0.0249, -0.0218,  0.0357,  ...,  0.0307,  0.0159,  0.0091]],\n",
      "\n",
      "        [[-0.0244, -0.0029,  0.0169,  ...,  0.0038,  0.0418,  0.0240],\n",
      "         [-0.0367, -0.0186,  0.0198,  ...,  0.0266,  0.0281,  0.0191],\n",
      "         [-0.0366, -0.0223,  0.0252,  ...,  0.0323,  0.0209,  0.0166],\n",
      "         ...,\n",
      "         [-0.0260, -0.0216,  0.0354,  ...,  0.0317,  0.0162,  0.0098],\n",
      "         [-0.0255, -0.0214,  0.0357,  ...,  0.0313,  0.0161,  0.0094],\n",
      "         [-0.0252, -0.0214,  0.0359,  ...,  0.0310,  0.0160,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0573, -0.0216,  0.0428,  ..., -0.0296,  0.0069,  0.0475],\n",
      "         [-0.0437, -0.0242,  0.0340,  ...,  0.0055,  0.0187,  0.0290],\n",
      "         [-0.0362, -0.0264,  0.0335,  ...,  0.0207,  0.0219,  0.0206],\n",
      "         ...,\n",
      "         [-0.0262, -0.0234,  0.0359,  ...,  0.0313,  0.0171,  0.0093],\n",
      "         [-0.0256, -0.0227,  0.0360,  ...,  0.0311,  0.0166,  0.0090],\n",
      "         [-0.0253, -0.0222,  0.0360,  ...,  0.0309,  0.0163,  0.0089]],\n",
      "\n",
      "        [[-0.0302, -0.0630, -0.0109,  ..., -0.0142,  0.0489,  0.0102],\n",
      "         [-0.0302, -0.0434,  0.0071,  ...,  0.0144,  0.0282,  0.0159],\n",
      "         [-0.0293, -0.0341,  0.0188,  ...,  0.0259,  0.0224,  0.0152],\n",
      "         ...,\n",
      "         [-0.0260, -0.0222,  0.0348,  ...,  0.0316,  0.0169,  0.0089],\n",
      "         [-0.0256, -0.0218,  0.0353,  ...,  0.0313,  0.0165,  0.0087],\n",
      "         [-0.0253, -0.0215,  0.0356,  ...,  0.0310,  0.0163,  0.0087]],\n",
      "\n",
      "        [[-0.0374, -0.0655,  0.0152,  ..., -0.0517,  0.0055, -0.0693],\n",
      "         [-0.0274, -0.0358,  0.0208,  ..., -0.0041,  0.0177, -0.0297],\n",
      "         [-0.0255, -0.0280,  0.0258,  ...,  0.0185,  0.0205, -0.0094],\n",
      "         ...,\n",
      "         [-0.0253, -0.0219,  0.0349,  ...,  0.0318,  0.0167,  0.0074],\n",
      "         [-0.0252, -0.0217,  0.0353,  ...,  0.0314,  0.0164,  0.0079],\n",
      "         [-0.0250, -0.0215,  0.0356,  ...,  0.0311,  0.0161,  0.0081]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0194, -0.0026,  0.0533,  ...,  0.0312,  0.0155,  0.0089],\n",
      "        [ 0.0078,  0.0028,  0.0060,  ...,  0.0307,  0.0159,  0.0091],\n",
      "        [-0.0244, -0.0029,  0.0169,  ...,  0.0310,  0.0160,  0.0092],\n",
      "        ...,\n",
      "        [-0.0573, -0.0216,  0.0428,  ...,  0.0309,  0.0163,  0.0089],\n",
      "        [-0.0302, -0.0630, -0.0109,  ...,  0.0310,  0.0163,  0.0087],\n",
      "        [-0.0374, -0.0655,  0.0152,  ...,  0.0311,  0.0161,  0.0081]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "15-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0504, -0.0456,  0.0102,  ..., -0.0086,  0.0372, -0.0117],\n",
      "         [-0.0459, -0.0404,  0.0206,  ...,  0.0142,  0.0221, -0.0004],\n",
      "         [-0.0373, -0.0322,  0.0265,  ...,  0.0232,  0.0167,  0.0079],\n",
      "         ...,\n",
      "         [-0.0255, -0.0222,  0.0356,  ...,  0.0303,  0.0152,  0.0093],\n",
      "         [-0.0252, -0.0218,  0.0358,  ...,  0.0304,  0.0154,  0.0091],\n",
      "         [-0.0250, -0.0216,  0.0360,  ...,  0.0305,  0.0156,  0.0089]],\n",
      "\n",
      "        [[-0.0380,  0.0455,  0.0966,  ...,  0.0705,  0.0245,  0.0637],\n",
      "         [-0.0299,  0.0043,  0.0567,  ...,  0.0587,  0.0250,  0.0363],\n",
      "         [-0.0284, -0.0130,  0.0456,  ...,  0.0514,  0.0191,  0.0257],\n",
      "         ...,\n",
      "         [-0.0260, -0.0219,  0.0368,  ...,  0.0332,  0.0159,  0.0099],\n",
      "         [-0.0256, -0.0217,  0.0365,  ...,  0.0322,  0.0159,  0.0094],\n",
      "         [-0.0254, -0.0215,  0.0363,  ...,  0.0315,  0.0159,  0.0091]],\n",
      "\n",
      "        [[-0.0134,  0.0237, -0.0069,  ..., -0.0148,  0.0034,  0.0654],\n",
      "         [-0.0266, -0.0022,  0.0127,  ...,  0.0126,  0.0048,  0.0309],\n",
      "         [-0.0251, -0.0138,  0.0254,  ...,  0.0231,  0.0101,  0.0195],\n",
      "         ...,\n",
      "         [-0.0250, -0.0217,  0.0356,  ...,  0.0318,  0.0161,  0.0104],\n",
      "         [-0.0250, -0.0216,  0.0357,  ...,  0.0315,  0.0161,  0.0099],\n",
      "         [-0.0250, -0.0214,  0.0358,  ...,  0.0313,  0.0161,  0.0095]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0321, -0.0474,  0.0055,  ..., -0.0346,  0.0132,  0.0253],\n",
      "         [-0.0334, -0.0365,  0.0249,  ...,  0.0022,  0.0153,  0.0193],\n",
      "         [-0.0310, -0.0307,  0.0326,  ...,  0.0184,  0.0154,  0.0171],\n",
      "         ...,\n",
      "         [-0.0254, -0.0221,  0.0364,  ...,  0.0306,  0.0159,  0.0098],\n",
      "         [-0.0252, -0.0217,  0.0363,  ...,  0.0307,  0.0159,  0.0094],\n",
      "         [-0.0250, -0.0215,  0.0362,  ...,  0.0307,  0.0159,  0.0091]],\n",
      "\n",
      "        [[-0.0872, -0.0500, -0.0036,  ..., -0.0592,  0.0072, -0.0174],\n",
      "         [-0.0521, -0.0459,  0.0145,  ..., -0.0093,  0.0025, -0.0025],\n",
      "         [-0.0377, -0.0386,  0.0246,  ...,  0.0122,  0.0063,  0.0085],\n",
      "         ...,\n",
      "         [-0.0256, -0.0230,  0.0357,  ...,  0.0299,  0.0148,  0.0100],\n",
      "         [-0.0253, -0.0223,  0.0359,  ...,  0.0302,  0.0152,  0.0095],\n",
      "         [-0.0251, -0.0219,  0.0360,  ...,  0.0303,  0.0154,  0.0092]],\n",
      "\n",
      "        [[-0.0030, -0.0191,  0.0106,  ...,  0.0064, -0.0058,  0.0123],\n",
      "         [-0.0127, -0.0208,  0.0130,  ...,  0.0125,  0.0034,  0.0051],\n",
      "         [-0.0140, -0.0196,  0.0243,  ...,  0.0189,  0.0088,  0.0082],\n",
      "         ...,\n",
      "         [-0.0240, -0.0212,  0.0360,  ...,  0.0310,  0.0160,  0.0090],\n",
      "         [-0.0244, -0.0213,  0.0361,  ...,  0.0311,  0.0160,  0.0089],\n",
      "         [-0.0246, -0.0213,  0.0360,  ...,  0.0310,  0.0160,  0.0088]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0504, -0.0456,  0.0102,  ...,  0.0305,  0.0156,  0.0089],\n",
      "        [-0.0380,  0.0455,  0.0966,  ...,  0.0315,  0.0159,  0.0091],\n",
      "        [-0.0134,  0.0237, -0.0069,  ...,  0.0313,  0.0161,  0.0095],\n",
      "        ...,\n",
      "        [-0.0321, -0.0474,  0.0055,  ...,  0.0307,  0.0159,  0.0091],\n",
      "        [-0.0872, -0.0500, -0.0036,  ...,  0.0303,  0.0154,  0.0092],\n",
      "        [-0.0030, -0.0191,  0.0106,  ...,  0.0310,  0.0160,  0.0088]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "16-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0949, -0.0309, -0.0179,  ...,  0.0044,  0.0263, -0.0081],\n",
      "         [-0.0656, -0.0312,  0.0036,  ...,  0.0224,  0.0167, -0.0069],\n",
      "         [-0.0474, -0.0288,  0.0194,  ...,  0.0285,  0.0143,  0.0001],\n",
      "         ...,\n",
      "         [-0.0261, -0.0223,  0.0356,  ...,  0.0314,  0.0155,  0.0088],\n",
      "         [-0.0256, -0.0219,  0.0359,  ...,  0.0312,  0.0156,  0.0088],\n",
      "         [-0.0252, -0.0217,  0.0360,  ...,  0.0310,  0.0157,  0.0088]],\n",
      "\n",
      "        [[-0.0251, -0.0581,  0.0347,  ...,  0.0306, -0.0133,  0.0621],\n",
      "         [-0.0303, -0.0436,  0.0288,  ...,  0.0290,  0.0034,  0.0388],\n",
      "         [-0.0297, -0.0335,  0.0308,  ...,  0.0278,  0.0112,  0.0261],\n",
      "         ...,\n",
      "         [-0.0260, -0.0219,  0.0357,  ...,  0.0301,  0.0165,  0.0102],\n",
      "         [-0.0256, -0.0216,  0.0358,  ...,  0.0303,  0.0163,  0.0096],\n",
      "         [-0.0253, -0.0215,  0.0359,  ...,  0.0304,  0.0162,  0.0093]],\n",
      "\n",
      "        [[-0.0498,  0.0018,  0.0483,  ..., -0.0178, -0.0053,  0.0468],\n",
      "         [-0.0422, -0.0159,  0.0347,  ...,  0.0055,  0.0023,  0.0272],\n",
      "         [-0.0352, -0.0212,  0.0326,  ...,  0.0170,  0.0076,  0.0202],\n",
      "         ...,\n",
      "         [-0.0258, -0.0221,  0.0362,  ...,  0.0300,  0.0152,  0.0098],\n",
      "         [-0.0255, -0.0218,  0.0362,  ...,  0.0303,  0.0154,  0.0094],\n",
      "         [-0.0252, -0.0216,  0.0362,  ...,  0.0304,  0.0156,  0.0091]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0962, -0.0485,  0.0879,  ...,  0.0136,  0.0091,  0.0172],\n",
      "         [-0.0573, -0.0388,  0.0561,  ...,  0.0160,  0.0080,  0.0056],\n",
      "         [-0.0388, -0.0300,  0.0466,  ...,  0.0217,  0.0113,  0.0086],\n",
      "         ...,\n",
      "         [-0.0258, -0.0223,  0.0372,  ...,  0.0309,  0.0158,  0.0092],\n",
      "         [-0.0254, -0.0219,  0.0368,  ...,  0.0309,  0.0158,  0.0089],\n",
      "         [-0.0252, -0.0217,  0.0365,  ...,  0.0308,  0.0158,  0.0088]],\n",
      "\n",
      "        [[-0.0286, -0.0572,  0.0429,  ..., -0.0173,  0.0240,  0.0904],\n",
      "         [-0.0289, -0.0315,  0.0277,  ...,  0.0080,  0.0156,  0.0474],\n",
      "         [-0.0264, -0.0219,  0.0267,  ...,  0.0199,  0.0138,  0.0312],\n",
      "         ...,\n",
      "         [-0.0250, -0.0208,  0.0352,  ...,  0.0313,  0.0156,  0.0101],\n",
      "         [-0.0250, -0.0210,  0.0356,  ...,  0.0312,  0.0157,  0.0095],\n",
      "         [-0.0249, -0.0211,  0.0358,  ...,  0.0310,  0.0158,  0.0091]],\n",
      "\n",
      "        [[-0.0983, -0.0578, -0.0160,  ...,  0.0018,  0.0236, -0.0176],\n",
      "         [-0.0645, -0.0386,  0.0094,  ...,  0.0188,  0.0150, -0.0063],\n",
      "         [-0.0457, -0.0306,  0.0243,  ...,  0.0273,  0.0111,  0.0010],\n",
      "         ...,\n",
      "         [-0.0255, -0.0216,  0.0358,  ...,  0.0315,  0.0149,  0.0088],\n",
      "         [-0.0251, -0.0214,  0.0359,  ...,  0.0312,  0.0153,  0.0088],\n",
      "         [-0.0250, -0.0213,  0.0360,  ...,  0.0310,  0.0156,  0.0088]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0949, -0.0309, -0.0179,  ...,  0.0310,  0.0157,  0.0088],\n",
      "        [-0.0251, -0.0581,  0.0347,  ...,  0.0304,  0.0162,  0.0093],\n",
      "        [-0.0498,  0.0018,  0.0483,  ...,  0.0304,  0.0156,  0.0091],\n",
      "        ...,\n",
      "        [-0.0962, -0.0485,  0.0879,  ...,  0.0308,  0.0158,  0.0088],\n",
      "        [-0.0286, -0.0572,  0.0429,  ...,  0.0310,  0.0158,  0.0091],\n",
      "        [-0.0983, -0.0578, -0.0160,  ...,  0.0310,  0.0156,  0.0088]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0110, -0.1071,  0.0352,  ..., -0.0218, -0.0109,  0.0029],\n",
      "         [-0.0226, -0.0724,  0.0375,  ...,  0.0109,  0.0001,  0.0077],\n",
      "         [-0.0250, -0.0502,  0.0353,  ...,  0.0255,  0.0064,  0.0102],\n",
      "         ...,\n",
      "         [-0.0254, -0.0232,  0.0356,  ...,  0.0317,  0.0159,  0.0093],\n",
      "         [-0.0252, -0.0224,  0.0357,  ...,  0.0314,  0.0160,  0.0091],\n",
      "         [-0.0250, -0.0219,  0.0359,  ...,  0.0311,  0.0160,  0.0090]],\n",
      "\n",
      "        [[ 0.0407,  0.0077,  0.0147,  ..., -0.0186,  0.0409,  0.0523],\n",
      "         [ 0.0207, -0.0120,  0.0148,  ..., -0.0004,  0.0110,  0.0369],\n",
      "         [ 0.0008, -0.0176,  0.0215,  ...,  0.0156,  0.0070,  0.0311],\n",
      "         ...,\n",
      "         [-0.0248, -0.0209,  0.0344,  ...,  0.0297,  0.0147,  0.0108],\n",
      "         [-0.0251, -0.0210,  0.0350,  ...,  0.0301,  0.0152,  0.0099],\n",
      "         [-0.0252, -0.0210,  0.0354,  ...,  0.0303,  0.0155,  0.0094]],\n",
      "\n",
      "        [[-0.0474,  0.0322,  0.0313,  ...,  0.0105,  0.0648,  0.0042],\n",
      "         [-0.0427, -0.0038,  0.0237,  ...,  0.0203,  0.0382,  0.0109],\n",
      "         [-0.0362, -0.0154,  0.0304,  ...,  0.0250,  0.0254,  0.0162],\n",
      "         ...,\n",
      "         [-0.0263, -0.0225,  0.0364,  ...,  0.0314,  0.0163,  0.0101],\n",
      "         [-0.0258, -0.0221,  0.0364,  ...,  0.0312,  0.0162,  0.0096],\n",
      "         [-0.0254, -0.0218,  0.0363,  ...,  0.0311,  0.0161,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0481, -0.0177, -0.0151,  ..., -0.0055,  0.0815, -0.0662],\n",
      "         [-0.0528, -0.0358,  0.0135,  ...,  0.0142,  0.0463, -0.0332],\n",
      "         [-0.0445, -0.0328,  0.0292,  ...,  0.0275,  0.0299, -0.0114],\n",
      "         ...,\n",
      "         [-0.0272, -0.0223,  0.0372,  ...,  0.0328,  0.0167,  0.0083],\n",
      "         [-0.0263, -0.0219,  0.0368,  ...,  0.0320,  0.0164,  0.0085],\n",
      "         [-0.0257, -0.0216,  0.0366,  ...,  0.0315,  0.0162,  0.0086]],\n",
      "\n",
      "        [[-0.1353, -0.0324,  0.0026,  ...,  0.0050,  0.0486, -0.0124],\n",
      "         [-0.0943, -0.0259,  0.0178,  ...,  0.0157,  0.0355, -0.0016],\n",
      "         [-0.0633, -0.0247,  0.0285,  ...,  0.0230,  0.0248,  0.0067],\n",
      "         ...,\n",
      "         [-0.0268, -0.0221,  0.0365,  ...,  0.0310,  0.0164,  0.0093],\n",
      "         [-0.0259, -0.0218,  0.0364,  ...,  0.0309,  0.0162,  0.0091],\n",
      "         [-0.0254, -0.0216,  0.0363,  ...,  0.0308,  0.0161,  0.0089]],\n",
      "\n",
      "        [[-0.0501, -0.0383,  0.0091,  ...,  0.0165,  0.0261,  0.0267],\n",
      "         [-0.0460, -0.0318,  0.0129,  ...,  0.0188,  0.0153,  0.0235],\n",
      "         [-0.0388, -0.0292,  0.0226,  ...,  0.0251,  0.0133,  0.0199],\n",
      "         ...,\n",
      "         [-0.0264, -0.0224,  0.0357,  ...,  0.0312,  0.0158,  0.0098],\n",
      "         [-0.0258, -0.0220,  0.0359,  ...,  0.0310,  0.0159,  0.0093],\n",
      "         [-0.0254, -0.0217,  0.0360,  ...,  0.0309,  0.0159,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0110, -0.1071,  0.0352,  ...,  0.0311,  0.0160,  0.0090],\n",
      "        [ 0.0407,  0.0077,  0.0147,  ...,  0.0303,  0.0155,  0.0094],\n",
      "        [-0.0474,  0.0322,  0.0313,  ...,  0.0311,  0.0161,  0.0092],\n",
      "        ...,\n",
      "        [-0.0481, -0.0177, -0.0151,  ...,  0.0315,  0.0162,  0.0086],\n",
      "        [-0.1353, -0.0324,  0.0026,  ...,  0.0308,  0.0161,  0.0089],\n",
      "        [-0.0501, -0.0383,  0.0091,  ...,  0.0309,  0.0159,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "18-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0756,  0.0166,  0.0073,  ...,  0.0152,  0.0029,  0.0209],\n",
      "         [-0.0591, -0.0090,  0.0165,  ...,  0.0267,  0.0137,  0.0100],\n",
      "         [-0.0463, -0.0190,  0.0264,  ...,  0.0306,  0.0175,  0.0081],\n",
      "         ...,\n",
      "         [-0.0264, -0.0223,  0.0365,  ...,  0.0312,  0.0162,  0.0087],\n",
      "         [-0.0257, -0.0220,  0.0365,  ...,  0.0310,  0.0160,  0.0087],\n",
      "         [-0.0253, -0.0218,  0.0364,  ...,  0.0309,  0.0159,  0.0087]],\n",
      "\n",
      "        [[-0.0562, -0.0487,  0.0226,  ..., -0.0510,  0.0396,  0.0381],\n",
      "         [-0.0468, -0.0405,  0.0260,  ..., -0.0095,  0.0249,  0.0197],\n",
      "         [-0.0374, -0.0325,  0.0290,  ...,  0.0112,  0.0195,  0.0156],\n",
      "         ...,\n",
      "         [-0.0257, -0.0219,  0.0350,  ...,  0.0307,  0.0161,  0.0090],\n",
      "         [-0.0253, -0.0217,  0.0354,  ...,  0.0308,  0.0160,  0.0088],\n",
      "         [-0.0251, -0.0215,  0.0356,  ...,  0.0308,  0.0160,  0.0087]],\n",
      "\n",
      "        [[-0.0558, -0.0255, -0.0067,  ...,  0.0070,  0.0318,  0.0800],\n",
      "         [-0.0363, -0.0230,  0.0067,  ...,  0.0224,  0.0210,  0.0480],\n",
      "         [-0.0302, -0.0233,  0.0194,  ...,  0.0270,  0.0199,  0.0324],\n",
      "         ...,\n",
      "         [-0.0254, -0.0214,  0.0356,  ...,  0.0311,  0.0162,  0.0101],\n",
      "         [-0.0253, -0.0212,  0.0359,  ...,  0.0310,  0.0161,  0.0094],\n",
      "         [-0.0251, -0.0212,  0.0360,  ...,  0.0309,  0.0160,  0.0091]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0027,  0.0140,  0.0191,  ..., -0.0278,  0.0200,  0.0098],\n",
      "         [-0.0138, -0.0013,  0.0214,  ...,  0.0026,  0.0092,  0.0090],\n",
      "         [-0.0186, -0.0115,  0.0279,  ...,  0.0179,  0.0076,  0.0089],\n",
      "         ...,\n",
      "         [-0.0246, -0.0214,  0.0357,  ...,  0.0311,  0.0151,  0.0085],\n",
      "         [-0.0247, -0.0214,  0.0358,  ...,  0.0310,  0.0155,  0.0086],\n",
      "         [-0.0248, -0.0214,  0.0359,  ...,  0.0309,  0.0157,  0.0086]],\n",
      "\n",
      "        [[ 0.0229,  0.0203,  0.0419,  ..., -0.0493,  0.0791, -0.0201],\n",
      "         [-0.0045, -0.0073,  0.0386,  ..., -0.0023,  0.0379, -0.0062],\n",
      "         [-0.0141, -0.0166,  0.0381,  ...,  0.0215,  0.0262,  0.0016],\n",
      "         ...,\n",
      "         [-0.0253, -0.0217,  0.0359,  ...,  0.0318,  0.0167,  0.0087],\n",
      "         [-0.0253, -0.0215,  0.0359,  ...,  0.0314,  0.0164,  0.0087],\n",
      "         [-0.0252, -0.0214,  0.0359,  ...,  0.0310,  0.0162,  0.0087]],\n",
      "\n",
      "        [[ 0.0010, -0.0153,  0.0708,  ..., -0.0067,  0.0344,  0.0566],\n",
      "         [-0.0132, -0.0277,  0.0432,  ...,  0.0181,  0.0141,  0.0338],\n",
      "         [-0.0203, -0.0291,  0.0362,  ...,  0.0283,  0.0077,  0.0237],\n",
      "         ...,\n",
      "         [-0.0253, -0.0230,  0.0357,  ...,  0.0316,  0.0140,  0.0102],\n",
      "         [-0.0252, -0.0224,  0.0359,  ...,  0.0313,  0.0147,  0.0096],\n",
      "         [-0.0251, -0.0219,  0.0360,  ...,  0.0310,  0.0151,  0.0093]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0756,  0.0166,  0.0073,  ...,  0.0309,  0.0159,  0.0087],\n",
      "        [-0.0562, -0.0487,  0.0226,  ...,  0.0308,  0.0160,  0.0087],\n",
      "        [-0.0558, -0.0255, -0.0067,  ...,  0.0309,  0.0160,  0.0091],\n",
      "        ...,\n",
      "        [ 0.0027,  0.0140,  0.0191,  ...,  0.0309,  0.0157,  0.0086],\n",
      "        [ 0.0229,  0.0203,  0.0419,  ...,  0.0310,  0.0162,  0.0087],\n",
      "        [ 0.0010, -0.0153,  0.0708,  ...,  0.0310,  0.0151,  0.0093]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "19-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0057, -0.0160,  0.0741,  ..., -0.0475, -0.0239, -0.0215],\n",
      "         [-0.0049, -0.0225,  0.0575,  ..., -0.0137, -0.0046, -0.0025],\n",
      "         [-0.0111, -0.0243,  0.0487,  ...,  0.0061,  0.0040,  0.0035],\n",
      "         ...,\n",
      "         [-0.0243, -0.0217,  0.0365,  ...,  0.0302,  0.0156,  0.0080],\n",
      "         [-0.0246, -0.0214,  0.0362,  ...,  0.0305,  0.0159,  0.0082],\n",
      "         [-0.0247, -0.0213,  0.0361,  ...,  0.0306,  0.0159,  0.0084]],\n",
      "\n",
      "        [[-0.0112, -0.0090,  0.0796,  ..., -0.0310, -0.0008,  0.0346],\n",
      "         [-0.0260, -0.0192,  0.0385,  ...,  0.0008,  0.0053,  0.0244],\n",
      "         [-0.0283, -0.0248,  0.0291,  ...,  0.0174,  0.0082,  0.0175],\n",
      "         ...,\n",
      "         [-0.0263, -0.0221,  0.0347,  ...,  0.0312,  0.0163,  0.0081],\n",
      "         [-0.0258, -0.0218,  0.0352,  ...,  0.0311,  0.0162,  0.0082],\n",
      "         [-0.0255, -0.0215,  0.0356,  ...,  0.0310,  0.0161,  0.0083]],\n",
      "\n",
      "        [[-0.0506, -0.0254,  0.0341,  ...,  0.0363,  0.0147,  0.0690],\n",
      "         [-0.0426, -0.0278,  0.0189,  ...,  0.0338,  0.0161,  0.0466],\n",
      "         [-0.0371, -0.0256,  0.0231,  ...,  0.0312,  0.0173,  0.0330],\n",
      "         ...,\n",
      "         [-0.0267, -0.0214,  0.0353,  ...,  0.0303,  0.0161,  0.0111],\n",
      "         [-0.0260, -0.0213,  0.0357,  ...,  0.0304,  0.0159,  0.0101],\n",
      "         [-0.0256, -0.0213,  0.0359,  ...,  0.0305,  0.0159,  0.0095]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0466, -0.0065, -0.0182,  ..., -0.0047,  0.0056,  0.0438],\n",
      "         [-0.0386, -0.0192,  0.0010,  ...,  0.0133,  0.0015,  0.0318],\n",
      "         [-0.0324, -0.0233,  0.0169,  ...,  0.0219,  0.0040,  0.0267],\n",
      "         ...,\n",
      "         [-0.0253, -0.0218,  0.0358,  ...,  0.0309,  0.0143,  0.0105],\n",
      "         [-0.0251, -0.0216,  0.0361,  ...,  0.0310,  0.0149,  0.0098],\n",
      "         [-0.0250, -0.0214,  0.0362,  ...,  0.0309,  0.0153,  0.0093]],\n",
      "\n",
      "        [[-0.0426, -0.0443, -0.0285,  ...,  0.0135,  0.0350,  0.0249],\n",
      "         [-0.0366, -0.0301,  0.0010,  ...,  0.0260,  0.0230,  0.0201],\n",
      "         [-0.0324, -0.0245,  0.0168,  ...,  0.0314,  0.0187,  0.0166],\n",
      "         ...,\n",
      "         [-0.0256, -0.0215,  0.0349,  ...,  0.0314,  0.0163,  0.0091],\n",
      "         [-0.0253, -0.0214,  0.0354,  ...,  0.0311,  0.0162,  0.0089],\n",
      "         [-0.0251, -0.0213,  0.0357,  ...,  0.0309,  0.0161,  0.0088]],\n",
      "\n",
      "        [[-0.0905, -0.0231,  0.0196,  ..., -0.0142,  0.0446,  0.0290],\n",
      "         [-0.0644, -0.0222,  0.0179,  ...,  0.0049,  0.0290,  0.0206],\n",
      "         [-0.0477, -0.0231,  0.0237,  ...,  0.0154,  0.0221,  0.0177],\n",
      "         ...,\n",
      "         [-0.0263, -0.0216,  0.0353,  ...,  0.0298,  0.0160,  0.0101],\n",
      "         [-0.0257, -0.0214,  0.0356,  ...,  0.0302,  0.0159,  0.0096],\n",
      "         [-0.0253, -0.0213,  0.0358,  ...,  0.0305,  0.0159,  0.0093]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0057, -0.0160,  0.0741,  ...,  0.0306,  0.0159,  0.0084],\n",
      "        [-0.0112, -0.0090,  0.0796,  ...,  0.0310,  0.0161,  0.0083],\n",
      "        [-0.0506, -0.0254,  0.0341,  ...,  0.0305,  0.0159,  0.0095],\n",
      "        ...,\n",
      "        [-0.0466, -0.0065, -0.0182,  ...,  0.0309,  0.0153,  0.0093],\n",
      "        [-0.0426, -0.0443, -0.0285,  ...,  0.0309,  0.0161,  0.0088],\n",
      "        [-0.0905, -0.0231,  0.0196,  ...,  0.0305,  0.0159,  0.0093]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "error: image name=>stephenfry_518.jpg des name=>stephenfry_518.txt label name=>stephenfry_518.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[ 0.0117,  0.0164,  0.0803,  ..., -0.0122,  0.0247,  0.0692],\n",
      "         [-0.0122, -0.0093,  0.0533,  ...,  0.0037,  0.0145,  0.0429],\n",
      "         [-0.0203, -0.0177,  0.0440,  ...,  0.0153,  0.0128,  0.0311],\n",
      "         ...,\n",
      "         [-0.0257, -0.0215,  0.0363,  ...,  0.0305,  0.0155,  0.0105],\n",
      "         [-0.0255, -0.0214,  0.0361,  ...,  0.0306,  0.0157,  0.0098],\n",
      "         [-0.0253, -0.0213,  0.0361,  ...,  0.0306,  0.0158,  0.0093]],\n",
      "\n",
      "        [[-0.0774,  0.0107, -0.0025,  ...,  0.0085, -0.0412,  0.0393],\n",
      "         [-0.0510, -0.0027,  0.0067,  ...,  0.0204, -0.0073,  0.0272],\n",
      "         [-0.0370, -0.0106,  0.0189,  ...,  0.0247,  0.0039,  0.0213],\n",
      "         ...,\n",
      "         [-0.0251, -0.0213,  0.0361,  ...,  0.0311,  0.0155,  0.0097],\n",
      "         [-0.0250, -0.0214,  0.0363,  ...,  0.0311,  0.0157,  0.0093],\n",
      "         [-0.0249, -0.0214,  0.0363,  ...,  0.0310,  0.0158,  0.0091]],\n",
      "\n",
      "        [[-0.0525, -0.0240,  0.0611,  ..., -0.0029,  0.0343,  0.0296],\n",
      "         [-0.0451, -0.0338,  0.0396,  ...,  0.0100,  0.0264,  0.0195],\n",
      "         [-0.0375, -0.0343,  0.0378,  ...,  0.0193,  0.0197,  0.0162],\n",
      "         ...,\n",
      "         [-0.0257, -0.0233,  0.0370,  ...,  0.0305,  0.0154,  0.0098],\n",
      "         [-0.0253, -0.0225,  0.0367,  ...,  0.0306,  0.0155,  0.0094],\n",
      "         [-0.0250, -0.0220,  0.0365,  ...,  0.0307,  0.0156,  0.0091]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0408, -0.0818,  0.0416,  ..., -0.0074,  0.0641,  0.0186],\n",
      "         [-0.0473, -0.0479,  0.0301,  ...,  0.0095,  0.0296,  0.0193],\n",
      "         [-0.0425, -0.0328,  0.0308,  ...,  0.0198,  0.0182,  0.0190],\n",
      "         ...,\n",
      "         [-0.0269, -0.0214,  0.0356,  ...,  0.0313,  0.0153,  0.0096],\n",
      "         [-0.0261, -0.0213,  0.0358,  ...,  0.0312,  0.0156,  0.0091],\n",
      "         [-0.0256, -0.0212,  0.0359,  ...,  0.0311,  0.0157,  0.0089]],\n",
      "\n",
      "        [[-0.0427,  0.0402,  0.0689,  ..., -0.0151,  0.0478,  0.0647],\n",
      "         [-0.0380,  0.0121,  0.0340,  ...,  0.0168,  0.0221,  0.0424],\n",
      "         [-0.0333, -0.0055,  0.0255,  ...,  0.0305,  0.0153,  0.0278],\n",
      "         ...,\n",
      "         [-0.0254, -0.0208,  0.0341,  ...,  0.0324,  0.0145,  0.0094],\n",
      "         [-0.0252, -0.0209,  0.0349,  ...,  0.0318,  0.0149,  0.0091],\n",
      "         [-0.0251, -0.0210,  0.0354,  ...,  0.0314,  0.0152,  0.0089]],\n",
      "\n",
      "        [[-0.0298, -0.0280,  0.0543,  ...,  0.0071,  0.0224,  0.0253],\n",
      "         [-0.0349, -0.0292,  0.0388,  ...,  0.0152,  0.0181,  0.0209],\n",
      "         [-0.0318, -0.0264,  0.0370,  ...,  0.0209,  0.0160,  0.0198],\n",
      "         ...,\n",
      "         [-0.0258, -0.0217,  0.0360,  ...,  0.0300,  0.0162,  0.0103],\n",
      "         [-0.0254, -0.0215,  0.0360,  ...,  0.0303,  0.0161,  0.0097],\n",
      "         [-0.0252, -0.0214,  0.0360,  ...,  0.0304,  0.0160,  0.0093]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[ 0.0117,  0.0164,  0.0803,  ...,  0.0306,  0.0158,  0.0093],\n",
      "        [-0.0774,  0.0107, -0.0025,  ...,  0.0310,  0.0158,  0.0091],\n",
      "        [-0.0525, -0.0240,  0.0611,  ...,  0.0307,  0.0156,  0.0091],\n",
      "        ...,\n",
      "        [-0.0408, -0.0818,  0.0416,  ...,  0.0311,  0.0157,  0.0089],\n",
      "        [-0.0427,  0.0402,  0.0689,  ...,  0.0314,  0.0152,  0.0089],\n",
      "        [-0.0298, -0.0280,  0.0543,  ...,  0.0304,  0.0160,  0.0093]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "21-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0934,  0.0322,  0.0117,  ..., -0.0193,  0.0244,  0.0278],\n",
      "         [-0.0656, -0.0005,  0.0183,  ...,  0.0109,  0.0253,  0.0185],\n",
      "         [-0.0473, -0.0142,  0.0247,  ...,  0.0260,  0.0236,  0.0146],\n",
      "         ...,\n",
      "         [-0.0261, -0.0215,  0.0357,  ...,  0.0313,  0.0167,  0.0090],\n",
      "         [-0.0255, -0.0214,  0.0359,  ...,  0.0310,  0.0164,  0.0089],\n",
      "         [-0.0252, -0.0213,  0.0360,  ...,  0.0308,  0.0162,  0.0088]],\n",
      "\n",
      "        [[-0.0526, -0.0026,  0.0002,  ...,  0.0214,  0.0589,  0.0212],\n",
      "         [-0.0444, -0.0206,  0.0146,  ...,  0.0254,  0.0367,  0.0139],\n",
      "         [-0.0378, -0.0243,  0.0257,  ...,  0.0268,  0.0266,  0.0147],\n",
      "         ...,\n",
      "         [-0.0259, -0.0218,  0.0356,  ...,  0.0306,  0.0165,  0.0096],\n",
      "         [-0.0254, -0.0215,  0.0358,  ...,  0.0306,  0.0162,  0.0092],\n",
      "         [-0.0251, -0.0214,  0.0359,  ...,  0.0307,  0.0161,  0.0090]],\n",
      "\n",
      "        [[-0.0954,  0.0205, -0.0623,  ..., -0.0207,  0.0356,  0.0664],\n",
      "         [-0.0549, -0.0110, -0.0199,  ...,  0.0126,  0.0264,  0.0388],\n",
      "         [-0.0420, -0.0179,  0.0049,  ...,  0.0277,  0.0217,  0.0259],\n",
      "         ...,\n",
      "         [-0.0262, -0.0216,  0.0348,  ...,  0.0312,  0.0156,  0.0098],\n",
      "         [-0.0257, -0.0215,  0.0354,  ...,  0.0309,  0.0156,  0.0093],\n",
      "         [-0.0254, -0.0215,  0.0357,  ...,  0.0307,  0.0157,  0.0090]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0232, -0.0108, -0.0121,  ..., -0.0111,  0.0359, -0.0097],\n",
      "         [-0.0263, -0.0273,  0.0083,  ...,  0.0055,  0.0202,  0.0095],\n",
      "         [-0.0262, -0.0281,  0.0224,  ...,  0.0175,  0.0169,  0.0163],\n",
      "         ...,\n",
      "         [-0.0254, -0.0227,  0.0359,  ...,  0.0305,  0.0158,  0.0104],\n",
      "         [-0.0252, -0.0221,  0.0360,  ...,  0.0306,  0.0158,  0.0097],\n",
      "         [-0.0251, -0.0218,  0.0361,  ...,  0.0307,  0.0158,  0.0093]],\n",
      "\n",
      "        [[-0.0539, -0.0092,  0.0463,  ..., -0.0160,  0.0436,  0.0511],\n",
      "         [-0.0425, -0.0218,  0.0273,  ...,  0.0112,  0.0334,  0.0393],\n",
      "         [-0.0336, -0.0230,  0.0251,  ...,  0.0209,  0.0249,  0.0288],\n",
      "         ...,\n",
      "         [-0.0251, -0.0218,  0.0347,  ...,  0.0304,  0.0161,  0.0100],\n",
      "         [-0.0250, -0.0216,  0.0353,  ...,  0.0306,  0.0160,  0.0094],\n",
      "         [-0.0249, -0.0215,  0.0356,  ...,  0.0306,  0.0159,  0.0091]],\n",
      "\n",
      "        [[-0.0601, -0.0619,  0.0240,  ..., -0.0321,  0.0519, -0.0030],\n",
      "         [-0.0491, -0.0467,  0.0241,  ..., -0.0021,  0.0354,  0.0063],\n",
      "         [-0.0407, -0.0359,  0.0290,  ...,  0.0162,  0.0264,  0.0118],\n",
      "         ...,\n",
      "         [-0.0263, -0.0220,  0.0356,  ...,  0.0306,  0.0162,  0.0096],\n",
      "         [-0.0257, -0.0217,  0.0358,  ...,  0.0306,  0.0161,  0.0092],\n",
      "         [-0.0253, -0.0215,  0.0359,  ...,  0.0306,  0.0160,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0934,  0.0322,  0.0117,  ...,  0.0308,  0.0162,  0.0088],\n",
      "        [-0.0526, -0.0026,  0.0002,  ...,  0.0307,  0.0161,  0.0090],\n",
      "        [-0.0954,  0.0205, -0.0623,  ...,  0.0307,  0.0157,  0.0090],\n",
      "        ...,\n",
      "        [-0.0232, -0.0108, -0.0121,  ...,  0.0307,  0.0158,  0.0093],\n",
      "        [-0.0539, -0.0092,  0.0463,  ...,  0.0306,  0.0159,  0.0091],\n",
      "        [-0.0601, -0.0619,  0.0240,  ...,  0.0306,  0.0160,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "22-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0036, -0.0048, -0.0012,  ..., -0.0061,  0.0432,  0.0205],\n",
      "         [-0.0214, -0.0201,  0.0139,  ...,  0.0170,  0.0277,  0.0168],\n",
      "         [-0.0270, -0.0240,  0.0234,  ...,  0.0269,  0.0201,  0.0151],\n",
      "         ...,\n",
      "         [-0.0259, -0.0222,  0.0351,  ...,  0.0310,  0.0155,  0.0090],\n",
      "         [-0.0255, -0.0218,  0.0355,  ...,  0.0308,  0.0156,  0.0088],\n",
      "         [-0.0252, -0.0216,  0.0357,  ...,  0.0307,  0.0157,  0.0087]],\n",
      "\n",
      "        [[-0.0264, -0.0493,  0.0092,  ..., -0.0011,  0.0673,  0.0135],\n",
      "         [-0.0340, -0.0418,  0.0210,  ...,  0.0091,  0.0389,  0.0129],\n",
      "         [-0.0325, -0.0349,  0.0287,  ...,  0.0173,  0.0264,  0.0146],\n",
      "         ...,\n",
      "         [-0.0260, -0.0220,  0.0358,  ...,  0.0300,  0.0162,  0.0099],\n",
      "         [-0.0256, -0.0216,  0.0359,  ...,  0.0303,  0.0161,  0.0095],\n",
      "         [-0.0253, -0.0214,  0.0360,  ...,  0.0304,  0.0160,  0.0092]],\n",
      "\n",
      "        [[-0.0330, -0.0505,  0.0307,  ..., -0.0496,  0.0127,  0.0139],\n",
      "         [-0.0307, -0.0419,  0.0254,  ..., -0.0077,  0.0138,  0.0188],\n",
      "         [-0.0299, -0.0326,  0.0290,  ...,  0.0097,  0.0160,  0.0190],\n",
      "         ...,\n",
      "         [-0.0261, -0.0220,  0.0357,  ...,  0.0300,  0.0166,  0.0093],\n",
      "         [-0.0256, -0.0217,  0.0358,  ...,  0.0304,  0.0164,  0.0089],\n",
      "         [-0.0253, -0.0215,  0.0359,  ...,  0.0305,  0.0162,  0.0088]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0561, -0.0070, -0.0603,  ...,  0.0383,  0.0364,  0.0237],\n",
      "         [-0.0472, -0.0198, -0.0196,  ...,  0.0430,  0.0211,  0.0114],\n",
      "         [-0.0401, -0.0230,  0.0048,  ...,  0.0407,  0.0160,  0.0102],\n",
      "         ...,\n",
      "         [-0.0266, -0.0214,  0.0346,  ...,  0.0320,  0.0158,  0.0091],\n",
      "         [-0.0260, -0.0212,  0.0353,  ...,  0.0314,  0.0159,  0.0089],\n",
      "         [-0.0256, -0.0212,  0.0356,  ...,  0.0311,  0.0159,  0.0088]],\n",
      "\n",
      "        [[-0.0256, -0.0368, -0.0130,  ...,  0.0182,  0.0541,  0.0126],\n",
      "         [-0.0338, -0.0353, -0.0039,  ...,  0.0283,  0.0256,  0.0218],\n",
      "         [-0.0336, -0.0310,  0.0101,  ...,  0.0294,  0.0158,  0.0220],\n",
      "         ...,\n",
      "         [-0.0258, -0.0223,  0.0349,  ...,  0.0305,  0.0148,  0.0103],\n",
      "         [-0.0254, -0.0218,  0.0355,  ...,  0.0306,  0.0152,  0.0096],\n",
      "         [-0.0252, -0.0216,  0.0358,  ...,  0.0306,  0.0154,  0.0092]],\n",
      "\n",
      "        [[-0.0371,  0.0039,  0.0095,  ..., -0.0454,  0.0704,  0.0810],\n",
      "         [-0.0394, -0.0203,  0.0129,  ...,  0.0049,  0.0390,  0.0508],\n",
      "         [-0.0358, -0.0253,  0.0190,  ...,  0.0238,  0.0260,  0.0344],\n",
      "         ...,\n",
      "         [-0.0263, -0.0222,  0.0352,  ...,  0.0317,  0.0162,  0.0101],\n",
      "         [-0.0257, -0.0218,  0.0357,  ...,  0.0313,  0.0160,  0.0094],\n",
      "         [-0.0253, -0.0216,  0.0359,  ...,  0.0310,  0.0160,  0.0091]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0036, -0.0048, -0.0012,  ...,  0.0307,  0.0157,  0.0087],\n",
      "        [-0.0264, -0.0493,  0.0092,  ...,  0.0304,  0.0160,  0.0092],\n",
      "        [-0.0330, -0.0505,  0.0307,  ...,  0.0305,  0.0162,  0.0088],\n",
      "        ...,\n",
      "        [-0.0561, -0.0070, -0.0603,  ...,  0.0311,  0.0159,  0.0088],\n",
      "        [-0.0256, -0.0368, -0.0130,  ...,  0.0306,  0.0154,  0.0092],\n",
      "        [-0.0371,  0.0039,  0.0095,  ...,  0.0310,  0.0160,  0.0091]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "23-torch.Size([16, 3, 448, 448]) 16 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0155, -0.0493,  0.0293,  ..., -0.0167,  0.0347,  0.0042],\n",
      "         [-0.0173, -0.0459,  0.0239,  ...,  0.0077,  0.0245,  0.0104],\n",
      "         [-0.0199, -0.0371,  0.0305,  ...,  0.0199,  0.0162,  0.0151],\n",
      "         ...,\n",
      "         [-0.0248, -0.0227,  0.0362,  ...,  0.0310,  0.0148,  0.0103],\n",
      "         [-0.0249, -0.0221,  0.0362,  ...,  0.0309,  0.0151,  0.0097],\n",
      "         [-0.0249, -0.0217,  0.0361,  ...,  0.0308,  0.0154,  0.0093]],\n",
      "\n",
      "        [[-0.0926, -0.0309,  0.0251,  ...,  0.0555,  0.0493, -0.0121],\n",
      "         [-0.0642, -0.0230,  0.0318,  ...,  0.0460,  0.0348, -0.0002],\n",
      "         [-0.0486, -0.0225,  0.0342,  ...,  0.0382,  0.0271,  0.0061],\n",
      "         ...,\n",
      "         [-0.0267, -0.0219,  0.0363,  ...,  0.0318,  0.0175,  0.0089],\n",
      "         [-0.0260, -0.0217,  0.0363,  ...,  0.0314,  0.0169,  0.0088],\n",
      "         [-0.0255, -0.0216,  0.0362,  ...,  0.0311,  0.0165,  0.0088]],\n",
      "\n",
      "        [[-0.0634, -0.0672,  0.0877,  ...,  0.0314,  0.0459, -0.0407],\n",
      "         [-0.0481, -0.0444,  0.0602,  ...,  0.0359,  0.0315, -0.0089],\n",
      "         [-0.0420, -0.0330,  0.0509,  ...,  0.0382,  0.0229,  0.0052],\n",
      "         ...,\n",
      "         [-0.0270, -0.0219,  0.0373,  ...,  0.0320,  0.0155,  0.0094],\n",
      "         [-0.0262, -0.0216,  0.0368,  ...,  0.0314,  0.0156,  0.0091],\n",
      "         [-0.0257, -0.0214,  0.0365,  ...,  0.0310,  0.0156,  0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0703,  0.0030, -0.0276,  ..., -0.0121,  0.0441, -0.0046],\n",
      "         [-0.0562, -0.0126, -0.0002,  ...,  0.0104,  0.0346,  0.0029],\n",
      "         [-0.0448, -0.0179,  0.0169,  ...,  0.0202,  0.0274,  0.0091],\n",
      "         ...,\n",
      "         [-0.0266, -0.0213,  0.0356,  ...,  0.0306,  0.0169,  0.0094],\n",
      "         [-0.0259, -0.0213,  0.0359,  ...,  0.0307,  0.0165,  0.0091],\n",
      "         [-0.0255, -0.0213,  0.0360,  ...,  0.0307,  0.0162,  0.0089]],\n",
      "\n",
      "        [[-0.0039, -0.0537,  0.0064,  ...,  0.0322,  0.0071, -0.0257],\n",
      "         [-0.0244, -0.0469,  0.0183,  ...,  0.0391,  0.0128, -0.0102],\n",
      "         [-0.0294, -0.0415,  0.0256,  ...,  0.0365,  0.0137,  0.0035],\n",
      "         ...,\n",
      "         [-0.0258, -0.0236,  0.0350,  ...,  0.0304,  0.0165,  0.0098],\n",
      "         [-0.0254, -0.0227,  0.0354,  ...,  0.0303,  0.0164,  0.0094],\n",
      "         [-0.0252, -0.0221,  0.0356,  ...,  0.0303,  0.0162,  0.0092]],\n",
      "\n",
      "        [[-0.1241,  0.0064, -0.0004,  ..., -0.0093,  0.0459,  0.0283],\n",
      "         [-0.0781, -0.0169,  0.0061,  ...,  0.0095,  0.0322,  0.0109],\n",
      "         [-0.0529, -0.0217,  0.0202,  ...,  0.0197,  0.0263,  0.0101],\n",
      "         ...,\n",
      "         [-0.0264, -0.0223,  0.0359,  ...,  0.0309,  0.0163,  0.0089],\n",
      "         [-0.0257, -0.0220,  0.0360,  ...,  0.0309,  0.0161,  0.0087],\n",
      "         [-0.0253, -0.0217,  0.0361,  ...,  0.0309,  0.0160,  0.0087]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0155, -0.0493,  0.0293,  ...,  0.0308,  0.0154,  0.0093],\n",
      "        [-0.0926, -0.0309,  0.0251,  ...,  0.0311,  0.0165,  0.0088],\n",
      "        [-0.0634, -0.0672,  0.0877,  ...,  0.0310,  0.0156,  0.0089],\n",
      "        ...,\n",
      "        [-0.0703,  0.0030, -0.0276,  ...,  0.0307,  0.0162,  0.0089],\n",
      "        [-0.0039, -0.0537,  0.0064,  ...,  0.0303,  0.0162,  0.0092],\n",
      "        [-0.1241,  0.0064, -0.0004,  ...,  0.0309,  0.0160,  0.0087]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "24-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0558, -0.0172, -0.0243,  ...,  0.0376,  0.0332,  0.0023],\n",
      "         [-0.0445, -0.0217,  0.0050,  ...,  0.0295,  0.0207,  0.0036],\n",
      "         [-0.0365, -0.0236,  0.0199,  ...,  0.0313,  0.0170,  0.0079],\n",
      "         ...,\n",
      "         [-0.0255, -0.0222,  0.0356,  ...,  0.0314,  0.0161,  0.0092],\n",
      "         [-0.0252, -0.0218,  0.0359,  ...,  0.0312,  0.0161,  0.0090],\n",
      "         [-0.0250, -0.0216,  0.0360,  ...,  0.0310,  0.0160,  0.0089]],\n",
      "\n",
      "        [[-0.0552,  0.0172, -0.0030,  ..., -0.0533,  0.0182,  0.0017],\n",
      "         [-0.0455, -0.0065,  0.0094,  ..., -0.0036,  0.0244,  0.0081],\n",
      "         [-0.0403, -0.0162,  0.0194,  ...,  0.0186,  0.0233,  0.0100],\n",
      "         ...,\n",
      "         [-0.0272, -0.0212,  0.0344,  ...,  0.0311,  0.0165,  0.0088],\n",
      "         [-0.0263, -0.0212,  0.0350,  ...,  0.0309,  0.0162,  0.0087],\n",
      "         [-0.0258, -0.0212,  0.0354,  ...,  0.0308,  0.0160,  0.0086]],\n",
      "\n",
      "        [[-0.0706, -0.0379,  0.0169,  ...,  0.0237, -0.0081,  0.0160],\n",
      "         [-0.0534, -0.0320,  0.0189,  ...,  0.0296,  0.0020,  0.0177],\n",
      "         [-0.0412, -0.0290,  0.0261,  ...,  0.0310,  0.0082,  0.0192],\n",
      "         ...,\n",
      "         [-0.0256, -0.0226,  0.0360,  ...,  0.0312,  0.0153,  0.0102],\n",
      "         [-0.0253, -0.0221,  0.0361,  ...,  0.0311,  0.0155,  0.0096],\n",
      "         [-0.0250, -0.0218,  0.0361,  ...,  0.0310,  0.0156,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0482,  0.0263, -0.0400,  ..., -0.0239, -0.0271,  0.0147],\n",
      "         [-0.0449,  0.0080, -0.0008,  ..., -0.0019, -0.0026,  0.0194],\n",
      "         [-0.0369, -0.0067,  0.0192,  ...,  0.0111,  0.0065,  0.0195],\n",
      "         ...,\n",
      "         [-0.0253, -0.0213,  0.0363,  ...,  0.0298,  0.0157,  0.0105],\n",
      "         [-0.0250, -0.0213,  0.0363,  ...,  0.0302,  0.0158,  0.0098],\n",
      "         [-0.0249, -0.0213,  0.0362,  ...,  0.0305,  0.0158,  0.0094]],\n",
      "\n",
      "        [[-0.0058, -0.0092,  0.0633,  ..., -0.0048,  0.0546, -0.0123],\n",
      "         [-0.0243, -0.0165,  0.0319,  ...,  0.0143,  0.0295,  0.0080],\n",
      "         [-0.0258, -0.0213,  0.0283,  ...,  0.0231,  0.0178,  0.0099],\n",
      "         ...,\n",
      "         [-0.0247, -0.0217,  0.0356,  ...,  0.0312,  0.0144,  0.0084],\n",
      "         [-0.0247, -0.0216,  0.0359,  ...,  0.0311,  0.0149,  0.0085],\n",
      "         [-0.0247, -0.0214,  0.0360,  ...,  0.0310,  0.0152,  0.0085]],\n",
      "\n",
      "        [[-0.0461, -0.0541,  0.0488,  ...,  0.0257,  0.0367,  0.0224],\n",
      "         [-0.0497, -0.0281,  0.0419,  ...,  0.0384,  0.0233,  0.0223],\n",
      "         [-0.0411, -0.0229,  0.0372,  ...,  0.0399,  0.0194,  0.0214],\n",
      "         ...,\n",
      "         [-0.0261, -0.0217,  0.0354,  ...,  0.0327,  0.0160,  0.0108],\n",
      "         [-0.0256, -0.0216,  0.0356,  ...,  0.0319,  0.0160,  0.0100],\n",
      "         [-0.0253, -0.0214,  0.0357,  ...,  0.0314,  0.0159,  0.0095]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0558, -0.0172, -0.0243,  ...,  0.0310,  0.0160,  0.0089],\n",
      "        [-0.0552,  0.0172, -0.0030,  ...,  0.0308,  0.0160,  0.0086],\n",
      "        [-0.0706, -0.0379,  0.0169,  ...,  0.0310,  0.0156,  0.0092],\n",
      "        ...,\n",
      "        [-0.0482,  0.0263, -0.0400,  ...,  0.0305,  0.0158,  0.0094],\n",
      "        [-0.0058, -0.0092,  0.0633,  ...,  0.0310,  0.0152,  0.0085],\n",
      "        [-0.0461, -0.0541,  0.0488,  ...,  0.0314,  0.0159,  0.0095]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "error: image name=>NeYoCompound_914.jpg des name=>NeYoCompound_914.txt label name=>NeYoCompound_914.txt\n",
      "25-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-5.6880e-03, -6.9117e-02,  2.4842e-02,  ...,  3.6932e-03,\n",
      "           4.8775e-02,  6.5533e-02],\n",
      "         [-3.4570e-02, -4.6848e-02,  1.2968e-02,  ...,  1.2001e-02,\n",
      "           2.3732e-02,  4.7226e-02],\n",
      "         [-3.5415e-02, -3.7309e-02,  1.7145e-02,  ...,  1.8158e-02,\n",
      "           1.6891e-02,  2.9389e-02],\n",
      "         ...,\n",
      "         [-2.5938e-02, -2.2868e-02,  3.5291e-02,  ...,  3.0079e-02,\n",
      "           1.5362e-02,  9.3438e-03],\n",
      "         [-2.5454e-02, -2.2220e-02,  3.5770e-02,  ...,  3.0336e-02,\n",
      "           1.5594e-02,  9.0152e-03],\n",
      "         [-2.5167e-02, -2.1816e-02,  3.5978e-02,  ...,  3.0459e-02,\n",
      "           1.5725e-02,  8.8642e-03]],\n",
      "\n",
      "        [[-3.0809e-02, -7.8805e-02,  1.5840e-02,  ...,  1.9359e-02,\n",
      "           2.1344e-02, -3.4565e-05],\n",
      "         [-3.0346e-02, -5.3297e-02,  2.9334e-02,  ...,  2.4980e-02,\n",
      "           8.8093e-03,  5.9910e-03],\n",
      "         [-2.8064e-02, -4.1185e-02,  3.5723e-02,  ...,  3.0718e-02,\n",
      "           1.0758e-02,  9.1336e-03],\n",
      "         ...,\n",
      "         [-2.5282e-02, -2.2844e-02,  3.6814e-02,  ...,  3.2077e-02,\n",
      "           1.6181e-02,  9.5299e-03],\n",
      "         [-2.5095e-02, -2.2190e-02,  3.6472e-02,  ...,  3.1573e-02,\n",
      "           1.6195e-02,  9.2592e-03],\n",
      "         [-2.4966e-02, -2.1791e-02,  3.6250e-02,  ...,  3.1211e-02,\n",
      "           1.6140e-02,  9.0701e-03]],\n",
      "\n",
      "        [[-1.1527e-01,  3.2460e-02,  8.0327e-02,  ...,  1.3633e-02,\n",
      "          -1.0064e-02,  5.0380e-03],\n",
      "         [-6.8623e-02,  2.5691e-04,  5.3499e-02,  ...,  1.8536e-02,\n",
      "           4.2608e-03,  6.0287e-03],\n",
      "         [-4.5530e-02, -1.0145e-02,  4.4348e-02,  ...,  2.1543e-02,\n",
      "           9.9513e-03,  9.4348e-03],\n",
      "         ...,\n",
      "         [-2.5578e-02, -2.0752e-02,  3.6589e-02,  ...,  3.1072e-02,\n",
      "           1.5776e-02,  8.7545e-03],\n",
      "         [-2.5245e-02, -2.0988e-02,  3.6389e-02,  ...,  3.1039e-02,\n",
      "           1.5850e-02,  8.6302e-03],\n",
      "         [-2.5054e-02, -2.1111e-02,  3.6256e-02,  ...,  3.0939e-02,\n",
      "           1.5872e-02,  8.5889e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.3390e-03, -2.1957e-02, -5.8554e-03,  ...,  2.7521e-03,\n",
      "           2.2044e-02,  8.9208e-02],\n",
      "         [-1.4797e-02, -2.5740e-02,  1.7789e-03,  ...,  1.6777e-02,\n",
      "           1.2599e-02,  5.6409e-02],\n",
      "         [-2.0318e-02, -2.5641e-02,  1.4053e-02,  ...,  2.3551e-02,\n",
      "           1.2642e-02,  3.6421e-02],\n",
      "         ...,\n",
      "         [-2.5164e-02, -2.2245e-02,  3.5101e-02,  ...,  3.0299e-02,\n",
      "           1.5752e-02,  9.9837e-03],\n",
      "         [-2.5088e-02, -2.1909e-02,  3.5601e-02,  ...,  3.0467e-02,\n",
      "           1.5859e-02,  9.3222e-03],\n",
      "         [-2.5006e-02, -2.1672e-02,  3.5843e-02,  ...,  3.0551e-02,\n",
      "           1.5899e-02,  8.9874e-03]],\n",
      "\n",
      "        [[-2.4057e-02, -1.5052e-02, -7.0215e-02,  ...,  1.9601e-02,\n",
      "           6.2979e-02,  9.4367e-02],\n",
      "         [-4.6895e-02, -6.7664e-03, -3.9968e-02,  ...,  2.6079e-02,\n",
      "           3.6267e-02,  5.2119e-02],\n",
      "         [-4.5812e-02, -1.2807e-02, -8.6863e-03,  ...,  3.0543e-02,\n",
      "           2.6131e-02,  3.1938e-02],\n",
      "         ...,\n",
      "         [-2.8302e-02, -2.1148e-02,  3.3257e-02,  ...,  3.1314e-02,\n",
      "           1.6985e-02,  9.7943e-03],\n",
      "         [-2.7003e-02, -2.1241e-02,  3.4461e-02,  ...,  3.1083e-02,\n",
      "           1.6615e-02,  9.2764e-03],\n",
      "         [-2.6158e-02, -2.1265e-02,  3.5146e-02,  ...,  3.0917e-02,\n",
      "           1.6352e-02,  9.0001e-03]],\n",
      "\n",
      "        [[-9.0309e-02, -3.3848e-03,  2.0181e-03,  ...,  2.9970e-02,\n",
      "           1.7129e-02,  1.9498e-02],\n",
      "         [-6.0570e-02, -1.5466e-02,  1.9473e-02,  ...,  3.1846e-02,\n",
      "           8.9629e-03,  9.4372e-03],\n",
      "         [-4.4876e-02, -1.9938e-02,  2.6697e-02,  ...,  3.0699e-02,\n",
      "           9.6041e-03,  9.2885e-03],\n",
      "         ...,\n",
      "         [-2.6321e-02, -2.1864e-02,  3.5819e-02,  ...,  3.0634e-02,\n",
      "           1.5278e-02,  8.5883e-03],\n",
      "         [-2.5726e-02, -2.1655e-02,  3.5965e-02,  ...,  3.0659e-02,\n",
      "           1.5501e-02,  8.6131e-03],\n",
      "         [-2.5352e-02, -2.1498e-02,  3.6023e-02,  ...,  3.0665e-02,\n",
      "           1.5625e-02,  8.6414e-03]]], grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0057, -0.0691,  0.0248,  ...,  0.0305,  0.0157,  0.0089],\n",
      "        [-0.0308, -0.0788,  0.0158,  ...,  0.0312,  0.0161,  0.0091],\n",
      "        [-0.1153,  0.0325,  0.0803,  ...,  0.0309,  0.0159,  0.0086],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0220, -0.0059,  ...,  0.0306,  0.0159,  0.0090],\n",
      "        [-0.0241, -0.0151, -0.0702,  ...,  0.0309,  0.0164,  0.0090],\n",
      "        [-0.0903, -0.0034,  0.0020,  ...,  0.0307,  0.0156,  0.0086]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "error: image name=>DulceMaria_11.jpg des name=>DulceMaria_11.txt label name=>DulceMaria_11.txt\n",
      "26-torch.Size([16, 3, 448, 448]) 16 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0120,  0.0013,  0.0261,  ..., -0.0077,  0.0156,  0.0848],\n",
      "         [-0.0233, -0.0133,  0.0182,  ...,  0.0199,  0.0130,  0.0504],\n",
      "         [-0.0260, -0.0192,  0.0240,  ...,  0.0263,  0.0125,  0.0314],\n",
      "         ...,\n",
      "         [-0.0256, -0.0216,  0.0360,  ...,  0.0303,  0.0156,  0.0102],\n",
      "         [-0.0253, -0.0215,  0.0361,  ...,  0.0303,  0.0158,  0.0096],\n",
      "         [-0.0251, -0.0214,  0.0362,  ...,  0.0304,  0.0158,  0.0092]],\n",
      "\n",
      "        [[-0.0448, -0.0522, -0.0072,  ..., -0.0450,  0.0204,  0.0210],\n",
      "         [-0.0383, -0.0412,  0.0155,  ..., -0.0047,  0.0163,  0.0152],\n",
      "         [-0.0331, -0.0342,  0.0264,  ...,  0.0135,  0.0142,  0.0140],\n",
      "         ...,\n",
      "         [-0.0253, -0.0221,  0.0361,  ...,  0.0305,  0.0155,  0.0097],\n",
      "         [-0.0251, -0.0217,  0.0361,  ...,  0.0307,  0.0157,  0.0093],\n",
      "         [-0.0249, -0.0215,  0.0361,  ...,  0.0307,  0.0158,  0.0091]],\n",
      "\n",
      "        [[-0.0350, -0.0240, -0.0165,  ..., -0.0054,  0.0485,  0.0055],\n",
      "         [-0.0349, -0.0223,  0.0072,  ...,  0.0178,  0.0230,  0.0115],\n",
      "         [-0.0322, -0.0234,  0.0206,  ...,  0.0270,  0.0175,  0.0126],\n",
      "         ...,\n",
      "         [-0.0258, -0.0222,  0.0355,  ...,  0.0310,  0.0162,  0.0087],\n",
      "         [-0.0255, -0.0218,  0.0358,  ...,  0.0308,  0.0162,  0.0086],\n",
      "         [-0.0252, -0.0216,  0.0359,  ...,  0.0307,  0.0161,  0.0086]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0629,  0.0308, -0.0360,  ...,  0.0078,  0.0557,  0.0503],\n",
      "         [-0.0434,  0.0004, -0.0093,  ...,  0.0236,  0.0321,  0.0326],\n",
      "         [-0.0338, -0.0140,  0.0090,  ...,  0.0306,  0.0234,  0.0231],\n",
      "         ...,\n",
      "         [-0.0250, -0.0208,  0.0345,  ...,  0.0316,  0.0157,  0.0093],\n",
      "         [-0.0249, -0.0209,  0.0352,  ...,  0.0313,  0.0157,  0.0090],\n",
      "         [-0.0248, -0.0210,  0.0356,  ...,  0.0311,  0.0157,  0.0089]],\n",
      "\n",
      "        [[-0.0330, -0.0087,  0.0071,  ..., -0.0177,  0.0160,  0.0420],\n",
      "         [-0.0257, -0.0179,  0.0170,  ...,  0.0106,  0.0125,  0.0220],\n",
      "         [-0.0224, -0.0194,  0.0245,  ...,  0.0241,  0.0116,  0.0167],\n",
      "         ...,\n",
      "         [-0.0245, -0.0203,  0.0353,  ...,  0.0316,  0.0157,  0.0094],\n",
      "         [-0.0247, -0.0206,  0.0356,  ...,  0.0313,  0.0158,  0.0092],\n",
      "         [-0.0247, -0.0208,  0.0358,  ...,  0.0311,  0.0159,  0.0090]],\n",
      "\n",
      "        [[-0.0364,  0.0266,  0.0275,  ...,  0.0322,  0.0200,  0.0058],\n",
      "         [-0.0281,  0.0002,  0.0110,  ...,  0.0336,  0.0051,  0.0021],\n",
      "         [-0.0253, -0.0147,  0.0149,  ...,  0.0357,  0.0049,  0.0078],\n",
      "         ...,\n",
      "         [-0.0250, -0.0215,  0.0345,  ...,  0.0319,  0.0144,  0.0093],\n",
      "         [-0.0250, -0.0214,  0.0352,  ...,  0.0314,  0.0150,  0.0091],\n",
      "         [-0.0250, -0.0213,  0.0356,  ...,  0.0311,  0.0153,  0.0089]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0120,  0.0013,  0.0261,  ...,  0.0304,  0.0158,  0.0092],\n",
      "        [-0.0448, -0.0522, -0.0072,  ...,  0.0307,  0.0158,  0.0091],\n",
      "        [-0.0350, -0.0240, -0.0165,  ...,  0.0307,  0.0161,  0.0086],\n",
      "        ...,\n",
      "        [-0.0629,  0.0308, -0.0360,  ...,  0.0311,  0.0157,  0.0089],\n",
      "        [-0.0330, -0.0087,  0.0071,  ...,  0.0311,  0.0159,  0.0090],\n",
      "        [-0.0364,  0.0266,  0.0275,  ...,  0.0311,  0.0153,  0.0089]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "27-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0593, -0.0195, -0.0167,  ...,  0.0301,  0.0419, -0.0120],\n",
      "         [-0.0499, -0.0226,  0.0022,  ...,  0.0345,  0.0253,  0.0019],\n",
      "         [-0.0404, -0.0229,  0.0169,  ...,  0.0361,  0.0202,  0.0071],\n",
      "         ...,\n",
      "         [-0.0264, -0.0219,  0.0353,  ...,  0.0324,  0.0168,  0.0090],\n",
      "         [-0.0258, -0.0217,  0.0357,  ...,  0.0318,  0.0165,  0.0089],\n",
      "         [-0.0254, -0.0215,  0.0358,  ...,  0.0314,  0.0163,  0.0088]],\n",
      "\n",
      "        [[-0.0567,  0.0040,  0.0220,  ...,  0.0171,  0.0354,  0.0274],\n",
      "         [-0.0430, -0.0069,  0.0242,  ...,  0.0296,  0.0157,  0.0219],\n",
      "         [-0.0325, -0.0131,  0.0287,  ...,  0.0321,  0.0093,  0.0159],\n",
      "         ...,\n",
      "         [-0.0248, -0.0208,  0.0357,  ...,  0.0324,  0.0145,  0.0081],\n",
      "         [-0.0247, -0.0210,  0.0359,  ...,  0.0318,  0.0150,  0.0082],\n",
      "         [-0.0247, -0.0211,  0.0360,  ...,  0.0314,  0.0154,  0.0084]],\n",
      "\n",
      "        [[-0.0498, -0.0188, -0.0061,  ...,  0.0473, -0.0279,  0.0680],\n",
      "         [-0.0466, -0.0202,  0.0077,  ...,  0.0409, -0.0046,  0.0460],\n",
      "         [-0.0401, -0.0205,  0.0206,  ...,  0.0370,  0.0064,  0.0288],\n",
      "         ...,\n",
      "         [-0.0268, -0.0213,  0.0357,  ...,  0.0314,  0.0162,  0.0093],\n",
      "         [-0.0261, -0.0213,  0.0359,  ...,  0.0311,  0.0161,  0.0090],\n",
      "         [-0.0256, -0.0213,  0.0360,  ...,  0.0309,  0.0160,  0.0088]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0043, -0.0436,  0.0523,  ..., -0.0137, -0.0010, -0.0197],\n",
      "         [-0.0165, -0.0301,  0.0417,  ...,  0.0120,  0.0002,  0.0033],\n",
      "         [-0.0219, -0.0255,  0.0397,  ...,  0.0240,  0.0057,  0.0104],\n",
      "         ...,\n",
      "         [-0.0255, -0.0220,  0.0366,  ...,  0.0316,  0.0156,  0.0091],\n",
      "         [-0.0253, -0.0217,  0.0364,  ...,  0.0313,  0.0157,  0.0088],\n",
      "         [-0.0252, -0.0215,  0.0362,  ...,  0.0310,  0.0158,  0.0087]],\n",
      "\n",
      "        [[ 0.0050,  0.0206,  0.0555,  ..., -0.0028,  0.0605,  0.0148],\n",
      "         [-0.0071, -0.0058,  0.0438,  ...,  0.0208,  0.0396,  0.0127],\n",
      "         [-0.0161, -0.0145,  0.0390,  ...,  0.0307,  0.0269,  0.0126],\n",
      "         ...,\n",
      "         [-0.0249, -0.0212,  0.0354,  ...,  0.0319,  0.0158,  0.0087],\n",
      "         [-0.0249, -0.0212,  0.0356,  ...,  0.0314,  0.0157,  0.0086],\n",
      "         [-0.0249, -0.0212,  0.0357,  ...,  0.0311,  0.0157,  0.0086]],\n",
      "\n",
      "        [[-0.0714, -0.0089,  0.0180,  ...,  0.0031,  0.0304,  0.0363],\n",
      "         [-0.0543, -0.0153,  0.0179,  ...,  0.0225,  0.0262,  0.0217],\n",
      "         [-0.0422, -0.0187,  0.0232,  ...,  0.0286,  0.0234,  0.0169],\n",
      "         ...,\n",
      "         [-0.0265, -0.0216,  0.0355,  ...,  0.0312,  0.0169,  0.0098],\n",
      "         [-0.0258, -0.0215,  0.0358,  ...,  0.0310,  0.0165,  0.0094],\n",
      "         [-0.0254, -0.0214,  0.0359,  ...,  0.0309,  0.0162,  0.0091]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0593, -0.0195, -0.0167,  ...,  0.0314,  0.0163,  0.0088],\n",
      "        [-0.0567,  0.0040,  0.0220,  ...,  0.0314,  0.0154,  0.0084],\n",
      "        [-0.0498, -0.0188, -0.0061,  ...,  0.0309,  0.0160,  0.0088],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0436,  0.0523,  ...,  0.0310,  0.0158,  0.0087],\n",
      "        [ 0.0050,  0.0206,  0.0555,  ...,  0.0311,  0.0157,  0.0086],\n",
      "        [-0.0714, -0.0089,  0.0180,  ...,  0.0309,  0.0162,  0.0091]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "28-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-6.1449e-02, -3.2913e-02, -1.2694e-02,  ...,  9.8327e-03,\n",
      "          -5.2866e-03,  2.4027e-02],\n",
      "         [-4.8172e-02, -3.4675e-02,  1.2006e-02,  ...,  1.0294e-02,\n",
      "           2.4082e-03,  2.4963e-02],\n",
      "         [-3.8779e-02, -3.1533e-02,  2.4600e-02,  ...,  1.5868e-02,\n",
      "           5.9839e-03,  2.1500e-02],\n",
      "         ...,\n",
      "         [-2.6174e-02, -2.2562e-02,  3.6085e-02,  ...,  2.9568e-02,\n",
      "           1.5012e-02,  1.0010e-02],\n",
      "         [-2.5662e-02, -2.2071e-02,  3.6176e-02,  ...,  3.0070e-02,\n",
      "           1.5385e-02,  9.4440e-03],\n",
      "         [-2.5331e-02, -2.1755e-02,  3.6187e-02,  ...,  3.0343e-02,\n",
      "           1.5593e-02,  9.1193e-03]],\n",
      "\n",
      "        [[-5.7437e-02, -2.8787e-02, -8.8547e-03,  ...,  2.4490e-02,\n",
      "           4.8340e-02,  1.8172e-02],\n",
      "         [-5.1666e-02, -2.0036e-02,  1.4818e-02,  ...,  2.9370e-02,\n",
      "           3.0623e-02,  2.0672e-02],\n",
      "         [-4.0962e-02, -2.0613e-02,  2.6410e-02,  ...,  3.0599e-02,\n",
      "           2.3554e-02,  2.1380e-02],\n",
      "         ...,\n",
      "         [-2.6042e-02, -2.1797e-02,  3.5910e-02,  ...,  3.1060e-02,\n",
      "           1.6905e-02,  1.0569e-02],\n",
      "         [-2.5548e-02, -2.1573e-02,  3.5988e-02,  ...,  3.0911e-02,\n",
      "           1.6550e-02,  9.8361e-03],\n",
      "         [-2.5243e-02, -2.1413e-02,  3.6013e-02,  ...,  3.0802e-02,\n",
      "           1.6296e-02,  9.3844e-03]],\n",
      "\n",
      "        [[-1.0038e-01, -2.5415e-02,  1.8544e-02,  ...,  3.2343e-02,\n",
      "           3.1343e-02, -1.9668e-02],\n",
      "         [-6.4422e-02, -3.0577e-02,  1.9607e-02,  ...,  4.3733e-02,\n",
      "           1.6587e-02, -3.2983e-03],\n",
      "         [-4.6276e-02, -2.9861e-02,  2.4348e-02,  ...,  3.9041e-02,\n",
      "           1.4119e-02,  5.1784e-03],\n",
      "         ...,\n",
      "         [-2.6108e-02, -2.2603e-02,  3.5240e-02,  ...,  3.0899e-02,\n",
      "           1.5214e-02,  8.3881e-03],\n",
      "         [-2.5570e-02, -2.2117e-02,  3.5631e-02,  ...,  3.0702e-02,\n",
      "           1.5418e-02,  8.4107e-03],\n",
      "         [-2.5246e-02, -2.1797e-02,  3.5840e-02,  ...,  3.0615e-02,\n",
      "           1.5555e-02,  8.4600e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.6773e-03, -5.2285e-02,  1.2166e-02,  ...,  2.3284e-02,\n",
      "           3.9949e-02,  6.7488e-02],\n",
      "         [-2.7443e-02, -3.8489e-02,  1.5502e-02,  ...,  2.7248e-02,\n",
      "           1.9320e-02,  4.1114e-02],\n",
      "         [-3.1158e-02, -3.1047e-02,  2.2451e-02,  ...,  2.8123e-02,\n",
      "           1.6964e-02,  2.7687e-02],\n",
      "         ...,\n",
      "         [-2.6270e-02, -2.2053e-02,  3.4810e-02,  ...,  3.0246e-02,\n",
      "           1.6120e-02,  9.5679e-03],\n",
      "         [-2.5747e-02, -2.1679e-02,  3.5266e-02,  ...,  3.0395e-02,\n",
      "           1.6053e-02,  9.1299e-03],\n",
      "         [-2.5395e-02, -2.1458e-02,  3.5549e-02,  ...,  3.0489e-02,\n",
      "           1.5992e-02,  8.9081e-03]],\n",
      "\n",
      "        [[-7.3916e-02, -3.3349e-02, -7.3785e-03,  ..., -3.4060e-03,\n",
      "           1.2973e-02, -3.9862e-03],\n",
      "         [-5.4771e-02, -3.0893e-02,  3.2582e-03,  ...,  2.0069e-02,\n",
      "           1.2148e-02,  7.8859e-03],\n",
      "         [-3.7798e-02, -2.8750e-02,  1.6033e-02,  ...,  2.7485e-02,\n",
      "           1.3042e-02,  1.3361e-02],\n",
      "         ...,\n",
      "         [-2.4593e-02, -2.1793e-02,  3.4739e-02,  ...,  3.1657e-02,\n",
      "           1.6160e-02,  9.6527e-03],\n",
      "         [-2.4626e-02, -2.1493e-02,  3.5241e-02,  ...,  3.1380e-02,\n",
      "           1.6130e-02,  9.2440e-03],\n",
      "         [-2.4669e-02, -2.1331e-02,  3.5536e-02,  ...,  3.1148e-02,\n",
      "           1.6065e-02,  9.0080e-03]],\n",
      "\n",
      "        [[-1.0475e-01,  3.4716e-02, -6.5869e-02,  ..., -8.2045e-04,\n",
      "           3.5552e-02,  1.0658e-02],\n",
      "         [-6.4309e-02, -3.3155e-07, -2.7500e-02,  ...,  1.2187e-02,\n",
      "           1.8885e-02,  6.8800e-03],\n",
      "         [-4.5962e-02, -1.3544e-02,  2.8966e-04,  ...,  1.9814e-02,\n",
      "           1.4922e-02,  1.0350e-02],\n",
      "         ...,\n",
      "         [-2.6502e-02, -2.1026e-02,  3.5088e-02,  ...,  3.0839e-02,\n",
      "           1.5332e-02,  9.1179e-03],\n",
      "         [-2.5895e-02, -2.1101e-02,  3.5703e-02,  ...,  3.0886e-02,\n",
      "           1.5546e-02,  8.9349e-03],\n",
      "         [-2.5503e-02, -2.1147e-02,  3.5965e-02,  ...,  3.0845e-02,\n",
      "           1.5674e-02,  8.8355e-03]]], grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0614, -0.0329, -0.0127,  ...,  0.0303,  0.0156,  0.0091],\n",
      "        [-0.0574, -0.0288, -0.0089,  ...,  0.0308,  0.0163,  0.0094],\n",
      "        [-0.1004, -0.0254,  0.0185,  ...,  0.0306,  0.0156,  0.0085],\n",
      "        ...,\n",
      "        [-0.0037, -0.0523,  0.0122,  ...,  0.0305,  0.0160,  0.0089],\n",
      "        [-0.0739, -0.0333, -0.0074,  ...,  0.0311,  0.0161,  0.0090],\n",
      "        [-0.1048,  0.0347, -0.0659,  ...,  0.0308,  0.0157,  0.0088]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-4.6008e-02, -2.1944e-02,  4.0873e-02,  ..., -2.3467e-02,\n",
      "           4.8328e-02, -8.9022e-03],\n",
      "         [-3.3782e-02, -2.7676e-02,  3.7890e-02,  ...,  5.7024e-03,\n",
      "           3.4707e-02,  1.5498e-02],\n",
      "         [-2.7372e-02, -2.8059e-02,  3.4839e-02,  ...,  2.0351e-02,\n",
      "           2.7233e-02,  1.9897e-02],\n",
      "         ...,\n",
      "         [-2.4974e-02, -2.2345e-02,  3.5520e-02,  ...,  3.0808e-02,\n",
      "           1.6900e-02,  9.7383e-03],\n",
      "         [-2.4944e-02, -2.1887e-02,  3.5753e-02,  ...,  3.0799e-02,\n",
      "           1.6472e-02,  9.1919e-03],\n",
      "         [-2.4898e-02, -2.1600e-02,  3.5892e-02,  ...,  3.0763e-02,\n",
      "           1.6201e-02,  8.9133e-03]],\n",
      "\n",
      "        [[-5.6132e-02,  7.2714e-03,  2.5638e-02,  ..., -2.9602e-02,\n",
      "           2.1966e-02,  1.6580e-02],\n",
      "         [-4.3128e-02, -1.8798e-02,  3.3374e-02,  ...,  1.0268e-02,\n",
      "           1.1264e-02,  1.2975e-02],\n",
      "         [-3.6427e-02, -2.6427e-02,  3.5880e-02,  ...,  2.5605e-02,\n",
      "           1.2016e-02,  1.5277e-02],\n",
      "         ...,\n",
      "         [-2.6186e-02, -2.3096e-02,  3.6380e-02,  ...,  3.1270e-02,\n",
      "           1.6083e-02,  1.0102e-02],\n",
      "         [-2.5688e-02, -2.2410e-02,  3.6291e-02,  ...,  3.0986e-02,\n",
      "           1.6065e-02,  9.5500e-03],\n",
      "         [-2.5356e-02, -2.1956e-02,  3.6217e-02,  ...,  3.0804e-02,\n",
      "           1.6010e-02,  9.2076e-03]],\n",
      "\n",
      "        [[-1.2162e-02,  4.8710e-04, -5.0679e-02,  ...,  1.9387e-03,\n",
      "          -6.9634e-03,  2.8468e-02],\n",
      "         [-1.6943e-02, -7.9520e-05, -1.9741e-02,  ...,  3.2715e-02,\n",
      "          -1.9690e-03,  1.8605e-02],\n",
      "         [-2.1504e-02, -9.5206e-03,  6.3958e-03,  ...,  3.7571e-02,\n",
      "           5.3275e-03,  1.5024e-02],\n",
      "         ...,\n",
      "         [-2.5334e-02, -2.1802e-02,  3.5526e-02,  ...,  3.3106e-02,\n",
      "           1.6209e-02,  9.0662e-03],\n",
      "         [-2.5180e-02, -2.1734e-02,  3.5850e-02,  ...,  3.2275e-02,\n",
      "           1.6191e-02,  8.9136e-03],\n",
      "         [-2.5043e-02, -2.1607e-02,  3.5961e-02,  ...,  3.1699e-02,\n",
      "           1.6110e-02,  8.8322e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.3038e-02,  2.4749e-02,  7.2211e-02,  ...,  2.0063e-02,\n",
      "           6.1110e-02,  7.0255e-02],\n",
      "         [-5.6219e-02, -1.8616e-02,  4.8879e-02,  ...,  2.1688e-02,\n",
      "           3.1039e-02,  3.7199e-02],\n",
      "         [-4.2225e-02, -2.7068e-02,  4.1432e-02,  ...,  2.4043e-02,\n",
      "           2.0976e-02,  2.6565e-02],\n",
      "         ...,\n",
      "         [-2.6232e-02, -2.3057e-02,  3.6759e-02,  ...,  3.0211e-02,\n",
      "           1.5822e-02,  9.7116e-03],\n",
      "         [-2.5699e-02, -2.2379e-02,  3.6539e-02,  ...,  3.0410e-02,\n",
      "           1.5879e-02,  9.1946e-03],\n",
      "         [-2.5357e-02, -2.1937e-02,  3.6373e-02,  ...,  3.0516e-02,\n",
      "           1.5900e-02,  8.9264e-03]],\n",
      "\n",
      "        [[-2.2020e-02, -5.0044e-02, -7.2886e-03,  ...,  8.2797e-03,\n",
      "           4.8290e-02,  8.2464e-02],\n",
      "         [-3.6008e-02, -4.0110e-02,  2.6255e-03,  ...,  2.4240e-02,\n",
      "           3.0052e-02,  5.1538e-02],\n",
      "         [-3.4458e-02, -3.5214e-02,  1.4628e-02,  ...,  2.9154e-02,\n",
      "           2.3061e-02,  3.4750e-02],\n",
      "         ...,\n",
      "         [-2.5452e-02, -2.2629e-02,  3.4580e-02,  ...,  3.1544e-02,\n",
      "           1.5979e-02,  1.0572e-02],\n",
      "         [-2.5140e-02, -2.2072e-02,  3.5267e-02,  ...,  3.1325e-02,\n",
      "           1.5884e-02,  9.7847e-03],\n",
      "         [-2.4968e-02, -2.1736e-02,  3.5651e-02,  ...,  3.1130e-02,\n",
      "           1.5841e-02,  9.3320e-03]],\n",
      "\n",
      "        [[-4.7747e-02, -4.7194e-02,  3.2957e-02,  ..., -2.5951e-02,\n",
      "           2.5968e-02,  4.1475e-02],\n",
      "         [-3.8671e-02, -3.7448e-02,  2.7876e-02,  ...,  2.8681e-03,\n",
      "           1.2197e-02,  3.0706e-02],\n",
      "         [-3.1039e-02, -3.0193e-02,  2.9262e-02,  ...,  1.7474e-02,\n",
      "           1.1089e-02,  2.4099e-02],\n",
      "         ...,\n",
      "         [-2.5095e-02, -2.1612e-02,  3.5271e-02,  ...,  3.0859e-02,\n",
      "           1.5388e-02,  1.0117e-02],\n",
      "         [-2.4992e-02, -2.1423e-02,  3.5574e-02,  ...,  3.0875e-02,\n",
      "           1.5621e-02,  9.5400e-03],\n",
      "         [-2.4923e-02, -2.1323e-02,  3.5758e-02,  ...,  3.0826e-02,\n",
      "           1.5738e-02,  9.2011e-03]]], grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0460, -0.0219,  0.0409,  ...,  0.0308,  0.0162,  0.0089],\n",
      "        [-0.0561,  0.0073,  0.0256,  ...,  0.0308,  0.0160,  0.0092],\n",
      "        [-0.0122,  0.0005, -0.0507,  ...,  0.0317,  0.0161,  0.0088],\n",
      "        ...,\n",
      "        [-0.0730,  0.0247,  0.0722,  ...,  0.0305,  0.0159,  0.0089],\n",
      "        [-0.0220, -0.0500, -0.0073,  ...,  0.0311,  0.0158,  0.0093],\n",
      "        [-0.0477, -0.0472,  0.0330,  ...,  0.0308,  0.0157,  0.0092]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "30-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0971,  0.0037,  0.0182,  ..., -0.0076,  0.0108,  0.0259],\n",
      "         [-0.0666, -0.0185,  0.0180,  ...,  0.0153,  0.0106,  0.0225],\n",
      "         [-0.0483, -0.0235,  0.0243,  ...,  0.0252,  0.0123,  0.0214],\n",
      "         ...,\n",
      "         [-0.0262, -0.0221,  0.0355,  ...,  0.0310,  0.0160,  0.0105],\n",
      "         [-0.0256, -0.0218,  0.0358,  ...,  0.0309,  0.0160,  0.0097],\n",
      "         [-0.0253, -0.0215,  0.0359,  ...,  0.0308,  0.0160,  0.0093]],\n",
      "\n",
      "        [[-0.0815,  0.0103,  0.0162,  ...,  0.0198,  0.0408, -0.0114],\n",
      "         [-0.0571, -0.0025,  0.0185,  ...,  0.0236,  0.0195,  0.0014],\n",
      "         [-0.0411, -0.0119,  0.0236,  ...,  0.0272,  0.0138,  0.0064],\n",
      "         ...,\n",
      "         [-0.0256, -0.0213,  0.0352,  ...,  0.0304,  0.0152,  0.0088],\n",
      "         [-0.0253, -0.0214,  0.0356,  ...,  0.0304,  0.0155,  0.0088],\n",
      "         [-0.0251, -0.0213,  0.0358,  ...,  0.0305,  0.0156,  0.0088]],\n",
      "\n",
      "        [[-0.0631, -0.0716,  0.0347,  ..., -0.0046,  0.0367,  0.0219],\n",
      "         [-0.0475, -0.0433,  0.0352,  ...,  0.0167,  0.0300,  0.0244],\n",
      "         [-0.0380, -0.0310,  0.0343,  ...,  0.0247,  0.0251,  0.0218],\n",
      "         ...,\n",
      "         [-0.0259, -0.0214,  0.0354,  ...,  0.0314,  0.0171,  0.0096],\n",
      "         [-0.0255, -0.0213,  0.0356,  ...,  0.0312,  0.0167,  0.0092],\n",
      "         [-0.0252, -0.0212,  0.0358,  ...,  0.0310,  0.0164,  0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0029,  0.0106, -0.0172,  ...,  0.0372,  0.0565,  0.0030],\n",
      "         [-0.0107, -0.0096, -0.0040,  ...,  0.0313,  0.0287,  0.0198],\n",
      "         [-0.0190, -0.0201,  0.0120,  ...,  0.0303,  0.0197,  0.0243],\n",
      "         ...,\n",
      "         [-0.0254, -0.0213,  0.0351,  ...,  0.0316,  0.0153,  0.0108],\n",
      "         [-0.0253, -0.0212,  0.0356,  ...,  0.0314,  0.0155,  0.0100],\n",
      "         [-0.0252, -0.0211,  0.0358,  ...,  0.0312,  0.0157,  0.0094]],\n",
      "\n",
      "        [[-0.0509, -0.0853,  0.0677,  ..., -0.0018,  0.0357,  0.0364],\n",
      "         [-0.0354, -0.0505,  0.0444,  ...,  0.0181,  0.0261,  0.0296],\n",
      "         [-0.0300, -0.0369,  0.0374,  ...,  0.0262,  0.0213,  0.0229],\n",
      "         ...,\n",
      "         [-0.0256, -0.0219,  0.0352,  ...,  0.0310,  0.0168,  0.0092],\n",
      "         [-0.0254, -0.0216,  0.0354,  ...,  0.0309,  0.0166,  0.0089],\n",
      "         [-0.0252, -0.0214,  0.0356,  ...,  0.0308,  0.0163,  0.0087]],\n",
      "\n",
      "        [[ 0.0427, -0.0308, -0.0053,  ...,  0.0237,  0.0214,  0.0186],\n",
      "         [ 0.0034, -0.0265,  0.0060,  ...,  0.0229,  0.0145,  0.0183],\n",
      "         [-0.0149, -0.0244,  0.0181,  ...,  0.0257,  0.0152,  0.0184],\n",
      "         ...,\n",
      "         [-0.0253, -0.0211,  0.0350,  ...,  0.0308,  0.0165,  0.0098],\n",
      "         [-0.0252, -0.0211,  0.0354,  ...,  0.0309,  0.0163,  0.0093],\n",
      "         [-0.0251, -0.0211,  0.0357,  ...,  0.0308,  0.0162,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0971,  0.0037,  0.0182,  ...,  0.0308,  0.0160,  0.0093],\n",
      "        [-0.0815,  0.0103,  0.0162,  ...,  0.0305,  0.0156,  0.0088],\n",
      "        [-0.0631, -0.0716,  0.0347,  ...,  0.0310,  0.0164,  0.0089],\n",
      "        ...,\n",
      "        [-0.0029,  0.0106, -0.0172,  ...,  0.0312,  0.0157,  0.0094],\n",
      "        [-0.0509, -0.0853,  0.0677,  ...,  0.0308,  0.0163,  0.0087],\n",
      "        [ 0.0427, -0.0308, -0.0053,  ...,  0.0308,  0.0162,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "31-torch.Size([16, 3, 448, 448]) 16 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0402, -0.0664,  0.0581,  ...,  0.0384,  0.0373,  0.0078],\n",
      "         [-0.0317, -0.0525,  0.0466,  ...,  0.0340,  0.0275,  0.0193],\n",
      "         [-0.0297, -0.0431,  0.0433,  ...,  0.0342,  0.0236,  0.0229],\n",
      "         ...,\n",
      "         [-0.0259, -0.0232,  0.0368,  ...,  0.0314,  0.0160,  0.0102],\n",
      "         [-0.0255, -0.0223,  0.0365,  ...,  0.0311,  0.0159,  0.0095],\n",
      "         [-0.0253, -0.0218,  0.0363,  ...,  0.0309,  0.0158,  0.0091]],\n",
      "\n",
      "        [[-0.1020,  0.0197,  0.0158,  ..., -0.0463,  0.0284, -0.0037],\n",
      "         [-0.0718, -0.0040,  0.0109,  ..., -0.0010,  0.0251, -0.0078],\n",
      "         [-0.0501, -0.0152,  0.0184,  ...,  0.0183,  0.0222, -0.0010],\n",
      "         ...,\n",
      "         [-0.0261, -0.0218,  0.0347,  ...,  0.0311,  0.0165,  0.0086],\n",
      "         [-0.0255, -0.0216,  0.0353,  ...,  0.0309,  0.0163,  0.0087],\n",
      "         [-0.0251, -0.0215,  0.0356,  ...,  0.0308,  0.0161,  0.0088]],\n",
      "\n",
      "        [[-0.0045, -0.0522, -0.0723,  ..., -0.0254,  0.0137,  0.0649],\n",
      "         [-0.0255, -0.0409, -0.0261,  ..., -0.0052,  0.0102,  0.0316],\n",
      "         [-0.0288, -0.0330,  0.0008,  ...,  0.0076,  0.0082,  0.0231],\n",
      "         ...,\n",
      "         [-0.0256, -0.0222,  0.0339,  ...,  0.0293,  0.0149,  0.0096],\n",
      "         [-0.0253, -0.0218,  0.0348,  ...,  0.0300,  0.0154,  0.0092],\n",
      "         [-0.0251, -0.0216,  0.0353,  ...,  0.0304,  0.0156,  0.0090]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0713, -0.0186, -0.0054,  ...,  0.0164,  0.0372,  0.0492],\n",
      "         [-0.0519, -0.0259,  0.0110,  ...,  0.0254,  0.0231,  0.0248],\n",
      "         [-0.0403, -0.0272,  0.0236,  ...,  0.0277,  0.0206,  0.0184],\n",
      "         ...,\n",
      "         [-0.0261, -0.0230,  0.0359,  ...,  0.0308,  0.0168,  0.0094],\n",
      "         [-0.0256, -0.0224,  0.0360,  ...,  0.0308,  0.0164,  0.0091],\n",
      "         [-0.0253, -0.0220,  0.0360,  ...,  0.0307,  0.0162,  0.0089]],\n",
      "\n",
      "        [[-0.0702,  0.0393,  0.0179,  ..., -0.0224,  0.0336,  0.0072],\n",
      "         [-0.0463, -0.0006,  0.0217,  ...,  0.0043,  0.0253,  0.0123],\n",
      "         [-0.0364, -0.0161,  0.0274,  ...,  0.0171,  0.0200,  0.0136],\n",
      "         ...,\n",
      "         [-0.0253, -0.0222,  0.0362,  ...,  0.0301,  0.0163,  0.0095],\n",
      "         [-0.0250, -0.0218,  0.0362,  ...,  0.0303,  0.0161,  0.0092],\n",
      "         [-0.0249, -0.0216,  0.0362,  ...,  0.0305,  0.0161,  0.0090]],\n",
      "\n",
      "        [[-0.0367, -0.0348,  0.0470,  ...,  0.0053,  0.0357,  0.0399],\n",
      "         [-0.0403, -0.0373,  0.0430,  ...,  0.0138,  0.0247,  0.0282],\n",
      "         [-0.0346, -0.0342,  0.0419,  ...,  0.0215,  0.0196,  0.0231],\n",
      "         ...,\n",
      "         [-0.0255, -0.0231,  0.0370,  ...,  0.0305,  0.0160,  0.0104],\n",
      "         [-0.0252, -0.0224,  0.0366,  ...,  0.0306,  0.0159,  0.0097],\n",
      "         [-0.0250, -0.0219,  0.0364,  ...,  0.0306,  0.0159,  0.0093]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0402, -0.0664,  0.0581,  ...,  0.0309,  0.0158,  0.0091],\n",
      "        [-0.1020,  0.0197,  0.0158,  ...,  0.0308,  0.0161,  0.0088],\n",
      "        [-0.0045, -0.0522, -0.0723,  ...,  0.0304,  0.0156,  0.0090],\n",
      "        ...,\n",
      "        [-0.0713, -0.0186, -0.0054,  ...,  0.0307,  0.0162,  0.0089],\n",
      "        [-0.0702,  0.0393,  0.0179,  ...,  0.0305,  0.0161,  0.0090],\n",
      "        [-0.0367, -0.0348,  0.0470,  ...,  0.0306,  0.0159,  0.0093]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "32-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0477, -0.0591,  0.0339,  ..., -0.0130,  0.0715,  0.0753],\n",
      "         [-0.0467, -0.0450,  0.0245,  ...,  0.0192,  0.0391,  0.0490],\n",
      "         [-0.0396, -0.0365,  0.0278,  ...,  0.0307,  0.0256,  0.0336],\n",
      "         ...,\n",
      "         [-0.0262, -0.0229,  0.0358,  ...,  0.0323,  0.0160,  0.0107],\n",
      "         [-0.0256, -0.0222,  0.0360,  ...,  0.0318,  0.0159,  0.0099],\n",
      "         [-0.0252, -0.0218,  0.0361,  ...,  0.0314,  0.0158,  0.0094]],\n",
      "\n",
      "        [[-0.0641, -0.0248,  0.0194,  ..., -0.0224,  0.0322,  0.0283],\n",
      "         [-0.0435, -0.0321,  0.0223,  ...,  0.0062,  0.0256,  0.0177],\n",
      "         [-0.0344, -0.0327,  0.0270,  ...,  0.0192,  0.0213,  0.0134],\n",
      "         ...,\n",
      "         [-0.0258, -0.0233,  0.0360,  ...,  0.0300,  0.0159,  0.0093],\n",
      "         [-0.0255, -0.0225,  0.0361,  ...,  0.0302,  0.0158,  0.0091],\n",
      "         [-0.0252, -0.0220,  0.0361,  ...,  0.0304,  0.0158,  0.0090]],\n",
      "\n",
      "        [[-0.0856, -0.0311,  0.0163,  ..., -0.0721,  0.0652, -0.0031],\n",
      "         [-0.0519, -0.0426,  0.0353,  ..., -0.0111,  0.0428,  0.0063],\n",
      "         [-0.0354, -0.0374,  0.0387,  ...,  0.0141,  0.0281,  0.0091],\n",
      "         ...,\n",
      "         [-0.0244, -0.0227,  0.0361,  ...,  0.0310,  0.0158,  0.0088],\n",
      "         [-0.0245, -0.0221,  0.0360,  ...,  0.0310,  0.0157,  0.0087],\n",
      "         [-0.0245, -0.0217,  0.0360,  ...,  0.0309,  0.0158,  0.0087]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0748, -0.0934,  0.0193,  ...,  0.0102,  0.0204,  0.0090],\n",
      "         [-0.0704, -0.0518,  0.0254,  ...,  0.0290,  0.0272,  0.0192],\n",
      "         [-0.0549, -0.0346,  0.0316,  ...,  0.0311,  0.0266,  0.0182],\n",
      "         ...,\n",
      "         [-0.0273, -0.0219,  0.0362,  ...,  0.0311,  0.0179,  0.0098],\n",
      "         [-0.0263, -0.0217,  0.0361,  ...,  0.0310,  0.0172,  0.0093],\n",
      "         [-0.0257, -0.0216,  0.0361,  ...,  0.0308,  0.0167,  0.0090]],\n",
      "\n",
      "        [[-0.0524, -0.0724,  0.0347,  ..., -0.0019,  0.0111, -0.0115],\n",
      "         [-0.0432, -0.0517,  0.0387,  ...,  0.0160,  0.0204, -0.0029],\n",
      "         [-0.0354, -0.0405,  0.0388,  ...,  0.0244,  0.0223,  0.0052],\n",
      "         ...,\n",
      "         [-0.0254, -0.0233,  0.0367,  ...,  0.0306,  0.0173,  0.0095],\n",
      "         [-0.0251, -0.0225,  0.0365,  ...,  0.0306,  0.0168,  0.0092],\n",
      "         [-0.0249, -0.0220,  0.0363,  ...,  0.0306,  0.0164,  0.0090]],\n",
      "\n",
      "        [[-0.0166, -0.0314,  0.0098,  ...,  0.0916, -0.0054, -0.0295],\n",
      "         [-0.0224, -0.0302,  0.0237,  ...,  0.0548, -0.0009, -0.0103],\n",
      "         [-0.0238, -0.0279,  0.0320,  ...,  0.0400,  0.0042,  0.0013],\n",
      "         ...,\n",
      "         [-0.0256, -0.0221,  0.0371,  ...,  0.0304,  0.0149,  0.0087],\n",
      "         [-0.0255, -0.0218,  0.0368,  ...,  0.0305,  0.0153,  0.0087],\n",
      "         [-0.0253, -0.0216,  0.0366,  ...,  0.0305,  0.0155,  0.0086]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0477, -0.0591,  0.0339,  ...,  0.0314,  0.0158,  0.0094],\n",
      "        [-0.0641, -0.0248,  0.0194,  ...,  0.0304,  0.0158,  0.0090],\n",
      "        [-0.0856, -0.0311,  0.0163,  ...,  0.0309,  0.0158,  0.0087],\n",
      "        ...,\n",
      "        [-0.0748, -0.0934,  0.0193,  ...,  0.0308,  0.0167,  0.0090],\n",
      "        [-0.0524, -0.0724,  0.0347,  ...,  0.0306,  0.0164,  0.0090],\n",
      "        [-0.0166, -0.0314,  0.0098,  ...,  0.0305,  0.0155,  0.0086]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "33-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[ 1.7258e-03, -4.1024e-02,  6.3994e-02,  ..., -1.2248e-02,\n",
      "           2.8902e-02,  2.3033e-02],\n",
      "         [-1.7283e-02, -3.5100e-02,  4.6988e-02,  ...,  1.1153e-02,\n",
      "           2.4802e-02,  1.9392e-02],\n",
      "         [-2.2165e-02, -2.8139e-02,  3.9561e-02,  ...,  2.4223e-02,\n",
      "           1.8254e-02,  1.9196e-02],\n",
      "         ...,\n",
      "         [-2.5091e-02, -2.1430e-02,  3.5512e-02,  ...,  3.1704e-02,\n",
      "           1.5431e-02,  1.0201e-02],\n",
      "         [-2.5003e-02, -2.1268e-02,  3.5656e-02,  ...,  3.1376e-02,\n",
      "           1.5565e-02,  9.6106e-03],\n",
      "         [-2.4929e-02, -2.1188e-02,  3.5772e-02,  ...,  3.1117e-02,\n",
      "           1.5657e-02,  9.2490e-03]],\n",
      "\n",
      "        [[-8.1323e-02, -1.4227e-02,  2.0977e-02,  ...,  5.1945e-03,\n",
      "           2.1590e-02,  4.3787e-03],\n",
      "         [-5.8140e-02, -3.1384e-02,  2.3092e-02,  ...,  1.0165e-02,\n",
      "           1.6543e-02,  9.1059e-03],\n",
      "         [-4.5364e-02, -2.9793e-02,  2.6409e-02,  ...,  1.6604e-02,\n",
      "           1.5387e-02,  1.0575e-02],\n",
      "         ...,\n",
      "         [-2.6797e-02, -2.2189e-02,  3.5600e-02,  ...,  2.9719e-02,\n",
      "           1.5473e-02,  9.0697e-03],\n",
      "         [-2.6035e-02, -2.1831e-02,  3.5844e-02,  ...,  3.0116e-02,\n",
      "           1.5598e-02,  8.8622e-03],\n",
      "         [-2.5553e-02, -2.1604e-02,  3.5959e-02,  ...,  3.0331e-02,\n",
      "           1.5680e-02,  8.7548e-03]],\n",
      "\n",
      "        [[-3.6517e-02, -1.1331e-02,  8.6304e-02,  ..., -1.0403e-02,\n",
      "           6.9848e-03,  1.5700e-02],\n",
      "         [-4.1369e-02, -2.6657e-02,  5.2626e-02,  ...,  1.7123e-02,\n",
      "          -4.9050e-03,  1.4011e-02],\n",
      "         [-3.5042e-02, -2.7352e-02,  4.2735e-02,  ...,  2.5390e-02,\n",
      "           2.5935e-05,  1.4840e-02],\n",
      "         ...,\n",
      "         [-2.5621e-02, -2.1818e-02,  3.6299e-02,  ...,  3.1314e-02,\n",
      "           1.4402e-02,  9.1754e-03],\n",
      "         [-2.5296e-02, -2.1527e-02,  3.6193e-02,  ...,  3.1125e-02,\n",
      "           1.4999e-02,  8.8918e-03],\n",
      "         [-2.5094e-02, -2.1365e-02,  3.6124e-02,  ...,  3.0966e-02,\n",
      "           1.5342e-02,  8.7561e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.3661e-03,  1.8179e-02,  1.8013e-02,  ..., -3.4395e-02,\n",
      "           4.0210e-02,  7.7423e-03],\n",
      "         [-2.3049e-02, -9.6438e-03,  2.2573e-02,  ..., -9.3801e-04,\n",
      "           3.0166e-02,  2.2014e-02],\n",
      "         [-2.6338e-02, -1.8164e-02,  3.0114e-02,  ...,  1.6320e-02,\n",
      "           2.4608e-02,  2.3111e-02],\n",
      "         ...,\n",
      "         [-2.6149e-02, -2.1665e-02,  3.6054e-02,  ...,  3.0685e-02,\n",
      "           1.6582e-02,  1.0782e-02],\n",
      "         [-2.5741e-02, -2.1518e-02,  3.6087e-02,  ...,  3.0762e-02,\n",
      "           1.6297e-02,  9.9630e-03],\n",
      "         [-2.5432e-02, -2.1407e-02,  3.6088e-02,  ...,  3.0759e-02,\n",
      "           1.6119e-02,  9.4621e-03]],\n",
      "\n",
      "        [[-9.1176e-02, -5.3152e-02,  5.8895e-02,  ...,  1.1996e-02,\n",
      "           3.7493e-02,  7.7823e-03],\n",
      "         [-6.3929e-02, -4.1144e-02,  3.6336e-02,  ...,  2.4327e-02,\n",
      "           3.0528e-02,  1.1195e-02],\n",
      "         [-4.9406e-02, -3.2555e-02,  3.4201e-02,  ...,  2.6891e-02,\n",
      "           2.7154e-02,  1.2917e-02],\n",
      "         ...,\n",
      "         [-2.7392e-02, -2.2611e-02,  3.5975e-02,  ...,  3.0418e-02,\n",
      "           1.7786e-02,  9.5618e-03],\n",
      "         [-2.6423e-02, -2.2076e-02,  3.6002e-02,  ...,  3.0503e-02,\n",
      "           1.7102e-02,  9.1850e-03],\n",
      "         [-2.5801e-02, -2.1727e-02,  3.6003e-02,  ...,  3.0549e-02,\n",
      "           1.6639e-02,  8.9648e-03]],\n",
      "\n",
      "        [[-3.3965e-02, -5.7864e-02,  2.5297e-03,  ...,  1.6977e-02,\n",
      "          -6.7946e-03,  4.5101e-03],\n",
      "         [-2.9636e-02, -4.4645e-02,  1.5843e-02,  ...,  1.9054e-02,\n",
      "           7.7196e-03,  9.7896e-03],\n",
      "         [-2.7008e-02, -3.4152e-02,  2.6964e-02,  ...,  2.3336e-02,\n",
      "           1.3738e-02,  1.3113e-02],\n",
      "         ...,\n",
      "         [-2.5422e-02, -2.1547e-02,  3.6289e-02,  ...,  3.0160e-02,\n",
      "           1.6303e-02,  9.2464e-03],\n",
      "         [-2.5224e-02, -2.1351e-02,  3.6256e-02,  ...,  3.0373e-02,\n",
      "           1.6157e-02,  8.9502e-03],\n",
      "         [-2.5069e-02, -2.1264e-02,  3.6198e-02,  ...,  3.0487e-02,\n",
      "           1.6045e-02,  8.7965e-03]]], grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[ 0.0017, -0.0410,  0.0640,  ...,  0.0311,  0.0157,  0.0092],\n",
      "        [-0.0813, -0.0142,  0.0210,  ...,  0.0303,  0.0157,  0.0088],\n",
      "        [-0.0365, -0.0113,  0.0863,  ...,  0.0310,  0.0153,  0.0088],\n",
      "        ...,\n",
      "        [-0.0094,  0.0182,  0.0180,  ...,  0.0308,  0.0161,  0.0095],\n",
      "        [-0.0912, -0.0532,  0.0589,  ...,  0.0305,  0.0166,  0.0090],\n",
      "        [-0.0340, -0.0579,  0.0025,  ...,  0.0305,  0.0160,  0.0088]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-7.5338e-02,  7.5365e-04, -3.6323e-02,  ...,  3.6067e-02,\n",
      "           4.4927e-02, -1.2034e-02],\n",
      "         [-5.7321e-02, -1.2718e-02, -7.2274e-03,  ...,  3.2428e-02,\n",
      "           2.3120e-02,  1.3782e-03],\n",
      "         [-4.3358e-02, -1.9086e-02,  1.2699e-02,  ...,  3.2775e-02,\n",
      "           1.6289e-02,  9.7426e-03],\n",
      "         ...,\n",
      "         [-2.6055e-02, -2.1743e-02,  3.5155e-02,  ...,  3.1261e-02,\n",
      "           1.5453e-02,  9.7034e-03],\n",
      "         [-2.5560e-02, -2.1565e-02,  3.5625e-02,  ...,  3.1082e-02,\n",
      "           1.5633e-02,  9.2889e-03],\n",
      "         [-2.5256e-02, -2.1433e-02,  3.5857e-02,  ...,  3.0943e-02,\n",
      "           1.5729e-02,  9.0363e-03]],\n",
      "\n",
      "        [[-7.1348e-02, -8.2351e-03,  6.7642e-02,  ..., -2.4184e-02,\n",
      "           5.2693e-02,  2.1775e-02],\n",
      "         [-5.4693e-02, -2.8072e-02,  4.9124e-02,  ...,  6.4515e-03,\n",
      "           3.8057e-02,  1.0795e-02],\n",
      "         [-4.4153e-02, -3.0341e-02,  4.2113e-02,  ...,  2.2444e-02,\n",
      "           3.0538e-02,  1.2264e-02],\n",
      "         ...,\n",
      "         [-2.6697e-02, -2.2631e-02,  3.5669e-02,  ...,  3.1106e-02,\n",
      "           1.6906e-02,  9.3959e-03],\n",
      "         [-2.5931e-02, -2.2030e-02,  3.5748e-02,  ...,  3.0895e-02,\n",
      "           1.6450e-02,  9.1138e-03],\n",
      "         [-2.5455e-02, -2.1665e-02,  3.5827e-02,  ...,  3.0751e-02,\n",
      "           1.6185e-02,  8.9439e-03]],\n",
      "\n",
      "        [[-8.9835e-02, -3.6576e-02,  5.9395e-03,  ...,  3.8796e-02,\n",
      "          -1.1493e-02, -1.4814e-02],\n",
      "         [-4.8637e-02, -3.2783e-02,  1.5908e-02,  ...,  2.4857e-02,\n",
      "           6.7266e-04,  4.3778e-03],\n",
      "         [-3.1700e-02, -2.9055e-02,  2.4811e-02,  ...,  2.3624e-02,\n",
      "           9.4069e-03,  1.1848e-02],\n",
      "         ...,\n",
      "         [-2.4679e-02, -2.2019e-02,  3.6147e-02,  ...,  2.9960e-02,\n",
      "           1.6264e-02,  1.0229e-02],\n",
      "         [-2.4797e-02, -2.1732e-02,  3.6227e-02,  ...,  3.0271e-02,\n",
      "           1.6200e-02,  9.6790e-03],\n",
      "         [-2.4844e-02, -2.1547e-02,  3.6218e-02,  ...,  3.0438e-02,\n",
      "           1.6114e-02,  9.3203e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0645e-01, -1.0178e-02,  3.2649e-03,  ...,  6.5734e-02,\n",
      "          -1.0768e-02,  8.3839e-02],\n",
      "         [-7.4823e-02, -8.1267e-03, -1.5739e-03,  ...,  5.1613e-02,\n",
      "          -4.1006e-05,  4.4705e-02],\n",
      "         [-5.2033e-02, -1.3342e-02,  1.1293e-02,  ...,  4.4638e-02,\n",
      "           1.0589e-02,  3.1459e-02],\n",
      "         ...,\n",
      "         [-2.7045e-02, -2.1325e-02,  3.4954e-02,  ...,  3.2727e-02,\n",
      "           1.6537e-02,  1.0899e-02],\n",
      "         [-2.6241e-02, -2.1363e-02,  3.5556e-02,  ...,  3.1996e-02,\n",
      "           1.6363e-02,  9.9977e-03],\n",
      "         [-2.5716e-02, -2.1341e-02,  3.5845e-02,  ...,  3.1501e-02,\n",
      "           1.6203e-02,  9.4571e-03]],\n",
      "\n",
      "        [[-3.7515e-02, -2.6606e-02,  3.2304e-02,  ..., -3.9712e-02,\n",
      "           4.2206e-02, -1.7758e-02],\n",
      "         [-3.6334e-02, -2.9747e-02,  2.9568e-02,  ..., -5.1042e-04,\n",
      "           2.1907e-02,  7.5606e-03],\n",
      "         [-3.1172e-02, -2.7758e-02,  3.1584e-02,  ...,  1.6834e-02,\n",
      "           1.5415e-02,  1.4408e-02],\n",
      "         ...,\n",
      "         [-2.5434e-02, -2.2096e-02,  3.5615e-02,  ...,  3.1509e-02,\n",
      "           1.5782e-02,  9.7245e-03],\n",
      "         [-2.5191e-02, -2.1739e-02,  3.5759e-02,  ...,  3.1326e-02,\n",
      "           1.5873e-02,  9.2984e-03],\n",
      "         [-2.5027e-02, -2.1514e-02,  3.5847e-02,  ...,  3.1132e-02,\n",
      "           1.5894e-02,  9.0494e-03]],\n",
      "\n",
      "        [[ 3.9332e-03, -5.6527e-02,  5.9934e-02,  ..., -3.7028e-02,\n",
      "           2.8172e-02,  4.7109e-02],\n",
      "         [-1.5953e-02, -4.3829e-02,  3.9277e-02,  ...,  2.4682e-03,\n",
      "           1.8495e-02,  2.8132e-02],\n",
      "         [-2.1167e-02, -3.6841e-02,  3.7367e-02,  ...,  1.9704e-02,\n",
      "           1.8589e-02,  1.9118e-02],\n",
      "         ...,\n",
      "         [-2.5090e-02, -2.2483e-02,  3.6184e-02,  ...,  3.1856e-02,\n",
      "           1.6899e-02,  8.9623e-03],\n",
      "         [-2.4992e-02, -2.1916e-02,  3.6077e-02,  ...,  3.1516e-02,\n",
      "           1.6569e-02,  8.7928e-03],\n",
      "         [-2.4908e-02, -2.1590e-02,  3.6014e-02,  ...,  3.1226e-02,\n",
      "           1.6322e-02,  8.7232e-03]]], grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0753,  0.0008, -0.0363,  ...,  0.0309,  0.0157,  0.0090],\n",
      "        [-0.0713, -0.0082,  0.0676,  ...,  0.0308,  0.0162,  0.0089],\n",
      "        [-0.0898, -0.0366,  0.0059,  ...,  0.0304,  0.0161,  0.0093],\n",
      "        ...,\n",
      "        [-0.1065, -0.0102,  0.0033,  ...,  0.0315,  0.0162,  0.0095],\n",
      "        [-0.0375, -0.0266,  0.0323,  ...,  0.0311,  0.0159,  0.0090],\n",
      "        [ 0.0039, -0.0565,  0.0599,  ...,  0.0312,  0.0163,  0.0087]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "35-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0546, -0.0412, -0.0057,  ..., -0.0445,  0.0488,  0.0444],\n",
      "         [-0.0535, -0.0409,  0.0025,  ..., -0.0070,  0.0289,  0.0162],\n",
      "         [-0.0449, -0.0385,  0.0150,  ...,  0.0128,  0.0223,  0.0130],\n",
      "         ...,\n",
      "         [-0.0263, -0.0235,  0.0349,  ...,  0.0300,  0.0157,  0.0092],\n",
      "         [-0.0257, -0.0226,  0.0355,  ...,  0.0303,  0.0157,  0.0089],\n",
      "         [-0.0253, -0.0221,  0.0358,  ...,  0.0305,  0.0158,  0.0088]],\n",
      "\n",
      "        [[-0.0129, -0.0395,  0.0228,  ..., -0.0257,  0.0122,  0.0421],\n",
      "         [-0.0299, -0.0344,  0.0335,  ...,  0.0039,  0.0171,  0.0298],\n",
      "         [-0.0329, -0.0299,  0.0361,  ...,  0.0184,  0.0168,  0.0229],\n",
      "         ...,\n",
      "         [-0.0258, -0.0223,  0.0363,  ...,  0.0317,  0.0159,  0.0103],\n",
      "         [-0.0253, -0.0219,  0.0362,  ...,  0.0315,  0.0159,  0.0096],\n",
      "         [-0.0251, -0.0216,  0.0362,  ...,  0.0312,  0.0158,  0.0093]],\n",
      "\n",
      "        [[-0.0789, -0.0084,  0.0164,  ...,  0.0087,  0.0272,  0.0635],\n",
      "         [-0.0642, -0.0196,  0.0148,  ...,  0.0224,  0.0205,  0.0344],\n",
      "         [-0.0486, -0.0268,  0.0229,  ...,  0.0287,  0.0180,  0.0250],\n",
      "         ...,\n",
      "         [-0.0261, -0.0235,  0.0357,  ...,  0.0311,  0.0160,  0.0102],\n",
      "         [-0.0255, -0.0227,  0.0359,  ...,  0.0310,  0.0159,  0.0096],\n",
      "         [-0.0252, -0.0221,  0.0360,  ...,  0.0309,  0.0159,  0.0092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0170, -0.0084,  0.0073,  ...,  0.0428,  0.0514,  0.0656],\n",
      "         [-0.0271, -0.0198,  0.0055,  ...,  0.0453,  0.0248,  0.0430],\n",
      "         [-0.0278, -0.0228,  0.0157,  ...,  0.0406,  0.0179,  0.0286],\n",
      "         ...,\n",
      "         [-0.0257, -0.0218,  0.0344,  ...,  0.0319,  0.0161,  0.0096],\n",
      "         [-0.0254, -0.0215,  0.0351,  ...,  0.0314,  0.0161,  0.0092],\n",
      "         [-0.0252, -0.0214,  0.0355,  ...,  0.0311,  0.0161,  0.0089]],\n",
      "\n",
      "        [[-0.0367, -0.0667, -0.0033,  ...,  0.0517,  0.0388,  0.0633],\n",
      "         [-0.0397, -0.0504,  0.0107,  ...,  0.0461,  0.0265,  0.0381],\n",
      "         [-0.0343, -0.0393,  0.0238,  ...,  0.0420,  0.0194,  0.0270],\n",
      "         ...,\n",
      "         [-0.0252, -0.0226,  0.0357,  ...,  0.0323,  0.0162,  0.0103],\n",
      "         [-0.0250, -0.0220,  0.0359,  ...,  0.0316,  0.0161,  0.0097],\n",
      "         [-0.0249, -0.0217,  0.0359,  ...,  0.0312,  0.0161,  0.0093]],\n",
      "\n",
      "        [[-0.0059, -0.0017,  0.0383,  ..., -0.0165,  0.0353,  0.0446],\n",
      "         [-0.0205, -0.0170,  0.0327,  ...,  0.0067,  0.0254,  0.0265],\n",
      "         [-0.0237, -0.0220,  0.0335,  ...,  0.0195,  0.0193,  0.0205],\n",
      "         ...,\n",
      "         [-0.0249, -0.0220,  0.0363,  ...,  0.0307,  0.0161,  0.0096],\n",
      "         [-0.0248, -0.0218,  0.0362,  ...,  0.0307,  0.0161,  0.0092],\n",
      "         [-0.0248, -0.0216,  0.0362,  ...,  0.0307,  0.0160,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0546, -0.0412, -0.0057,  ...,  0.0305,  0.0158,  0.0088],\n",
      "        [-0.0129, -0.0395,  0.0228,  ...,  0.0312,  0.0158,  0.0093],\n",
      "        [-0.0789, -0.0084,  0.0164,  ...,  0.0309,  0.0159,  0.0092],\n",
      "        ...,\n",
      "        [-0.0170, -0.0084,  0.0073,  ...,  0.0311,  0.0161,  0.0089],\n",
      "        [-0.0367, -0.0667, -0.0033,  ...,  0.0312,  0.0161,  0.0093],\n",
      "        [-0.0059, -0.0017,  0.0383,  ...,  0.0307,  0.0160,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "36-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0874, -0.0293, -0.0153,  ..., -0.0156,  0.0396, -0.0576],\n",
      "         [-0.0678, -0.0238, -0.0057,  ...,  0.0143,  0.0219, -0.0256],\n",
      "         [-0.0494, -0.0234,  0.0096,  ...,  0.0262,  0.0158, -0.0073],\n",
      "         ...,\n",
      "         [-0.0263, -0.0221,  0.0347,  ...,  0.0320,  0.0157,  0.0085],\n",
      "         [-0.0256, -0.0219,  0.0354,  ...,  0.0315,  0.0158,  0.0086],\n",
      "         [-0.0253, -0.0217,  0.0357,  ...,  0.0312,  0.0159,  0.0087]],\n",
      "\n",
      "        [[-0.1247, -0.0341, -0.0002,  ...,  0.0147,  0.0731,  0.0276],\n",
      "         [-0.0852, -0.0352,  0.0094,  ...,  0.0242,  0.0453,  0.0130],\n",
      "         [-0.0571, -0.0322,  0.0219,  ...,  0.0289,  0.0306,  0.0120],\n",
      "         ...,\n",
      "         [-0.0260, -0.0226,  0.0363,  ...,  0.0310,  0.0159,  0.0091],\n",
      "         [-0.0254, -0.0221,  0.0364,  ...,  0.0309,  0.0158,  0.0089],\n",
      "         [-0.0251, -0.0218,  0.0363,  ...,  0.0308,  0.0158,  0.0088]],\n",
      "\n",
      "        [[-0.1011, -0.0004, -0.0311,  ...,  0.0252,  0.0282, -0.0307],\n",
      "         [-0.0739, -0.0192,  0.0008,  ...,  0.0195,  0.0206, -0.0065],\n",
      "         [-0.0529, -0.0247,  0.0195,  ...,  0.0220,  0.0187,  0.0058],\n",
      "         ...,\n",
      "         [-0.0268, -0.0228,  0.0363,  ...,  0.0305,  0.0165,  0.0094],\n",
      "         [-0.0260, -0.0223,  0.0363,  ...,  0.0306,  0.0164,  0.0092],\n",
      "         [-0.0255, -0.0219,  0.0363,  ...,  0.0307,  0.0162,  0.0090]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0446, -0.0534,  0.0446,  ..., -0.0552,  0.0170,  0.0322],\n",
      "         [-0.0415, -0.0453,  0.0388,  ..., -0.0027,  0.0104,  0.0226],\n",
      "         [-0.0376, -0.0374,  0.0387,  ...,  0.0175,  0.0094,  0.0200],\n",
      "         ...,\n",
      "         [-0.0264, -0.0224,  0.0363,  ...,  0.0304,  0.0153,  0.0103],\n",
      "         [-0.0258, -0.0218,  0.0362,  ...,  0.0304,  0.0156,  0.0096],\n",
      "         [-0.0254, -0.0215,  0.0361,  ...,  0.0305,  0.0157,  0.0092]],\n",
      "\n",
      "        [[-0.0972, -0.0241,  0.0073,  ...,  0.0157,  0.0571, -0.0293],\n",
      "         [-0.0580, -0.0334,  0.0091,  ...,  0.0203,  0.0323, -0.0065],\n",
      "         [-0.0416, -0.0337,  0.0216,  ...,  0.0265,  0.0234,  0.0076],\n",
      "         ...,\n",
      "         [-0.0260, -0.0236,  0.0357,  ...,  0.0314,  0.0165,  0.0099],\n",
      "         [-0.0256, -0.0227,  0.0359,  ...,  0.0312,  0.0163,  0.0094],\n",
      "         [-0.0253, -0.0222,  0.0360,  ...,  0.0310,  0.0161,  0.0091]],\n",
      "\n",
      "        [[-0.0093, -0.0258,  0.0357,  ..., -0.0153,  0.0343,  0.0294],\n",
      "         [-0.0214, -0.0257,  0.0365,  ...,  0.0044,  0.0175,  0.0331],\n",
      "         [-0.0261, -0.0248,  0.0345,  ...,  0.0158,  0.0106,  0.0265],\n",
      "         ...,\n",
      "         [-0.0256, -0.0217,  0.0355,  ...,  0.0296,  0.0148,  0.0098],\n",
      "         [-0.0253, -0.0215,  0.0357,  ...,  0.0300,  0.0153,  0.0093],\n",
      "         [-0.0251, -0.0214,  0.0358,  ...,  0.0302,  0.0155,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0874, -0.0293, -0.0153,  ...,  0.0312,  0.0159,  0.0087],\n",
      "        [-0.1247, -0.0341, -0.0002,  ...,  0.0308,  0.0158,  0.0088],\n",
      "        [-0.1011, -0.0004, -0.0311,  ...,  0.0307,  0.0162,  0.0090],\n",
      "        ...,\n",
      "        [-0.0446, -0.0534,  0.0446,  ...,  0.0305,  0.0157,  0.0092],\n",
      "        [-0.0972, -0.0241,  0.0073,  ...,  0.0310,  0.0161,  0.0091],\n",
      "        [-0.0093, -0.0258,  0.0357,  ...,  0.0302,  0.0155,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[ 0.0137, -0.0173,  0.0567,  ...,  0.0589, -0.0179,  0.0488],\n",
      "         [-0.0036, -0.0231,  0.0352,  ...,  0.0441, -0.0053,  0.0360],\n",
      "         [-0.0126, -0.0241,  0.0334,  ...,  0.0359,  0.0034,  0.0272],\n",
      "         ...,\n",
      "         [-0.0245, -0.0218,  0.0365,  ...,  0.0314,  0.0153,  0.0103],\n",
      "         [-0.0247, -0.0216,  0.0364,  ...,  0.0312,  0.0156,  0.0096],\n",
      "         [-0.0248, -0.0214,  0.0363,  ...,  0.0310,  0.0157,  0.0093]],\n",
      "\n",
      "        [[-0.0650, -0.0442, -0.0009,  ..., -0.0229,  0.0135,  0.0660],\n",
      "         [-0.0524, -0.0291,  0.0190,  ...,  0.0035,  0.0185,  0.0396],\n",
      "         [-0.0422, -0.0243,  0.0313,  ...,  0.0168,  0.0189,  0.0256],\n",
      "         ...,\n",
      "         [-0.0264, -0.0212,  0.0369,  ...,  0.0307,  0.0165,  0.0093],\n",
      "         [-0.0258, -0.0212,  0.0366,  ...,  0.0308,  0.0162,  0.0090],\n",
      "         [-0.0254, -0.0212,  0.0364,  ...,  0.0307,  0.0161,  0.0088]],\n",
      "\n",
      "        [[-0.0526,  0.0067,  0.0291,  ..., -0.0018,  0.0524,  0.0481],\n",
      "         [-0.0512, -0.0134,  0.0229,  ...,  0.0209,  0.0346,  0.0247],\n",
      "         [-0.0433, -0.0195,  0.0270,  ...,  0.0282,  0.0260,  0.0159],\n",
      "         ...,\n",
      "         [-0.0265, -0.0220,  0.0366,  ...,  0.0312,  0.0159,  0.0092],\n",
      "         [-0.0258, -0.0218,  0.0366,  ...,  0.0311,  0.0158,  0.0090],\n",
      "         [-0.0254, -0.0216,  0.0365,  ...,  0.0309,  0.0157,  0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0424, -0.0575,  0.0590,  ...,  0.0006,  0.0321,  0.0236],\n",
      "         [-0.0353, -0.0446,  0.0414,  ...,  0.0182,  0.0165,  0.0158],\n",
      "         [-0.0304, -0.0332,  0.0354,  ...,  0.0280,  0.0130,  0.0126],\n",
      "         ...,\n",
      "         [-0.0257, -0.0212,  0.0354,  ...,  0.0319,  0.0152,  0.0086],\n",
      "         [-0.0254, -0.0211,  0.0356,  ...,  0.0315,  0.0155,  0.0086],\n",
      "         [-0.0252, -0.0211,  0.0358,  ...,  0.0312,  0.0156,  0.0086]],\n",
      "\n",
      "        [[-0.0569, -0.0346,  0.0565,  ..., -0.0114,  0.0084,  0.0052],\n",
      "         [-0.0477, -0.0290,  0.0415,  ...,  0.0146,  0.0178,  0.0094],\n",
      "         [-0.0423, -0.0281,  0.0354,  ...,  0.0256,  0.0179,  0.0093],\n",
      "         ...,\n",
      "         [-0.0272, -0.0223,  0.0355,  ...,  0.0318,  0.0161,  0.0085],\n",
      "         [-0.0263, -0.0219,  0.0357,  ...,  0.0314,  0.0160,  0.0085],\n",
      "         [-0.0257, -0.0216,  0.0359,  ...,  0.0311,  0.0160,  0.0086]],\n",
      "\n",
      "        [[-0.0516, -0.0074,  0.0828,  ...,  0.0094,  0.0731,  0.0093],\n",
      "         [-0.0548, -0.0138,  0.0501,  ...,  0.0192,  0.0399,  0.0098],\n",
      "         [-0.0498, -0.0201,  0.0412,  ...,  0.0259,  0.0270,  0.0114],\n",
      "         ...,\n",
      "         [-0.0280, -0.0219,  0.0364,  ...,  0.0308,  0.0162,  0.0093],\n",
      "         [-0.0267, -0.0217,  0.0363,  ...,  0.0307,  0.0160,  0.0091],\n",
      "         [-0.0260, -0.0215,  0.0362,  ...,  0.0307,  0.0160,  0.0090]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[ 0.0137, -0.0173,  0.0567,  ...,  0.0310,  0.0157,  0.0093],\n",
      "        [-0.0650, -0.0442, -0.0009,  ...,  0.0307,  0.0161,  0.0088],\n",
      "        [-0.0526,  0.0067,  0.0291,  ...,  0.0309,  0.0157,  0.0089],\n",
      "        ...,\n",
      "        [-0.0424, -0.0575,  0.0590,  ...,  0.0312,  0.0156,  0.0086],\n",
      "        [-0.0569, -0.0346,  0.0565,  ...,  0.0311,  0.0160,  0.0086],\n",
      "        [-0.0516, -0.0074,  0.0828,  ...,  0.0307,  0.0160,  0.0090]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "38-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0519, -0.0410, -0.0197,  ..., -0.0240,  0.0534,  0.0136],\n",
      "         [-0.0450, -0.0305,  0.0063,  ..., -0.0013,  0.0293,  0.0134],\n",
      "         [-0.0362, -0.0271,  0.0208,  ...,  0.0144,  0.0202,  0.0149],\n",
      "         ...,\n",
      "         [-0.0253, -0.0217,  0.0358,  ...,  0.0308,  0.0161,  0.0100],\n",
      "         [-0.0251, -0.0215,  0.0360,  ...,  0.0309,  0.0160,  0.0095],\n",
      "         [-0.0249, -0.0214,  0.0361,  ...,  0.0309,  0.0160,  0.0092]],\n",
      "\n",
      "        [[-0.1033, -0.0350, -0.0075,  ..., -0.0304,  0.0661,  0.0435],\n",
      "         [-0.0685, -0.0245,  0.0017,  ...,  0.0041,  0.0321,  0.0248],\n",
      "         [-0.0474, -0.0230,  0.0154,  ...,  0.0216,  0.0224,  0.0187],\n",
      "         ...,\n",
      "         [-0.0257, -0.0219,  0.0356,  ...,  0.0313,  0.0161,  0.0100],\n",
      "         [-0.0253, -0.0217,  0.0360,  ...,  0.0311,  0.0160,  0.0095],\n",
      "         [-0.0251, -0.0216,  0.0361,  ...,  0.0309,  0.0160,  0.0092]],\n",
      "\n",
      "        [[-0.0493,  0.0302, -0.0245,  ...,  0.0096, -0.0416,  0.0331],\n",
      "         [-0.0383,  0.0071, -0.0015,  ...,  0.0246, -0.0069,  0.0206],\n",
      "         [-0.0334, -0.0061,  0.0179,  ...,  0.0298,  0.0064,  0.0176],\n",
      "         ...,\n",
      "         [-0.0260, -0.0218,  0.0365,  ...,  0.0319,  0.0161,  0.0097],\n",
      "         [-0.0256, -0.0218,  0.0365,  ...,  0.0315,  0.0160,  0.0093],\n",
      "         [-0.0253, -0.0216,  0.0364,  ...,  0.0312,  0.0160,  0.0091]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0438, -0.0060,  0.0647,  ..., -0.0105,  0.0714,  0.0366],\n",
      "         [-0.0398, -0.0153,  0.0438,  ...,  0.0118,  0.0438,  0.0281],\n",
      "         [-0.0332, -0.0181,  0.0394,  ...,  0.0212,  0.0290,  0.0225],\n",
      "         ...,\n",
      "         [-0.0254, -0.0212,  0.0360,  ...,  0.0309,  0.0162,  0.0102],\n",
      "         [-0.0252, -0.0212,  0.0360,  ...,  0.0309,  0.0160,  0.0095],\n",
      "         [-0.0250, -0.0212,  0.0360,  ...,  0.0309,  0.0159,  0.0092]],\n",
      "\n",
      "        [[-0.0279, -0.0344,  0.0330,  ..., -0.0251,  0.0476, -0.0342],\n",
      "         [-0.0312, -0.0323,  0.0304,  ...,  0.0064,  0.0305, -0.0039],\n",
      "         [-0.0308, -0.0299,  0.0302,  ...,  0.0190,  0.0220,  0.0069],\n",
      "         ...,\n",
      "         [-0.0254, -0.0222,  0.0351,  ...,  0.0299,  0.0155,  0.0091],\n",
      "         [-0.0251, -0.0218,  0.0355,  ...,  0.0302,  0.0156,  0.0089],\n",
      "         [-0.0250, -0.0216,  0.0357,  ...,  0.0303,  0.0156,  0.0088]],\n",
      "\n",
      "        [[-0.0679,  0.0010,  0.0085,  ...,  0.0161,  0.0537,  0.0464],\n",
      "         [-0.0513, -0.0138,  0.0161,  ...,  0.0212,  0.0388,  0.0328],\n",
      "         [-0.0392, -0.0179,  0.0247,  ...,  0.0274,  0.0276,  0.0265],\n",
      "         ...,\n",
      "         [-0.0257, -0.0216,  0.0355,  ...,  0.0318,  0.0163,  0.0102],\n",
      "         [-0.0254, -0.0215,  0.0357,  ...,  0.0315,  0.0161,  0.0095],\n",
      "         [-0.0252, -0.0215,  0.0359,  ...,  0.0313,  0.0160,  0.0092]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0519, -0.0410, -0.0197,  ...,  0.0309,  0.0160,  0.0092],\n",
      "        [-0.1033, -0.0350, -0.0075,  ...,  0.0309,  0.0160,  0.0092],\n",
      "        [-0.0493,  0.0302, -0.0245,  ...,  0.0312,  0.0160,  0.0091],\n",
      "        ...,\n",
      "        [-0.0438, -0.0060,  0.0647,  ...,  0.0309,  0.0159,  0.0092],\n",
      "        [-0.0279, -0.0344,  0.0330,  ...,  0.0303,  0.0156,  0.0088],\n",
      "        [-0.0679,  0.0010,  0.0085,  ...,  0.0313,  0.0160,  0.0092]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "39-torch.Size([16, 3, 448, 448]) 16 16\n",
      "torch.Size([16, 768]), torch.Size([16, 768]). torch.Size([16, 768])\n",
      "initial inputs:torch.Size([16, 1, 37]), hidden_state:torch.Size([1, 16, 768]), cell_state:torch.Size([1, 16, 768])\n",
      "torch.Size([16, 10, 37])\n",
      "out: tensor([[[-0.0591,  0.0512,  0.0640,  ..., -0.0340,  0.0199,  0.0505],\n",
      "         [-0.0461,  0.0069,  0.0370,  ...,  0.0018,  0.0248,  0.0328],\n",
      "         [-0.0368, -0.0091,  0.0339,  ...,  0.0176,  0.0217,  0.0232],\n",
      "         ...,\n",
      "         [-0.0255, -0.0216,  0.0362,  ...,  0.0315,  0.0161,  0.0095],\n",
      "         [-0.0252, -0.0215,  0.0362,  ...,  0.0313,  0.0160,  0.0091],\n",
      "         [-0.0251, -0.0215,  0.0361,  ...,  0.0311,  0.0159,  0.0089]],\n",
      "\n",
      "        [[-0.0264, -0.0585,  0.0162,  ...,  0.0222,  0.0080, -0.0406],\n",
      "         [-0.0284, -0.0459,  0.0313,  ...,  0.0289,  0.0159, -0.0166],\n",
      "         [-0.0286, -0.0389,  0.0361,  ...,  0.0320,  0.0193, -0.0019],\n",
      "         ...,\n",
      "         [-0.0257, -0.0239,  0.0369,  ...,  0.0315,  0.0173,  0.0089],\n",
      "         [-0.0254, -0.0230,  0.0366,  ...,  0.0311,  0.0168,  0.0089],\n",
      "         [-0.0252, -0.0223,  0.0364,  ...,  0.0309,  0.0165,  0.0089]],\n",
      "\n",
      "        [[-0.0881, -0.0565,  0.0392,  ..., -0.0078, -0.0332,  0.0286],\n",
      "         [-0.0624, -0.0458,  0.0361,  ...,  0.0149, -0.0099,  0.0151],\n",
      "         [-0.0447, -0.0368,  0.0380,  ...,  0.0253,  0.0016,  0.0125],\n",
      "         ...,\n",
      "         [-0.0257, -0.0226,  0.0366,  ...,  0.0319,  0.0150,  0.0092],\n",
      "         [-0.0253, -0.0220,  0.0364,  ...,  0.0315,  0.0154,  0.0090],\n",
      "         [-0.0251, -0.0217,  0.0363,  ...,  0.0313,  0.0156,  0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0532,  0.0251,  0.0581,  ..., -0.0235, -0.0012,  0.0896],\n",
      "         [-0.0434,  0.0055,  0.0301,  ..., -0.0016,  0.0096,  0.0507],\n",
      "         [-0.0366, -0.0092,  0.0281,  ...,  0.0132,  0.0135,  0.0318],\n",
      "         ...,\n",
      "         [-0.0261, -0.0213,  0.0350,  ...,  0.0307,  0.0163,  0.0107],\n",
      "         [-0.0256, -0.0213,  0.0355,  ...,  0.0309,  0.0162,  0.0099],\n",
      "         [-0.0253, -0.0213,  0.0357,  ...,  0.0309,  0.0161,  0.0094]],\n",
      "\n",
      "        [[-0.0328, -0.0748, -0.0100,  ..., -0.0159,  0.1041,  0.0188],\n",
      "         [-0.0337, -0.0532,  0.0038,  ...,  0.0067,  0.0497,  0.0161],\n",
      "         [-0.0328, -0.0405,  0.0188,  ...,  0.0206,  0.0304,  0.0133],\n",
      "         ...,\n",
      "         [-0.0259, -0.0231,  0.0353,  ...,  0.0319,  0.0166,  0.0089],\n",
      "         [-0.0254, -0.0224,  0.0356,  ...,  0.0316,  0.0164,  0.0088],\n",
      "         [-0.0252, -0.0219,  0.0357,  ...,  0.0313,  0.0162,  0.0087]],\n",
      "\n",
      "        [[-0.0850, -0.0290,  0.0490,  ..., -0.0341,  0.0377,  0.0521],\n",
      "         [-0.0638, -0.0241,  0.0361,  ...,  0.0027,  0.0241,  0.0356],\n",
      "         [-0.0460, -0.0225,  0.0373,  ...,  0.0192,  0.0187,  0.0270],\n",
      "         ...,\n",
      "         [-0.0262, -0.0213,  0.0364,  ...,  0.0316,  0.0158,  0.0102],\n",
      "         [-0.0257, -0.0213,  0.0363,  ...,  0.0314,  0.0158,  0.0095],\n",
      "         [-0.0253, -0.0212,  0.0362,  ...,  0.0312,  0.0158,  0.0092]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------------------------\n",
      "out changed shape: tensor([[-0.0591,  0.0512,  0.0640,  ...,  0.0311,  0.0159,  0.0089],\n",
      "        [-0.0264, -0.0585,  0.0162,  ...,  0.0309,  0.0165,  0.0089],\n",
      "        [-0.0881, -0.0565,  0.0392,  ...,  0.0313,  0.0156,  0.0089],\n",
      "        ...,\n",
      "        [-0.0532,  0.0251,  0.0581,  ...,  0.0309,  0.0161,  0.0094],\n",
      "        [-0.0328, -0.0748, -0.0100,  ...,  0.0313,  0.0162,  0.0087],\n",
      "        [-0.0850, -0.0290,  0.0490,  ...,  0.0312,  0.0158,  0.0092]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40-torch.Size([16, 3, 448, 448]) 16 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(description)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(label)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m img_embed\u001b[38;5;241m=\u001b[39mImage_Enc(images\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 11\u001b[0m sen_embed\u001b[38;5;241m=\u001b[39m\u001b[43mText_Enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m concat_embed\u001b[38;5;241m=\u001b[39mimg_embed\u001b[38;5;241m+\u001b[39msen_embed\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_embed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msen_embed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconcat_embed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mText_Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(x, \n\u001b[1;32m     39\u001b[0m          add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     40\u001b[0m          max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m     41\u001b[0m          padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m          truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m          return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m tokens\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 45\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBERT\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:286\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    278\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 286\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PATH=\"/workspace/team2/data/filter_50000/\"\n",
    "BATCH_SIZE = 16\n",
    "dataloaders = build_dataloader(PATH=PATH, batch_size=BATCH_SIZE)\n",
    "for index, batch in enumerate(dataloaders['train']):\n",
    "    images = batch[0]\n",
    "    description = batch[1]\n",
    "    label = batch[2]\n",
    "    print(f\"{index}-{images.shape} {len(description)} {len(label)}\")\n",
    "    \n",
    "    img_embed=Image_Enc(images.to(device))\n",
    "    sen_embed=Text_Enc(description)\n",
    "    concat_embed=img_embed+sen_embed\n",
    "    \n",
    "    print(f\"{img_embed.shape}, {sen_embed.shape}. {concat_embed.shape}\")\n",
    "    initial_c=torch.zeros(1,BATCH_SIZE,768).to(device)\n",
    "    start_token = torch.zeros(BATCH_SIZE,1,37).to(device) # start token??\n",
    "    target_len=10\n",
    "    outputs=torch.zeros(BATCH_SIZE, target_len, 37)\n",
    "\n",
    "    inputs=start_token\n",
    "    hidden_state=concat_embed.unsqueeze(0)\n",
    "    cell_state=initial_c\n",
    "    print(f\"initial inputs:{inputs.shape}, hidden_state:{hidden_state.shape}, cell_state:{cell_state.shape}\")\n",
    "    \n",
    "    for i in range(target_len):\n",
    "            output,(hidden_state, cell_state)=decoder(inputs,(hidden_state, cell_state))\n",
    "            inputs=output\n",
    "#             print(f\"{i}-output_shape:{output.shape}\")\n",
    "#             print(f\"{outputs[:,i,:].shape}, {output.shape}\" )\n",
    "            outputs[:,i,:]=output.squeeze(1)\n",
    "    print(outputs.shape)\n",
    "    \n",
    "    \n",
    "    target=alp_to_mat(label,len(label))\n",
    "#     output=model(image, description)\n",
    "    \n",
    "    target_for_loss=target.view(-1,370).to(device)\n",
    "    output_for_loss=outputs.view(-1,370).to(device)\n",
    "#     print(f\"out: {outputs}\\n------------------------------\")\n",
    "    print(f\"out changed shape: {target_for_loss}\")\n",
    "    \n",
    "#     loss=criterion(output_for_loss, target_for_loss, torch.Tensor(output_for_loss.size(0)).cuda().fill_(1.0))\n",
    "#     print(f\"output shape:{output.shape} <=> label shape:{len(label)}, target shape:{target.shape}\")\n",
    "#     print(f\"loss: {loss}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "992212fe",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17ed724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 37])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=torch.zeros(8, 10, 37)\n",
    "test[0,:,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a91d7",
   "metadata": {},
   "source": [
    "## nickCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55dfcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nickCLIP(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.image_encoder=image_encoder.to(device)\n",
    "        self.text_encoder=text_encoder.to(device)\n",
    "        self.decoder=decoder.to(device)\n",
    "        \n",
    "    def forward(self, image, description):\n",
    "        img_embed=self.image_encoder(image).unsqueeze(0)\n",
    "        sen_embed=self.text_encoder(description).unsqueeze(0)\n",
    "        merged_embed=img_embed+sen_embed\n",
    "        \n",
    "        batch_size=image.shape[0]\n",
    "        target_len=10\n",
    "        initial_c=torch.zeros(1,batch_size,768).to(device)\n",
    "        start_token = torch.zeros(batch_size,1,37).to(device) # start token??\n",
    "        \n",
    "        inputs=start_token\n",
    "        hidden_state=merged_embed\n",
    "        cell_state=initial_c\n",
    "        \n",
    "        outputs=torch.zeros(batch_size, target_len, 37)\n",
    "#         for b in range(batch_size):\n",
    "        for i in range(target_len):\n",
    "            output,(hidden_state, cell_state)=decoder(inputs,(hidden_state, cell_state))\n",
    "            inputs=output\n",
    "#             print(f\"{i}-output_shape:{output.shape}\")\n",
    "#             print(f\"{outputs[:,i,:].shape}, {output.shape}\" )\n",
    "            outputs[:,i,:]=output.squeeze(1)\n",
    "        \n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cd216e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nickCLIP(\n",
       "  (image_encoder): Image_Encoder(\n",
       "    (backbone): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (10): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Flatten(start_dim=1, end_dim=-1)\n",
       "      (13): Linear(in_features=784, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (text_encoder): Text_Encoder(\n",
       "    (BERT): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): lstm_decoder(\n",
       "    (lstm): LSTM(37, 768, batch_first=True)\n",
       "    (linear): Linear(in_features=768, out_features=37, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image_Enc = Image_Encoder()\n",
    "Text_Enc=Text_Encoder(device=device)\n",
    "Decoder=lstm_decoder(input_size=37, hidden_size=768)\n",
    "model=nickCLIP(image_encoder=Image_Enc, text_encoder=Text_Enc, decoder=Decoder, device=device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee25bb11",
   "metadata": {},
   "source": [
    "PATH=\"/workspace/team2/data/filter_50000/\"\n",
    "BATCH_SIZE = 16\n",
    "dataloaders = build_dataloader(PATH=PATH, batch_size=BATCH_SIZE)\n",
    "for index, batch in enumerate(dataloaders['train']):\n",
    "    images = batch[0]\n",
    "    description = batch[1]\n",
    "    label = batch[2]\n",
    "    print(f\"{index}-{images.shape} {len(description)} {len(label)}\")\n",
    "    \n",
    "    img_embed=Image_Enc(images.to(device))\n",
    "    sen_embed=Text_Enc(description)\n",
    "    concat_embed=img_embed+sen_embed\n",
    "    target=alp_to_mat(label,len(label))\n",
    "    \n",
    "    \n",
    "    \n",
    "    result=model(images.to(device), description)\n",
    "    \n",
    "    print(f\"img_embed:{img_embed.shape}, sen_embed:{sen_embed.shape}, concat_embed:{concat_embed.shape}, result:{result.shape}, target:{target.shape} \")\n",
    "#     target=alp_to_mat(label,len(label))\n",
    "#     output=model(image, description)\n",
    "    \n",
    "#     target_for_loss=target.view(-1,370).to(device)\n",
    "#     output_for_loss=output.view(-1,370).to(device)\n",
    "    \n",
    "#     loss=criterion(output_for_loss, target_for_loss, torch.Tensor(output_for_loss.size(0)).cuda().fill_(1.0))\n",
    "#     print(f\"output shape:{output.shape} <=> label shape:{len(label)}, target shape:{target.shape}\")\n",
    "#     print(f\"loss: {loss}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0877f3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d99077a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, des, label\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    des_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        des_list.append(b)\n",
    "        label_list.append(c)\n",
    "\n",
    "    return torch.stack(image_list, dim=0), des_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d825a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot dict\n",
    "def alp_to_mat(string_list,batch_size):\n",
    "    one_hot_dict={'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'0':26,'1':27,'2':28,'3':29,'4':30,'5':31,'6':32,'7':33,'8':34,'9':35,' ':36}\n",
    "    alp_to_num_list=[]\n",
    "    batch_size=len(string_list)\n",
    "    alp_to_mat_list=torch.zeros((batch_size,len(string_list[0]),37)) #(B,name_len,one_hot_len)\n",
    "    for j in range(batch_size):\n",
    "        for i in range(len(string_list[j])):\n",
    "            char=string_list[j][i]\n",
    "            if(char in one_hot_dict.keys()):\n",
    "                pass\n",
    "            else:\n",
    "                char=' '\n",
    "                \n",
    "            alp_to_num_list.append(one_hot_dict[char])\n",
    "            mat=torch.zeros(37)\n",
    "    #         print(len(string_list))\n",
    "    #         print(mat.shape)\n",
    "            mat[one_hot_dict[char]]=1\n",
    "#             print(f\"{alp_to_mat_list.shape}-{mat.shape}\")\n",
    "            alp_to_mat_list[j,i,:]=mat\n",
    "    #         result=torch.Tensor(alp_to_mat_list)\n",
    "    return alp_to_mat_list\n",
    "    \n",
    "\n",
    "\n",
    "# test=[list(\"heloo     \"),list(\"     fffff\"),list(\"kdkdkkdkdk\"),list(\"abcdefghij\")]\n",
    "# result=alp_to_mat(test,len(test))\n",
    "# result.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b565e41",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "criterion=torch.nn.CosineEmbeddingLoss()\n",
    "for index, batch in enumerate(dataloaders['train']):\n",
    "    images = batch[0]\n",
    "    description = batch[1]\n",
    "    label = batch[2]\n",
    "    print(f\"{index}-{images.shape} {description} {len(label)}\")\n",
    "    image=images.to(device)\n",
    "    \n",
    "    target=alp_to_mat(label,len(label))\n",
    "    output=model(image, description)\n",
    "    \n",
    "    target_for_loss=target.view(-1,370).to(device)\n",
    "    output_for_loss=output.view(-1,370).to(device)\n",
    "    \n",
    "    loss=criterion(output_for_loss, target_for_loss, torch.Tensor(output_for_loss.size(0)).cuda().fill_(1.0))\n",
    "    print(f\"output shape:{output.shape} <=> label shape:{len(label)}, target shape:{target.shape}\")\n",
    "    print(f\"loss: {loss}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc063d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cf74912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, criterion, optimizer, device):\n",
    "    train_loss = defaultdict(float)\n",
    "    val_loss = defaultdict(float)\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        running_loss = defaultdict(float)\n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0].to(device)\n",
    "            description = batch[1]\n",
    "            label = batch[2]\n",
    "            \n",
    "            target=alp_to_mat(label,len(label))\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                predictions = model(images, description)\n",
    "                \n",
    "            target_for_loss=target.view(-1,370).to(device)\n",
    "            predictions_for_loss=predictions.view(-1,370).to(device)\n",
    "\n",
    "            loss=criterion(predictions_for_loss, target_for_loss, torch.Tensor(predictions_for_loss.size(0)).cuda().fill_(1.0))\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss[\"total_loss\"] += loss.item()\n",
    "                \n",
    "                train_loss[\"total_loss\"] += loss.item()\n",
    "                \n",
    "                if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                    text = f\"<<<iteration:[{index}/{len(dataloaders[phase])}] - \"\n",
    "                    for k, v in running_loss.items():\n",
    "                        text += f\"{k}: {v/VERBOSE_FREQ:.4f}  \"\n",
    "                        running_loss[k] = 0.\n",
    "                    print(text)\n",
    "            else:\n",
    "                val_loss[\"total_loss\"] += loss.item()\n",
    "        \n",
    "    for k in train_loss.keys():\n",
    "        train_loss[k] /= len(dataloaders[\"train\"])\n",
    "        val_loss[k] /= len(dataloaders[\"val\"])\n",
    "            \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f2c61d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset:43864 validset:11340\n"
     ]
    }
   ],
   "source": [
    "PATH=\"/workspace/team2/data/filter_50000/\"\n",
    "\n",
    "is_cuda = True\n",
    "\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 16\n",
    "VERBOSE_FREQ = 20\n",
    "LR=0.001\n",
    "\n",
    "IMAGE_ENC=\"RESNET34\"\n",
    "TEXT_ENC=\"BERT\"\n",
    "DECODER=\"LSTM\"\n",
    "num_epochs = 100\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "\n",
    "dataloaders = build_dataloader(PATH=PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "Image_Enc = Image_Encoder()\n",
    "Text_Enc=Text_Encoder(device=device)\n",
    "Decoder=lstm_decoder(input_size=37, hidden_size=768)\n",
    "model=nickCLIP(image_encoder=Image_Enc, text_encoder=Text_Enc, decoder=Decoder, device=device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54238182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgomduribo\u001b[0m (\u001b[33murp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/team2/yb_workspace/nickCLIP/experiments/wandb/run-20231116_113955-jqynwomj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/urp/nickclip_baseline/runs/jqynwomj' target=\"_blank\">tough-serenity-3</a></strong> to <a href='https://wandb.ai/urp/nickclip_baseline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/urp/nickclip_baseline' target=\"_blank\">https://wandb.ai/urp/nickclip_baseline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/urp/nickclip_baseline/runs/jqynwomj' target=\"_blank\">https://wandb.ai/urp/nickclip_baseline/runs/jqynwomj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/urp/nickclip_baseline/runs/jqynwomj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2e9952fee0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"nickclip_baseline\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"image encoder\":IMAGE_ENC,\n",
    "    \"test encoder\":TEXT_ENC,\n",
    "    \"decoder\":DECODER,\n",
    "    \"dataset\": PATH,\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea48e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[20/2742] - total_loss: 0.8908  \n",
      "<<<iteration:[40/2742] - total_loss: 0.8179  \n",
      "<<<iteration:[60/2742] - total_loss: 0.8105  \n",
      "<<<iteration:[80/2742] - total_loss: 0.8076  \n",
      "<<<iteration:[100/2742] - total_loss: 0.8065  \n",
      "<<<iteration:[120/2742] - total_loss: 0.8028  \n",
      "<<<iteration:[140/2742] - total_loss: 0.8068  \n",
      "<<<iteration:[160/2742] - total_loss: 0.8052  \n",
      "<<<iteration:[180/2742] - total_loss: 0.7988  \n",
      "<<<iteration:[200/2742] - total_loss: 0.7983  \n",
      "<<<iteration:[220/2742] - total_loss: 0.8050  \n",
      "<<<iteration:[240/2742] - total_loss: 0.8014  \n",
      "<<<iteration:[260/2742] - total_loss: 0.8027  \n",
      "<<<iteration:[280/2742] - total_loss: 0.7973  \n",
      "<<<iteration:[300/2742] - total_loss: 0.7948  \n",
      "<<<iteration:[320/2742] - total_loss: 0.8000  \n",
      "<<<iteration:[340/2742] - total_loss: 0.7926  \n",
      "<<<iteration:[360/2742] - total_loss: 0.7940  \n",
      "<<<iteration:[380/2742] - total_loss: 0.7946  \n",
      "<<<iteration:[400/2742] - total_loss: 0.7971  \n",
      "<<<iteration:[420/2742] - total_loss: 0.7965  \n",
      "<<<iteration:[440/2742] - total_loss: 0.7956  \n",
      "<<<iteration:[460/2742] - total_loss: 0.7927  \n",
      "<<<iteration:[480/2742] - total_loss: 0.7919  \n",
      "<<<iteration:[500/2742] - total_loss: 0.7967  \n",
      "<<<iteration:[520/2742] - total_loss: 0.7956  \n",
      "<<<iteration:[540/2742] - total_loss: 0.7956  \n",
      "<<<iteration:[560/2742] - total_loss: 0.7898  \n",
      "<<<iteration:[580/2742] - total_loss: 0.7904  \n",
      "<<<iteration:[600/2742] - total_loss: 0.7931  \n",
      "<<<iteration:[620/2742] - total_loss: 0.7932  \n",
      "<<<iteration:[640/2742] - total_loss: 0.7960  \n",
      "<<<iteration:[660/2742] - total_loss: 0.7901  \n",
      "<<<iteration:[680/2742] - total_loss: 0.7890  \n",
      "<<<iteration:[700/2742] - total_loss: 0.7915  \n",
      "<<<iteration:[720/2742] - total_loss: 0.7883  \n",
      "<<<iteration:[740/2742] - total_loss: 0.7905  \n",
      "<<<iteration:[760/2742] - total_loss: 0.7929  \n",
      "<<<iteration:[780/2742] - total_loss: 0.7879  \n",
      "<<<iteration:[800/2742] - total_loss: 0.7860  \n",
      "<<<iteration:[820/2742] - total_loss: 0.7918  \n",
      "<<<iteration:[840/2742] - total_loss: 0.7873  \n",
      "<<<iteration:[860/2742] - total_loss: 0.7896  \n",
      "<<<iteration:[880/2742] - total_loss: 0.7911  \n",
      "<<<iteration:[900/2742] - total_loss: 0.7907  \n",
      "<<<iteration:[920/2742] - total_loss: 0.7879  \n",
      "<<<iteration:[940/2742] - total_loss: 0.7896  \n",
      "<<<iteration:[960/2742] - total_loss: 0.7926  \n",
      "<<<iteration:[980/2742] - total_loss: 0.7873  \n",
      "<<<iteration:[1000/2742] - total_loss: 0.7902  \n",
      "<<<iteration:[1020/2742] - total_loss: 0.7889  \n",
      "<<<iteration:[1040/2742] - total_loss: 0.7888  \n",
      "<<<iteration:[1060/2742] - total_loss: 0.7926  \n",
      "<<<iteration:[1080/2742] - total_loss: 0.7904  \n",
      "<<<iteration:[1100/2742] - total_loss: 0.7880  \n",
      "<<<iteration:[1120/2742] - total_loss: 0.7913  \n",
      "<<<iteration:[1140/2742] - total_loss: 0.7893  \n",
      "<<<iteration:[1160/2742] - total_loss: 0.7854  \n",
      "<<<iteration:[1180/2742] - total_loss: 0.7920  \n",
      "<<<iteration:[1200/2742] - total_loss: 0.7912  \n",
      "<<<iteration:[1220/2742] - total_loss: 0.7908  \n",
      "<<<iteration:[1240/2742] - total_loss: 0.7866  \n",
      "<<<iteration:[1260/2742] - total_loss: 0.7878  \n",
      "<<<iteration:[1280/2742] - total_loss: 0.7874  \n",
      "<<<iteration:[1300/2742] - total_loss: 0.7905  \n",
      "<<<iteration:[1320/2742] - total_loss: 0.7944  \n",
      "<<<iteration:[1340/2742] - total_loss: 0.7900  \n",
      "<<<iteration:[1360/2742] - total_loss: 0.7891  \n",
      "<<<iteration:[1380/2742] - total_loss: 0.7824  \n",
      "<<<iteration:[1400/2742] - total_loss: 0.7866  \n",
      "<<<iteration:[1420/2742] - total_loss: 0.7846  \n",
      "<<<iteration:[1440/2742] - total_loss: 0.7835  \n",
      "<<<iteration:[1460/2742] - total_loss: 0.7938  \n",
      "<<<iteration:[1480/2742] - total_loss: 0.7894  \n",
      "<<<iteration:[1500/2742] - total_loss: 0.7905  \n",
      "<<<iteration:[1520/2742] - total_loss: 0.7876  \n",
      "<<<iteration:[1540/2742] - total_loss: 0.7875  \n",
      "<<<iteration:[1560/2742] - total_loss: 0.7855  \n",
      "<<<iteration:[1580/2742] - total_loss: 0.7862  \n",
      "<<<iteration:[1600/2742] - total_loss: 0.7894  \n",
      "<<<iteration:[1620/2742] - total_loss: 0.7882  \n",
      "<<<iteration:[1640/2742] - total_loss: 0.7860  \n",
      "<<<iteration:[1660/2742] - total_loss: 0.7870  \n",
      "<<<iteration:[1680/2742] - total_loss: 0.7862  \n",
      "<<<iteration:[1700/2742] - total_loss: 0.7885  \n",
      "<<<iteration:[1720/2742] - total_loss: 0.7895  \n",
      "<<<iteration:[1740/2742] - total_loss: 0.7882  \n",
      "<<<iteration:[1760/2742] - total_loss: 0.7871  \n",
      "<<<iteration:[1780/2742] - total_loss: 0.7899  \n",
      "<<<iteration:[1800/2742] - total_loss: 0.7889  \n",
      "<<<iteration:[1820/2742] - total_loss: 0.7879  \n",
      "<<<iteration:[1840/2742] - total_loss: 0.7822  \n",
      "<<<iteration:[1860/2742] - total_loss: 0.7861  \n",
      "<<<iteration:[1880/2742] - total_loss: 0.7859  \n",
      "<<<iteration:[1900/2742] - total_loss: 0.7876  \n",
      "<<<iteration:[1920/2742] - total_loss: 0.7867  \n",
      "<<<iteration:[1940/2742] - total_loss: 0.7902  \n",
      "<<<iteration:[1960/2742] - total_loss: 0.7824  \n",
      "<<<iteration:[1980/2742] - total_loss: 0.7839  \n",
      "<<<iteration:[2000/2742] - total_loss: 0.7883  \n",
      "<<<iteration:[2020/2742] - total_loss: 0.7842  \n",
      "<<<iteration:[2040/2742] - total_loss: 0.7854  \n",
      "<<<iteration:[2060/2742] - total_loss: 0.7910  \n",
      "<<<iteration:[2080/2742] - total_loss: 0.7833  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[2100/2742] - total_loss: 0.7917  \n",
      "<<<iteration:[2120/2742] - total_loss: 0.7864  \n",
      "<<<iteration:[2140/2742] - total_loss: 0.7829  \n",
      "<<<iteration:[2160/2742] - total_loss: 0.7892  \n",
      "<<<iteration:[2180/2742] - total_loss: 0.7880  \n",
      "<<<iteration:[2200/2742] - total_loss: 0.7857  \n",
      "<<<iteration:[2220/2742] - total_loss: 0.7851  \n",
      "<<<iteration:[2240/2742] - total_loss: 0.7889  \n",
      "<<<iteration:[2260/2742] - total_loss: 0.7887  \n",
      "<<<iteration:[2280/2742] - total_loss: 0.7858  \n",
      "<<<iteration:[2300/2742] - total_loss: 0.7842  \n",
      "<<<iteration:[2320/2742] - total_loss: 0.7866  \n",
      "<<<iteration:[2340/2742] - total_loss: 0.7845  \n",
      "<<<iteration:[2360/2742] - total_loss: 0.7879  \n",
      "<<<iteration:[2380/2742] - total_loss: 0.7884  \n",
      "<<<iteration:[2400/2742] - total_loss: 0.7844  \n",
      "<<<iteration:[2420/2742] - total_loss: 0.7831  \n",
      "<<<iteration:[2440/2742] - total_loss: 0.7874  \n",
      "<<<iteration:[2460/2742] - total_loss: 0.7886  \n",
      "<<<iteration:[2480/2742] - total_loss: 0.7878  \n",
      "<<<iteration:[2500/2742] - total_loss: 0.7878  \n",
      "<<<iteration:[2520/2742] - total_loss: 0.7855  \n",
      "<<<iteration:[2540/2742] - total_loss: 0.7911  \n",
      "<<<iteration:[2560/2742] - total_loss: 0.7866  \n",
      "<<<iteration:[2580/2742] - total_loss: 0.7836  \n",
      "<<<iteration:[2600/2742] - total_loss: 0.7836  \n",
      "<<<iteration:[2620/2742] - total_loss: 0.7882  \n",
      "<<<iteration:[2640/2742] - total_loss: 0.7865  \n",
      "<<<iteration:[2660/2742] - total_loss: 0.7874  \n",
      "<<<iteration:[2680/2742] - total_loss: 0.7818  \n",
      "<<<iteration:[2700/2742] - total_loss: 0.7871  \n",
      "<<<iteration:[2720/2742] - total_loss: 0.7869  \n",
      "<<<iteration:[2740/2742] - total_loss: 0.7859  \n",
      "\n",
      "epoch:1/100 - Train Loss: 0.7907, Val Loss: 0.7865\n",
      "\n",
      "<<<iteration:[20/2742] - total_loss: 0.8259  \n",
      "<<<iteration:[40/2742] - total_loss: 0.7863  \n",
      "<<<iteration:[60/2742] - total_loss: 0.7872  \n",
      "<<<iteration:[80/2742] - total_loss: 0.7857  \n",
      "<<<iteration:[100/2742] - total_loss: 0.7808  \n",
      "<<<iteration:[120/2742] - total_loss: 0.7848  \n",
      "<<<iteration:[140/2742] - total_loss: 0.7857  \n",
      "<<<iteration:[160/2742] - total_loss: 0.7884  \n",
      "<<<iteration:[180/2742] - total_loss: 0.7854  \n",
      "<<<iteration:[200/2742] - total_loss: 0.7889  \n",
      "<<<iteration:[220/2742] - total_loss: 0.7863  \n",
      "<<<iteration:[240/2742] - total_loss: 0.7869  \n",
      "<<<iteration:[260/2742] - total_loss: 0.7822  \n",
      "<<<iteration:[280/2742] - total_loss: 0.7868  \n",
      "<<<iteration:[300/2742] - total_loss: 0.7811  \n",
      "<<<iteration:[320/2742] - total_loss: 0.7868  \n",
      "<<<iteration:[340/2742] - total_loss: 0.7901  \n",
      "<<<iteration:[360/2742] - total_loss: 0.7865  \n",
      "<<<iteration:[380/2742] - total_loss: 0.7898  \n",
      "<<<iteration:[400/2742] - total_loss: 0.7869  \n",
      "<<<iteration:[420/2742] - total_loss: 0.7912  \n",
      "<<<iteration:[440/2742] - total_loss: 0.7846  \n",
      "<<<iteration:[460/2742] - total_loss: 0.7889  \n",
      "<<<iteration:[480/2742] - total_loss: 0.7836  \n",
      "<<<iteration:[500/2742] - total_loss: 0.7893  \n",
      "<<<iteration:[520/2742] - total_loss: 0.7854  \n",
      "<<<iteration:[540/2742] - total_loss: 0.7858  \n",
      "<<<iteration:[560/2742] - total_loss: 0.7889  \n",
      "<<<iteration:[580/2742] - total_loss: 0.7869  \n",
      "<<<iteration:[600/2742] - total_loss: 0.7866  \n",
      "<<<iteration:[620/2742] - total_loss: 0.7842  \n",
      "<<<iteration:[640/2742] - total_loss: 0.7876  \n",
      "<<<iteration:[660/2742] - total_loss: 0.7875  \n",
      "<<<iteration:[680/2742] - total_loss: 0.7864  \n",
      "<<<iteration:[700/2742] - total_loss: 0.7862  \n",
      "<<<iteration:[720/2742] - total_loss: 0.7894  \n",
      "<<<iteration:[740/2742] - total_loss: 0.7901  \n",
      "<<<iteration:[760/2742] - total_loss: 0.7852  \n",
      "<<<iteration:[780/2742] - total_loss: 0.7893  \n",
      "<<<iteration:[800/2742] - total_loss: 0.7866  \n",
      "<<<iteration:[820/2742] - total_loss: 0.7873  \n",
      "<<<iteration:[840/2742] - total_loss: 0.7827  \n",
      "<<<iteration:[860/2742] - total_loss: 0.7886  \n",
      "<<<iteration:[880/2742] - total_loss: 0.7839  \n",
      "<<<iteration:[900/2742] - total_loss: 0.7796  \n",
      "<<<iteration:[920/2742] - total_loss: 0.7883  \n",
      "<<<iteration:[940/2742] - total_loss: 0.7900  \n",
      "<<<iteration:[960/2742] - total_loss: 0.7880  \n",
      "<<<iteration:[980/2742] - total_loss: 0.7886  \n",
      "<<<iteration:[1000/2742] - total_loss: 0.7848  \n",
      "<<<iteration:[1020/2742] - total_loss: 0.7845  \n",
      "<<<iteration:[1040/2742] - total_loss: 0.7880  \n",
      "<<<iteration:[1060/2742] - total_loss: 0.7862  \n",
      "<<<iteration:[1080/2742] - total_loss: 0.7886  \n",
      "<<<iteration:[1100/2742] - total_loss: 0.7891  \n",
      "<<<iteration:[1120/2742] - total_loss: 0.7835  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[1140/2742] - total_loss: 0.7869  \n",
      "<<<iteration:[1160/2742] - total_loss: 0.7853  \n",
      "<<<iteration:[1180/2742] - total_loss: 0.7851  \n",
      "<<<iteration:[1200/2742] - total_loss: 0.7860  \n",
      "error: image name=>charliesheen_50.jpg des name=>charliesheen_50.txt label name=>charliesheen_50.txt\n",
      "<<<iteration:[1220/2742] - total_loss: 0.7868  \n",
      "<<<iteration:[1240/2742] - total_loss: 0.7902  \n",
      "<<<iteration:[1260/2742] - total_loss: 0.7871  \n",
      "<<<iteration:[1280/2742] - total_loss: 0.7850  \n",
      "<<<iteration:[1300/2742] - total_loss: 0.7831  \n",
      "<<<iteration:[1320/2742] - total_loss: 0.7889  \n",
      "<<<iteration:[1340/2742] - total_loss: 0.7846  \n",
      "<<<iteration:[1360/2742] - total_loss: 0.7862  \n",
      "<<<iteration:[1380/2742] - total_loss: 0.7886  \n",
      "<<<iteration:[1400/2742] - total_loss: 0.7829  \n",
      "<<<iteration:[1420/2742] - total_loss: 0.7834  \n",
      "<<<iteration:[1440/2742] - total_loss: 0.7841  \n",
      "<<<iteration:[1460/2742] - total_loss: 0.7824  \n",
      "<<<iteration:[1480/2742] - total_loss: 0.7903  \n",
      "<<<iteration:[1500/2742] - total_loss: 0.7838  \n",
      "<<<iteration:[1520/2742] - total_loss: 0.7850  \n",
      "<<<iteration:[1540/2742] - total_loss: 0.7850  \n",
      "<<<iteration:[1560/2742] - total_loss: 0.7862  \n",
      "<<<iteration:[1580/2742] - total_loss: 0.7831  \n",
      "<<<iteration:[1600/2742] - total_loss: 0.7889  \n"
     ]
    }
   ],
   "source": [
    "best_epoch = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_one_epoch(dataloaders, model, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "#     wandb.log({\"Train Loss\": train_loss['total_loss'],\n",
    "#                \"Val Loss\": val_loss['total_loss'],})\n",
    "    print(f\"\\nepoch:{epoch+1}/{num_epochs} - Train Loss: {train_loss['total_loss']:.4f}, Val Loss: {val_loss['total_loss']:.4f}\\n\")\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model(model.state_dict(), f'model_{epoch+1}.pth', save_dir=f\"./trained_model/{IMAGE_ENC}_{TEXT_ENC}_{DECODER}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d14a4e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a1f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, device):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model=nickCLIP(image_encoder=Image_Enc, text_encoder=Text_Enc, decoder=Decoder, device=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6fb5f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e50db5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path=\"/workspace/team2/yb_workspace/nickCLIP/experiments/trained_model/RESNET34_BERT_LSTM/model_90.pth\"\n",
    "model = load_model(ckpt_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a82c67b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8464"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root='/workspace/team2/data/filter_50000/'\n",
    "test_dataset=Dataset(root=root, phase=\"test\", transformer=transformer)\n",
    "test_dataloaders = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cda899be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f53db3577b45f69c3a1d7e59ba0b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4231, description='index', max=8463), Output()), _dom_classes=('widget-i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def idx_to_c(idx):\n",
    "    one_hot_dict={'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'0':26,'1':27,'2':28,'3':29,'4':30,'5':31,'6':32,'7':33,'8':34,'9':35,' ':36}\n",
    "    idx_to_c = {v: k for k, v in one_hot_dict.items()}\n",
    "    return idx_to_c[idx]\n",
    "# print(idx_to_c)\n",
    "@interact(index=(0, len(test_dataset)-1))\n",
    "def show_sample(index):\n",
    "    img, des, label = test_dataset[index]\n",
    "#     image=img['image'].permute(1,2,0).numpy()\n",
    "    image=img.unsqueeze(0).to(device)\n",
    "    print(f\"description: {des}\")\n",
    "    print(f\"label: {''.join(label)}\")\n",
    "    \n",
    "    predictions = model(images, des)\n",
    "    result_torch=torch.argmax(predictions, dim=2)\n",
    "    result_list=list(result_torch[0])\n",
    "    result=list(map(int, result_list))\n",
    "    result_str=''.join(list(map(idx_to_c, result)))\n",
    "    print(predictions[0][1])\n",
    "    print(result)\n",
    "    print(result_str)\n",
    "#     print\n",
    "    \n",
    "    \n",
    "    image=image.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65edf97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e68b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95cebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
