{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6ba10f-0def-48d3-b726-926f4558d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ce285c-6ee1-41ab-ace2-f49af16857c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d5c2f-6373-4d1e-a762-41d5d53c1f9a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93377edb-602d-4ddd-9c12-d89c1e445297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, root, phase, transformer=None):\n",
    "        self.root=root\n",
    "        self.phase=phase\n",
    "        self.transformer=transformer\n",
    "        self.image_list=sorted(os.listdir(root+\"image/\"+phase))\n",
    "        self.des_list=sorted(os.listdir(root+\"description/\"+phase))\n",
    "        self.label_list=sorted(os.listdir(root+\"label/\"+phase))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, des, label = self.get_data(index)\n",
    "        return img, des, label\n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def get_data(self, index):\n",
    "        # label\n",
    "        label_file_name=self.label_list[index]\n",
    "        lab_f=open(self.root+\"label/\"+self.phase+\"/\"+label_file_name, \"r\")\n",
    "        label=lab_f.read()\n",
    "\n",
    "        # description\n",
    "        des_file_name=self.des_list[index]\n",
    "        des_f=open(self.root+\"description/\"+self.phase+\"/\"+des_file_name, \"r\")\n",
    "        des_text=des_f.read()\n",
    "        des=des_text.split(\" \")\n",
    "\n",
    "        # image\n",
    "        img_file_name=self.image_list[index]\n",
    "        image=cv2.imread(self.root+\"image/\"+self.phase+\"/\"+img_file_name)\n",
    "        img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if(self.transformer!=None):\n",
    "            transformed_img=self.transformer(image=img)\n",
    "            img=transformed_img\n",
    "        \n",
    "        return img, des, label\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37180c3-40c0-4f29-84b7-da062c85be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=448\n",
    "transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e40fb9-26a2-4084-a74c-62d7647138cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='/workspace/team2/data/nickData/'\n",
    "train_dataset=Dataset(root=root, phase=\"train\", transformer=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4086df6-9f89-488e-9b8d-a7414ba2eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, des, label=train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25b3eb2-ebf3-403c-98db-b3b6fd964e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 448, 448])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb5975-f093-4940-952b-abff5967f2fc",
   "metadata": {},
   "source": [
    "## MODELs\n",
    " ![Untitled](../img/nickCLIP_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae290c3",
   "metadata": {},
   "source": [
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d328de53-85c7-43e6-808c-32c4241de00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        resnet = torchvision.models.resnet34(pretrained = True)\n",
    "        layers = [m for m in resnet.children()]\n",
    "        \n",
    "        self.backbone = nn.Sequential(*layers[:-2]) \n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, padding=0,bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.head(out) # final output=> (1, 196)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b041c468-eff7-4f10-a803-cebecf1bbe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-11         [-1, 64, 112, 112]               0\n",
      "           Conv2d-12         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
      "             ReLU-14         [-1, 64, 112, 112]               0\n",
      "           Conv2d-15         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-16         [-1, 64, 112, 112]             128\n",
      "             ReLU-17         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-18         [-1, 64, 112, 112]               0\n",
      "           Conv2d-19         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-20         [-1, 64, 112, 112]             128\n",
      "             ReLU-21         [-1, 64, 112, 112]               0\n",
      "           Conv2d-22         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-23         [-1, 64, 112, 112]             128\n",
      "             ReLU-24         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-25         [-1, 64, 112, 112]               0\n",
      "           Conv2d-26          [-1, 128, 56, 56]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 56, 56]             256\n",
      "             ReLU-28          [-1, 128, 56, 56]               0\n",
      "           Conv2d-29          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 56, 56]             256\n",
      "           Conv2d-31          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
      "             ReLU-33          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-34          [-1, 128, 56, 56]               0\n",
      "           Conv2d-35          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 56, 56]             256\n",
      "             ReLU-37          [-1, 128, 56, 56]               0\n",
      "           Conv2d-38          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 56, 56]             256\n",
      "             ReLU-40          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-41          [-1, 128, 56, 56]               0\n",
      "           Conv2d-42          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 56, 56]             256\n",
      "             ReLU-44          [-1, 128, 56, 56]               0\n",
      "           Conv2d-45          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 56, 56]             256\n",
      "             ReLU-47          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-48          [-1, 128, 56, 56]               0\n",
      "           Conv2d-49          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 56, 56]             256\n",
      "             ReLU-51          [-1, 128, 56, 56]               0\n",
      "           Conv2d-52          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 56, 56]             256\n",
      "             ReLU-54          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-55          [-1, 128, 56, 56]               0\n",
      "           Conv2d-56          [-1, 256, 28, 28]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 28, 28]             512\n",
      "             ReLU-58          [-1, 256, 28, 28]               0\n",
      "           Conv2d-59          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 28, 28]             512\n",
      "           Conv2d-61          [-1, 256, 28, 28]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 28, 28]             512\n",
      "             ReLU-63          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-64          [-1, 256, 28, 28]               0\n",
      "           Conv2d-65          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 28, 28]             512\n",
      "             ReLU-67          [-1, 256, 28, 28]               0\n",
      "           Conv2d-68          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 28, 28]             512\n",
      "             ReLU-70          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-71          [-1, 256, 28, 28]               0\n",
      "           Conv2d-72          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 28, 28]             512\n",
      "             ReLU-74          [-1, 256, 28, 28]               0\n",
      "           Conv2d-75          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 28, 28]             512\n",
      "             ReLU-77          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-78          [-1, 256, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 28, 28]             512\n",
      "             ReLU-84          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-85          [-1, 256, 28, 28]               0\n",
      "           Conv2d-86          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 28, 28]             512\n",
      "             ReLU-88          [-1, 256, 28, 28]               0\n",
      "           Conv2d-89          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 28, 28]             512\n",
      "             ReLU-91          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-92          [-1, 256, 28, 28]               0\n",
      "           Conv2d-93          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 28, 28]             512\n",
      "             ReLU-95          [-1, 256, 28, 28]               0\n",
      "           Conv2d-96          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 28, 28]             512\n",
      "             ReLU-98          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-99          [-1, 256, 28, 28]               0\n",
      "          Conv2d-100          [-1, 512, 14, 14]       1,179,648\n",
      "     BatchNorm2d-101          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-102          [-1, 512, 14, 14]               0\n",
      "          Conv2d-103          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-104          [-1, 512, 14, 14]           1,024\n",
      "          Conv2d-105          [-1, 512, 14, 14]         131,072\n",
      "     BatchNorm2d-106          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-107          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-108          [-1, 512, 14, 14]               0\n",
      "          Conv2d-109          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-110          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-111          [-1, 512, 14, 14]               0\n",
      "          Conv2d-112          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-113          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-114          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-115          [-1, 512, 14, 14]               0\n",
      "          Conv2d-116          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-117          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-118          [-1, 512, 14, 14]               0\n",
      "          Conv2d-119          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-120          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-121          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-122          [-1, 512, 14, 14]               0\n",
      "          Conv2d-123          [-1, 256, 14, 14]         131,072\n",
      "     BatchNorm2d-124          [-1, 256, 14, 14]             512\n",
      "            ReLU-125          [-1, 256, 14, 14]               0\n",
      "          Conv2d-126          [-1, 128, 14, 14]         294,912\n",
      "     BatchNorm2d-127          [-1, 128, 14, 14]             256\n",
      "            ReLU-128          [-1, 128, 14, 14]               0\n",
      "          Conv2d-129           [-1, 32, 14, 14]          36,864\n",
      "     BatchNorm2d-130           [-1, 32, 14, 14]              64\n",
      "            ReLU-131           [-1, 32, 14, 14]               0\n",
      "          Conv2d-132            [-1, 1, 14, 14]             288\n",
      "     BatchNorm2d-133            [-1, 1, 14, 14]               2\n",
      "            ReLU-134            [-1, 1, 14, 14]               0\n",
      "         Flatten-135                  [-1, 196]               0\n",
      "================================================================\n",
      "Total params: 21,748,642\n",
      "Trainable params: 21,748,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 386.98\n",
      "Params size (MB): 82.96\n",
      "Estimated Total Size (MB): 472.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Image_Enc = Image_Encoder()\n",
    "Image_Enc.to(device)\n",
    "torchsummary.summary(Image_Enc, (3,448,448))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04259a95",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d841b979-6222-4183-854d-a83b06918ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.BERT = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out=self.BERT(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "926c73a0-8099-4ced-a244-25a563220ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "The 'safetensors>=0.3.1' distribution was not found and is required by this application. \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/torch/hub/huggingface_pytorch-transformers_main/src/transformers/utils/versions.py:102\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     got_ver \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/metadata.py:530\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m:return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/metadata.py:503\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m:return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/metadata.py:177\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: safetensors",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Text_Enc\u001b[38;5;241m=\u001b[39m\u001b[43mText_Encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m, in \u001b[0;36mText_Encoder.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBERT \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuggingface/pytorch-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py:542\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    539\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    540\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 542\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py:569\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, hubconf_dir)\n\u001b[1;32m    568\u001b[0m hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[0;32m--> 569\u001b[0m hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m    572\u001b[0m model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py:90\u001b[0m, in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     88\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[0;32m---> 90\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:848\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/torch/hub/huggingface_pytorch-transformers_main/hubconf.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m SRC_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(SRC_DIR)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     AutoConfig,\n\u001b[1;32m     24\u001b[0m     AutoModel,\n\u001b[1;32m     25\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     26\u001b[0m     AutoModelForMaskedLM,\n\u001b[1;32m     27\u001b[0m     AutoModelForQuestionAnswering,\n\u001b[1;32m     28\u001b[0m     AutoModelForSequenceClassification,\n\u001b[1;32m     29\u001b[0m     AutoTokenizer,\n\u001b[1;32m     30\u001b[0m     add_start_docstrings,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     34\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilelock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportlib_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(AutoConfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/.cache/torch/hub/huggingface_pytorch-transformers_main/src/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     logging,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/huggingface_pytorch-transformers_main/src/transformers/dependency_versions_check.py:57\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/torch/hub/huggingface_pytorch-transformers_main/src/transformers/utils/versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install transformers -U or pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/huggingface_pytorch-transformers_main/src/transformers/utils/versions.py:104\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    102\u001b[0m     got_ver \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(pkg)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m distribution was not found and is required by this application. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# check that the right version is installed if version number or a range was provided\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: The 'safetensors>=0.3.1' distribution was not found and is required by this application. \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main"
     ]
    }
   ],
   "source": [
    "Text_Enc=Text_Encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d95a6-5999-45a7-9573-4749e7931ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
